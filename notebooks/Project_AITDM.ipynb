{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mM4J2YDWDx4r"
      },
      "source": [
        "# Enhancing Brain Disease Diagnosis\n",
        "\n",
        "## Federated Learning for Multi-Center Medical Imaging\n",
        "\n",
        "### AI for Trustworthy Decision Making\n",
        "\n",
        "- Poață Andrei-Cătălin\n",
        "- Vulpe Ștefan\n",
        "- Vișan Ionuț"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXlb41oIJqYa",
        "outputId": "8db07529-836f-417e-8a23-381d82a2a6cf"
      },
      "outputs": [],
      "source": [
        "!pip install segmentation_models_pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYFrmuebJtzx"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/client.zip -d /content/client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5a1acYv0Ju3C"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/Preprocessed-Data.zip -d /content/Preprocessed-Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_K1UZo4RoWi",
        "outputId": "f77d988a-b4ea-45af-b649-171723e42a79"
      },
      "outputs": [],
      "source": [
        "!pip install -q \"flwr[simulation]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYfjGgr9E0Su"
      },
      "source": [
        "# **M1 - Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "KJRjNQW9FGwU",
        "outputId": "26cd2b2c-6a31-431c-d03d-ec39038a70cf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load all files\n",
        "mri = np.load(\"/content/PatientID_0036_mri.npy\")\n",
        "tumor = np.load(\"/content/PatientID_0036_tumor.npy\")\n",
        "atlas = np.load(\"/content/PatientID_0036_regions.npy\")\n",
        "\n",
        "# Print shapes\n",
        "print(\"MRI shape:\", mri.shape)\n",
        "print(\"Tumor shape:\", tumor.shape)\n",
        "print(\"Atlas shape:\", atlas.shape)\n",
        "\n",
        "# Plot them together\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# MRI\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(mri, cmap=\"gray\")\n",
        "plt.title(\"MRI\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "# Tumor mask\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(tumor, cmap=\"hot\")\n",
        "plt.title(\"Tumor mask\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "# Atlas\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(atlas, cmap=\"nipy_spectral\")\n",
        "plt.title(\"Atlas regions\")\n",
        "plt.axis(\"off\")\n",
        "plt.colorbar()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zn2kMfdIK4et"
      },
      "source": [
        "***MRI (left image)***\n",
        "\n",
        "- This is the actual brain scan.\n",
        "\n",
        "- It shows the anatomical structures and tissue appearance.\n",
        "\n",
        "- The model uses this image as the input.\n",
        "\n",
        "--------------------------------------------\n",
        "***Tumor Mask (middle image)***\n",
        "\n",
        "This is the ground-truth annotation.\n",
        "\n",
        "- Yellow = whole tumor region\n",
        "\n",
        "- Red = core or high-confidence tumor area\n",
        "\n",
        "- Black = background\n",
        "\n",
        "The model tries to predict this mask.\n",
        "\n",
        "--------------------------------------------\n",
        "***Atlas Regions (right image)***\n",
        "\n",
        "- This is a brain anatomical atlas aligned to the MRI.\n",
        "\n",
        "- Each color corresponds to a different anatomical region.\n",
        "\n",
        "- It gives the model context about where in the brain each pixel is located.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFOZ6KnBLrIt"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/Preprocessed-Data.zip -d /content/Preprocessed-Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 879
        },
        "id": "AsYhGMN5OMs7",
        "outputId": "0464a8a2-087d-483f-dc65-65fe2fc82ee4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "DATA_ROOT = \"/content/Preprocessed-Data\"\n",
        "USE_ATLAS = True\n",
        "\n",
        "rows = []\n",
        "\n",
        "# 1. Iterate over all patient folders in Preprocessed-Data\n",
        "for pid in sorted(os.listdir(DATA_ROOT)):\n",
        "    patient_dir = os.path.join(DATA_ROOT, pid)\n",
        "    if not os.path.isdir(patient_dir):\n",
        "        continue\n",
        "\n",
        "    tumor_path = os.path.join(patient_dir, f\"{pid}_tumor.npy\")\n",
        "    atlas_path = os.path.join(patient_dir, f\"{pid}_regions.npy\")\n",
        "\n",
        "    # Skip if tumor mask is missing\n",
        "    if not os.path.isfile(tumor_path):\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # Load tumor mask\n",
        "        tumor = np.load(tumor_path).astype(np.float32)\n",
        "        mask = tumor > 0.5\n",
        "\n",
        "        # Tumor area and presence\n",
        "        area = float(mask.sum())\n",
        "        has_tumor = 1 if area >= 1 else 0\n",
        "\n",
        "        # Dominant atlas region (inside tumor) if atlas exists\n",
        "        dom_region = -1\n",
        "        if USE_ATLAS and os.path.isfile(atlas_path) and mask.any():\n",
        "            atlas = np.load(atlas_path).astype(np.int32)\n",
        "            vals, counts = np.unique(atlas[mask], return_counts=True)\n",
        "            if len(vals) > 0:\n",
        "                dom_region = int(vals[np.argmax(counts)])\n",
        "\n",
        "        rows.append(\n",
        "            {\n",
        "                \"pid\": pid,\n",
        "                \"has_tumor\": has_tumor,\n",
        "                \"area\": area,\n",
        "                \"dom\": dom_region,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Failed for {pid}: {e}\")\n",
        "\n",
        "# 2. Build DataFrame with metadata\n",
        "meta = pd.DataFrame(rows)\n",
        "print(\"Metadata head:\")\n",
        "print(meta.head())\n",
        "print(\"\\nColumns:\", meta.columns.tolist())\n",
        "print(\"\\nNumber of patients:\", len(meta))\n",
        "\n",
        "# 3. Compute size_bin (tumor size bins) like in your partitioning\n",
        "meta[\"size_bin\"] = 0\n",
        "mask_has_tumor = meta[\"has_tumor\"] == 1\n",
        "\n",
        "if mask_has_tumor.any():\n",
        "    areas = meta.loc[mask_has_tumor, \"area\"].values\n",
        "    qs = np.quantile(areas, np.linspace(0, 1, 4))\n",
        "    qs = np.unique(qs)\n",
        "    if len(qs) > 2:\n",
        "        bins = np.digitize(areas, qs[1:-1], right=True)\n",
        "    else:\n",
        "        bins = np.zeros_like(areas, dtype=int)\n",
        "    meta.loc[mask_has_tumor, \"size_bin\"] = bins.astype(int)\n",
        "\n",
        "print(\"\\nSize bin value counts:\")\n",
        "print(meta[\"size_bin\"].value_counts().sort_index())\n",
        "\n",
        "# 4. Plot the distributions used in data partitioning\n",
        "\n",
        "fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "# Tumor size bins\n",
        "meta[\"size_bin\"].value_counts().sort_index().plot(kind=\"bar\", ax=axs[0, 0])\n",
        "axs[0, 0].set_title(\"Tumor size bins\")\n",
        "axs[0, 0].set_xlabel(\"size_bin\")\n",
        "axs[0, 0].set_ylabel(\"Number of patients\")\n",
        "\n",
        "# Has tumor (0/1)\n",
        "meta[\"has_tumor\"].value_counts().sort_index().plot(kind=\"bar\", ax=axs[0, 1])\n",
        "axs[0, 1].set_title(\"Has tumor\")\n",
        "axs[0, 1].set_xlabel(\"has_tumor (0 = no, 1 = yes)\")\n",
        "axs[0, 1].set_ylabel(\"Number of patients\")\n",
        "\n",
        "# Tumor area histogram\n",
        "meta[\"area\"].hist(bins=30, ax=axs[1, 0])\n",
        "axs[1, 0].set_title(\"Tumor area (voxel count)\")\n",
        "axs[1, 0].set_xlabel(\"area\")\n",
        "axs[1, 0].set_ylabel(\"Number of patients\")\n",
        "\n",
        "# Dominant atlas regions (top 10)\n",
        "meta[\"dom\"].value_counts().head(10).plot(kind=\"bar\", ax=axs[1, 1])\n",
        "axs[1, 1].set_title(\"Top 10 dominant atlas regions\")\n",
        "axs[1, 1].set_xlabel(\"region label\")\n",
        "axs[1, 1].set_ylabel(\"Number of patients\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rxd1FA6yPLhc"
      },
      "source": [
        "***Tumor size bins***\n",
        "\n",
        "- These bins represent three tumor-size groups created using quantiles of the tumor voxel count.\n",
        "\n",
        "- They ensure that small, medium, and large tumors are balanced across the dataset and across client splits.\n",
        "\n",
        "------------------------------------\n",
        "***Tumor area (voxel count)***\n",
        "\n",
        "- This histogram shows the raw distribution of tumor sizes in the dataset.\n",
        "\n",
        "- It highlights the variability in tumor burden, from small lesions to very large ones.\n",
        "\n",
        "------------------------------------\n",
        "***Top dominant atlas regions***\n",
        "\n",
        "- Each tumor belongs mostly to one anatomical region in the brain atlas.\n",
        "\n",
        "- This plot shows which regions occur most frequently and helps guide stratification, since some regions are common while others are rare.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 937
        },
        "id": "NWJbDT_JSujU",
        "outputId": "34aed7cd-932f-4c2a-f26f-64bd91590c94"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the cleaned dataframe from pickle\n",
        "with open(\"cleaned_df.pkl\", \"rb\") as f:\n",
        "    df = pickle.load(f)\n",
        "\n",
        "# Print all column names\n",
        "print(\"Columns in cleaned_df:\")\n",
        "for c in df.columns:\n",
        "    print(\" -\", c)\n",
        "\n",
        "# Column names used for plotting\n",
        "col_sex = \"Sex at Birth\"\n",
        "col_race = \"Race\"\n",
        "col_age = \"Age at diagnosis\"\n",
        "col_diag = \"Primary Diagnosis\"\n",
        "\n",
        "# Create a 2x2 grid of subplots\n",
        "fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Pie chart: Sex at Birth\n",
        "df[col_sex].value_counts(dropna=False).plot(\n",
        "    kind=\"pie\", autopct=\"%1.1f%%\", ax=axs[0, 0], ylabel=\"\"\n",
        ")\n",
        "axs[0, 0].set_title(\"Sex at Birth\")\n",
        "\n",
        "# Pie chart: Race\n",
        "df[col_race].value_counts(dropna=False).plot(\n",
        "    kind=\"pie\", autopct=\"%1.1f%%\", ax=axs[0, 1], ylabel=\"\"\n",
        ")\n",
        "axs[0, 1].set_title(\"Race\")\n",
        "\n",
        "# Pie chart: Primary Diagnosis\n",
        "df[col_diag].value_counts(dropna=False).plot(\n",
        "    kind=\"pie\", autopct=\"%1.1f%%\", ax=axs[1, 0], ylabel=\"\"\n",
        ")\n",
        "axs[1, 0].set_title(\"Primary Diagnosis\")\n",
        "\n",
        "# Create age bins and pie chart: Age at Diagnosis\n",
        "age_bins = pd.cut(df[col_age], bins=[0, 20, 40, 60, 80, 120])\n",
        "age_bins.value_counts().sort_index().plot(\n",
        "    kind=\"pie\", autopct=\"%1.1f%%\", ax=axs[1, 1], ylabel=\"\"\n",
        ")\n",
        "axs[1, 1].set_title(\"Age at Diagnosis (binned)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GZTx9vVNTPY"
      },
      "source": [
        "**<h1>Data Partitioning<h1>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9y8S3ShnIyP"
      },
      "source": [
        "<h2>One client, all data<h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JG6B5jVm5yH",
        "outputId": "3e67ce56-51e2-4a10-86cb-1a9ecac93cce"
      },
      "outputs": [],
      "source": [
        "%%writefile data_prep_split.py\n",
        "import os, json, pickle, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Tuple\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Paths and global configuration\n",
        "DATA_ROOT = \"Preprocessed-Data\"\n",
        "METADATA_DF_PATH = \"cleaned_df.pkl\"\n",
        "OUT_BASE = \"AITDM\"\n",
        "CLIENT_DIR = os.path.join(OUT_BASE, \"client\")\n",
        "os.makedirs(CLIENT_DIR, exist_ok=True)\n",
        "\n",
        "USE_ATLAS = True\n",
        "N_CLIENTS = 1\n",
        "VAL_FRAC_PER_CLIENT = 0.2\n",
        "BATCH_SIZE = 8\n",
        "NUM_WORKERS = 2\n",
        "SEED = 42\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    \"\"\"Set all relevant random seeds for reproducibility.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "set_seed(SEED)\n",
        "\n",
        "\n",
        "class ImageOnlyGliomaDataset(Dataset):\n",
        "    \"\"\"Dataset that loads MRI, tumor mask and optional atlas for each patient.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        metadata_df_path,\n",
        "        data_root,\n",
        "        use_atlas=True,\n",
        "        exclude_ids=None,\n",
        "        transform=None,\n",
        "    ):\n",
        "        with open(metadata_df_path, \"rb\") as f:\n",
        "            df = pickle.load(f)\n",
        "\n",
        "        if exclude_ids is None:\n",
        "            exclude_ids = [\"PatientID_0191\"]\n",
        "\n",
        "        # Filter out excluded patient IDs\n",
        "        self.df = df[~df[\"Patient_ID\"].isin(exclude_ids)].reset_index(drop=True)\n",
        "        self.data_root = data_root\n",
        "        self.use_atlas = use_atlas\n",
        "        self.transform = transform\n",
        "\n",
        "        # Keep only patients for which all required .npy files exist\n",
        "        self.patient_ids = []\n",
        "        for pid in sorted(self.df[\"Patient_ID\"].tolist()):\n",
        "            mri_p = os.path.join(self.data_root, pid, f\"{pid}_mri.npy\")\n",
        "            tumor_p = os.path.join(self.data_root, pid, f\"{pid}_tumor.npy\")\n",
        "            if self.use_atlas:\n",
        "                reg_p = os.path.join(self.data_root, pid, f\"{pid}_regions.npy\")\n",
        "                if (\n",
        "                    os.path.isfile(mri_p)\n",
        "                    and os.path.isfile(tumor_p)\n",
        "                    and os.path.isfile(reg_p)\n",
        "                ):\n",
        "                    self.patient_ids.append(pid)\n",
        "            else:\n",
        "                if os.path.isfile(mri_p) and os.path.isfile(tumor_p):\n",
        "                    self.patient_ids.append(pid)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patient_ids)\n",
        "\n",
        "    @staticmethod\n",
        "    def _minmax(x):\n",
        "        \"\"\"Apply simple min-max normalization to a numpy array.\"\"\"\n",
        "        x = x.astype(np.float32)\n",
        "        mn, mx = np.min(x), np.max(x)\n",
        "        return (\n",
        "            (x - mn) / (mx - mn)\n",
        "            if mx > mn\n",
        "            else np.zeros_like(x, dtype=np.float32)\n",
        "        )\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Load one patient sample (MRI, tumor mask, and optional atlas).\"\"\"\n",
        "        pid = self.patient_ids[idx]\n",
        "        base = os.path.join(self.data_root, pid)\n",
        "\n",
        "        # Load MRI and tumor mask\n",
        "        mri = np.load(os.path.join(base, f\"{pid}_mri.npy\")).astype(np.float32)\n",
        "        tumor = np.load(os.path.join(base, f\"{pid}_tumor.npy\")).astype(\n",
        "            np.float32\n",
        "        )\n",
        "\n",
        "        # Normalize MRI and binarize tumor mask\n",
        "        mri = self._minmax(mri)\n",
        "        tumor = (tumor > 0.5).astype(np.float32)\n",
        "\n",
        "        sample = {\"patient_id\": pid, \"mri\": mri, \"tumor\": tumor}\n",
        "\n",
        "        # Optionally load and normalize atlas regions\n",
        "        if self.use_atlas:\n",
        "            regions = np.load(os.path.join(base, f\"{pid}_regions.npy\")).astype(\n",
        "                np.float32\n",
        "            )\n",
        "            regions = self._minmax(regions)\n",
        "            sample[\"regions\"] = regions\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "def image_only_collate_fn(batch, use_atlas=True):\n",
        "    \"\"\"\n",
        "    Custom collate function to build batched tensors:\n",
        "    x: [B, C, H, W], y: [B, 1, H, W], pid: list of patient IDs.\n",
        "    \"\"\"\n",
        "    mri = torch.stack([torch.tensor(it[\"mri\"]) for it in batch]).unsqueeze(1)\n",
        "    y = torch.stack([torch.tensor(it[\"tumor\"]) for it in batch]).unsqueeze(1)\n",
        "\n",
        "    if use_atlas:\n",
        "        regs = torch.stack(\n",
        "            [torch.tensor(it[\"regions\"]) for it in batch]\n",
        "        ).unsqueeze(1)\n",
        "        x = torch.cat([mri.float(), regs.float()], dim=1)\n",
        "    else:\n",
        "        x = mri.float()\n",
        "\n",
        "    pids = [it[\"patient_id\"] for it in batch]\n",
        "    return {\"x\": x, \"y\": y.float(), \"pid\": pids}\n",
        "\n",
        "\n",
        "def patient_meta(pid: str) -> Tuple[int, float, int]:\n",
        "    \"\"\"\n",
        "    Compute simple metadata for one patient:\n",
        "    - has_tumor: 0/1 depending on tumor area\n",
        "    - area: tumor voxel count\n",
        "    - dom_region: dominant atlas region inside tumor mask\n",
        "    \"\"\"\n",
        "    base = os.path.join(DATA_ROOT, pid)\n",
        "\n",
        "    tumor = np.load(os.path.join(base, f\"{pid}_tumor.npy\")).astype(np.float32)\n",
        "    mask = tumor > 0.5\n",
        "    area = float(mask.sum())\n",
        "    has_tumor = 1 if area >= 1 else 0\n",
        "\n",
        "    dom_region = -1\n",
        "    reg_path = os.path.join(base, f\"{pid}_regions.npy\")\n",
        "\n",
        "    # If atlas is present and tumor exists, find region with most tumor voxels\n",
        "    if USE_ATLAS and os.path.isfile(reg_path) and mask.any():\n",
        "        regs = np.load(reg_path).astype(np.int32)\n",
        "        vals, counts = np.unique(regs[mask], return_counts=True)\n",
        "        if len(vals) > 0:\n",
        "            dom_region = int(vals[np.argmax(counts)])\n",
        "\n",
        "    return has_tumor, area, dom_region\n",
        "\n",
        "\n",
        "def build_meta_for(dataset: ImageOnlyGliomaDataset) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Build a DataFrame with metadata for each patient:\n",
        "    - pid\n",
        "    - has_tumor\n",
        "    - area\n",
        "    - dom (dominant region)\n",
        "    - size_bin (tumor size bin)\n",
        "    - strat_label (full stratification label)\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    for pid in dataset.patient_ids:\n",
        "        try:\n",
        "            ht, area, dom = patient_meta(pid)\n",
        "            rows.append({\"pid\": pid, \"has_tumor\": ht, \"area\": area, \"dom\": dom})\n",
        "        except Exception:\n",
        "            # Skip patients that fail to load or compute\n",
        "            pass\n",
        "\n",
        "    meta = pd.DataFrame(rows)\n",
        "\n",
        "    # Default size bin = 0 (no tumor or very small)\n",
        "    meta[\"size_bin\"] = 0\n",
        "    m = meta[\"has_tumor\"] == 1\n",
        "\n",
        "    # For patients with tumor, compute quantile-based size bins\n",
        "    if m.any():\n",
        "        areas = meta.loc[m, \"area\"].values\n",
        "        qs = np.quantile(areas, np.linspace(0, 1, 4))\n",
        "        qs = np.unique(qs)\n",
        "        if len(qs) > 2:\n",
        "            bins = np.digitize(areas, qs[1:-1], right=True)\n",
        "        else:\n",
        "            bins = np.zeros_like(areas, dtype=int)\n",
        "        meta.loc[m, \"size_bin\"] = bins.astype(int)\n",
        "\n",
        "    # Full stratification label combining several attributes\n",
        "    meta[\"strat_label\"] = [\n",
        "        f\"{int(ht)}_{int(dom)}_{int(sb)}\"\n",
        "        for ht, dom, sb in zip(meta[\"has_tumor\"], meta[\"dom\"], meta[\"size_bin\"])\n",
        "    ]\n",
        "    return meta\n",
        "\n",
        "\n",
        "def build_label(meta_df: pd.DataFrame, level: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Build a stratification label at different granularity levels:\n",
        "    - \"full\": has_tumor + dom + size_bin\n",
        "    - \"ht_dom\": has_tumor + dom\n",
        "    - \"ht_size\": has_tumor + size_bin\n",
        "    - \"ht\": has_tumor only\n",
        "    \"\"\"\n",
        "    if level == \"full\":\n",
        "        return np.array(\n",
        "            [\n",
        "                f\"{int(ht)}_{int(dom)}_{int(sb)}\"\n",
        "                for ht, dom, sb in zip(\n",
        "                    meta_df[\"has_tumor\"], meta_df[\"dom\"], meta_df[\"size_bin\"]\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "    if level == \"ht_dom\":\n",
        "        return np.array(\n",
        "            [\n",
        "                f\"{int(ht)}_{int(dom)}\"\n",
        "                for ht, dom in zip(meta_df[\"has_tumor\"], meta_df[\"dom\"])\n",
        "            ]\n",
        "        )\n",
        "    if level == \"ht_size\":\n",
        "        return np.array(\n",
        "            [\n",
        "                f\"{int(ht)}_{int(sb)}\"\n",
        "                for ht, sb in zip(meta_df[\"has_tumor\"], meta_df[\"size_bin\"])\n",
        "            ]\n",
        "        )\n",
        "    if level == \"ht\":\n",
        "        return meta_df[\"has_tumor\"].astype(str).values\n",
        "    raise ValueError(level)\n",
        "\n",
        "\n",
        "def pick_strat_labels_for_client(\n",
        "    meta_client_df: pd.DataFrame, min_per_class: int = 2\n",
        "):\n",
        "    \"\"\"\n",
        "    Try different stratification levels and pick the most detailed one\n",
        "    that has at least `min_per_class` samples for each class.\n",
        "    \"\"\"\n",
        "    for level in [\"full\", \"ht_dom\", \"ht_size\", \"ht\"]:\n",
        "        y = build_label(meta_client_df, level)\n",
        "        counts = pd.Series(y).value_counts()\n",
        "        if (counts >= min_per_class).all():\n",
        "            print(\n",
        "                f\"[INFO] Using client train/val stratification level: {level}\"\n",
        "            )\n",
        "            return y, level\n",
        "    print(\"[WARN] Falling back to NON-stratified train/val.\")\n",
        "    return None, \"none\"\n",
        "\n",
        "\n",
        "# Load full dataset and build metadata\n",
        "dataset = ImageOnlyGliomaDataset(\n",
        "    METADATA_DF_PATH,\n",
        "    DATA_ROOT,\n",
        "    use_atlas=USE_ATLAS,\n",
        "    exclude_ids=[\"PatientID_0191\"],\n",
        ")\n",
        "meta = build_meta_for(dataset)\n",
        "\n",
        "clients: List[Dict[str, List[str]]] = []\n",
        "client_metas = []\n",
        "\n",
        "# In this script we only create a single \"client_0\" with a stratified train/val split\n",
        "meta_c = meta.reset_index(drop=True)\n",
        "Xc = meta_c[\"pid\"].values\n",
        "\n",
        "y_client, tv_level = pick_strat_labels_for_client(meta_c, min_per_class=2)\n",
        "\n",
        "if tv_level != \"none\":\n",
        "    # Stratified train/validation split\n",
        "    sss = StratifiedShuffleSplit(\n",
        "        n_splits=1, test_size=VAL_FRAC_PER_CLIENT, random_state=SEED\n",
        "    )\n",
        "    ((tr_idx, va_idx),) = sss.split(Xc, y_client)\n",
        "    train_pids = Xc[tr_idx].tolist()\n",
        "    val_pids = Xc[va_idx].tolist()\n",
        "else:\n",
        "    # Fallback: random split without stratification\n",
        "    rng = np.random.RandomState(SEED)\n",
        "    perm = rng.permutation(len(Xc))\n",
        "    split_at = int((1.0 - VAL_FRAC_PER_CLIENT) * len(Xc))\n",
        "    train_pids = Xc[perm[:split_at]].tolist()\n",
        "    val_pids = Xc[perm[split_at:]].tolist()\n",
        "\n",
        "# Save client_0 train/val patient ID lists\n",
        "cdir = os.path.join(CLIENT_DIR, \"client_0\")\n",
        "os.makedirs(cdir, exist_ok=True)\n",
        "with open(os.path.join(cdir, \"train_pids.json\"), \"w\") as f:\n",
        "    json.dump(train_pids, f, indent=2)\n",
        "with open(os.path.join(cdir, \"val_pids.json\"), \"w\") as f:\n",
        "    json.dump(val_pids, f, indent=2)\n",
        "\n",
        "clients.append({\"train\": train_pids, \"val\": val_pids})\n",
        "client_metas.append(meta_c)\n",
        "\n",
        "# Save basic manifest of the split configuration\n",
        "manifest = {\n",
        "    \"seed\": SEED,\n",
        "    \"use_atlas\": USE_ATLAS,\n",
        "    \"n_clients\": 1,\n",
        "    \"val_frac_per_client\": VAL_FRAC_PER_CLIENT,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"num_workers\": NUM_WORKERS,\n",
        "    \"kfold_level\": \"single_client\",\n",
        "}\n",
        "with open(os.path.join(CLIENT_DIR, \"manifest.json\"), \"w\") as f:\n",
        "    json.dump(manifest, f, indent=2)\n",
        "\n",
        "\n",
        "class SubsetByPIDs(Dataset):\n",
        "    \"\"\"Simple subset wrapper that restricts the dataset to a given list of patient IDs.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, full_dataset: ImageOnlyGliomaDataset, pid_list: List[str]\n",
        "    ):\n",
        "        self.ds = full_dataset\n",
        "        pid_to_idx = {pid: i for i, pid in enumerate(self.ds.patient_ids)}\n",
        "        # Keep only IDs that exist in the base dataset\n",
        "        self.indices = [pid_to_idx[p] for p in pid_list if p in pid_to_idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.ds[self.indices[i]]\n",
        "\n",
        "\n",
        "def make_loader(ds, shuffle):\n",
        "    \"\"\"Create a DataLoader with the custom collate function.\"\"\"\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "        collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "        generator=torch.Generator().manual_seed(SEED),\n",
        "    )\n",
        "\n",
        "\n",
        "# Build small preview loaders for sanity checks\n",
        "preview_loaders = []\n",
        "for cid, cl in enumerate(clients):\n",
        "    ds_tr = SubsetByPIDs(dataset, cl[\"train\"])\n",
        "    ds_va = SubsetByPIDs(dataset, cl[\"val\"])\n",
        "    ld_tr = make_loader(ds_tr, shuffle=True)\n",
        "    ld_va = make_loader(ds_va, shuffle=False)\n",
        "    preview_loaders.append((ld_tr, ld_va))\n",
        "\n",
        "\n",
        "def summarize(meta_df: pd.DataFrame, name: str) -> Dict:\n",
        "    \"\"\"\n",
        "    Create a compact summary of a metadata subset:\n",
        "    - number of patients\n",
        "    - tumor presence counts\n",
        "    - size bin counts\n",
        "    - top 5 dominant regions\n",
        "    \"\"\"\n",
        "    s = {\"name\": name, \"n\": int(len(meta_df))}\n",
        "    s[\"has_tumor_counts\"] = meta_df[\"has_tumor\"].value_counts().to_dict()\n",
        "    s[\"size_bin_counts\"] = meta_df[\"size_bin\"].value_counts().to_dict()\n",
        "    s[\"dom_region_top5\"] = meta_df[\"dom\"].value_counts().head(5).to_dict()\n",
        "    return s\n",
        "\n",
        "\n",
        "# Global summary\n",
        "summary_all = summarize(meta, \"ALL\")\n",
        "print(\"\\n=== Global summary ===\")\n",
        "print(summary_all)\n",
        "\n",
        "# Per-client train/val summaries (here only one client)\n",
        "per_client_summaries = []\n",
        "for cid, cl in enumerate(clients):\n",
        "    meta_c = client_metas[cid]\n",
        "    tr = meta_c[meta_c[\"pid\"].isin(cl[\"train\"])]\n",
        "    va = meta_c[meta_c[\"pid\"].isin(cl[\"val\"])]\n",
        "    s_client = {\n",
        "        \"client\": \"client\",\n",
        "        \"train\": summarize(tr, \"client_train\"),\n",
        "        \"val\": summarize(va, \"client_val\"),\n",
        "    }\n",
        "    per_client_summaries.append(s_client)\n",
        "    print(\"\\n=== Client ===\")\n",
        "    print(s_client)\n",
        "\n",
        "# Save all summaries to disk\n",
        "with open(os.path.join(CLIENT_DIR, \"summary.json\"), \"w\") as f:\n",
        "    json.dump(\n",
        "        {\"global\": summary_all, \"per_client\": per_client_summaries}, f, indent=2\n",
        "    )\n",
        "\n",
        "print(f\"\\nSaved client split and summaries to: {CLIENT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "if1Ip7f-Gn01"
      },
      "source": [
        "**Data Distribution**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6DL3ct7luT4"
      },
      "outputs": [],
      "source": [
        "{\n",
        "    \"global\": {\n",
        "        \"name\": \"ALL\",\n",
        "        \"n\": 202,\n",
        "        \"has_tumor_counts\": {\"1\": 202},\n",
        "        \"size_bin_counts\": {\"0\": 68, \"1\": 67, \"2\": 67},\n",
        "        \"dom_region_top5\": {\"61\": 85, \"50\": 81, \"0\": 10, \"10\": 4, \"22\": 3},\n",
        "    },\n",
        "    \"per_client\": [\n",
        "        {\n",
        "            \"client\": \"client\",\n",
        "            \"train\": {\n",
        "                \"name\": \"client_train\",\n",
        "                \"n\": 161,\n",
        "                \"has_tumor_counts\": {\"1\": 161},\n",
        "                \"size_bin_counts\": {\"2\": 54, \"0\": 54, \"1\": 53},\n",
        "                \"dom_region_top5\": {\n",
        "                    \"61\": 68,\n",
        "                    \"50\": 64,\n",
        "                    \"0\": 9,\n",
        "                    \"10\": 3,\n",
        "                    \"15\": 2,\n",
        "                },\n",
        "            },\n",
        "            \"val\": {\n",
        "                \"name\": \"client_val\",\n",
        "                \"n\": 41,\n",
        "                \"has_tumor_counts\": {\"1\": 41},\n",
        "                \"size_bin_counts\": {\"1\": 14, \"0\": 14, \"2\": 13},\n",
        "                \"dom_region_top5\": {\n",
        "                    \"61\": 17,\n",
        "                    \"50\": 17,\n",
        "                    \"31\": 1,\n",
        "                    \"53\": 1,\n",
        "                    \"10\": 1,\n",
        "                },\n",
        "            },\n",
        "        }\n",
        "    ],\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iqd4YVbAmZe-"
      },
      "source": [
        "<h2>Same distribution between 3 clients<h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FHiqxXmkUeF",
        "outputId": "20000cb3-f7eb-45be-ee68-8747efecb7fd"
      },
      "outputs": [],
      "source": [
        "%%writefile data_prep_split.py\n",
        "import os, json, pickle, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Tuple\n",
        "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Global paths and configuration\n",
        "DATA_ROOT = \"Preprocessed-Data\"\n",
        "METADATA_DF_PATH = \"cleaned_df.pkl\"\n",
        "OUT_BASE = \"AITDM\"\n",
        "CLIENT_DIR = os.path.join(OUT_BASE, \"client\")\n",
        "os.makedirs(CLIENT_DIR, exist_ok=True)\n",
        "\n",
        "USE_ATLAS = True\n",
        "N_CLIENTS = 3\n",
        "VAL_FRAC_PER_CLIENT = 0.2\n",
        "BATCH_SIZE = 8\n",
        "NUM_WORKERS = 2\n",
        "SEED = 42\n",
        "\n",
        "\n",
        "# Set all random seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "set_seed(SEED)\n",
        "\n",
        "\n",
        "# Dataset that loads MRI, tumor mask and optional atlas for each patient\n",
        "class ImageOnlyGliomaDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        metadata_df_path,\n",
        "        data_root,\n",
        "        use_atlas=True,\n",
        "        exclude_ids=None,\n",
        "        transform=None,\n",
        "    ):\n",
        "        with open(metadata_df_path, \"rb\") as f:\n",
        "            df = pickle.load(f)\n",
        "        if exclude_ids is None:\n",
        "            exclude_ids = [\"PatientID_0191\"]\n",
        "        self.df = df[~df[\"Patient_ID\"].isin(exclude_ids)].reset_index(drop=True)\n",
        "        self.data_root = data_root\n",
        "        self.use_atlas = use_atlas\n",
        "        self.transform = transform\n",
        "\n",
        "        # Collect patient IDs that have all required .npy files\n",
        "        self.patient_ids = []\n",
        "        for pid in sorted(self.df[\"Patient_ID\"].tolist()):\n",
        "            mri_p = os.path.join(self.data_root, pid, f\"{pid}_mri.npy\")\n",
        "            tumor_p = os.path.join(self.data_root, pid, f\"{pid}_tumor.npy\")\n",
        "            if self.use_atlas:\n",
        "                reg_p = os.path.join(self.data_root, pid, f\"{pid}_regions.npy\")\n",
        "                if (\n",
        "                    os.path.isfile(mri_p)\n",
        "                    and os.path.isfile(tumor_p)\n",
        "                    and os.path.isfile(reg_p)\n",
        "                ):\n",
        "                    self.patient_ids.append(pid)\n",
        "            else:\n",
        "                if os.path.isfile(mri_p) and os.path.isfile(tumor_p):\n",
        "                    self.patient_ids.append(pid)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patient_ids)\n",
        "\n",
        "    # Simple min–max normalization\n",
        "    @staticmethod\n",
        "    def _minmax(x):\n",
        "        x = x.astype(np.float32)\n",
        "        mn, mx = np.min(x), np.max(x)\n",
        "        return (\n",
        "            (x - mn) / (mx - mn)\n",
        "            if mx > mn\n",
        "            else np.zeros_like(x, dtype=np.float32)\n",
        "        )\n",
        "\n",
        "    # Load a single patient sample\n",
        "    def __getitem__(self, idx):\n",
        "        pid = self.patient_ids[idx]\n",
        "        base = os.path.join(self.data_root, pid)\n",
        "        mri = np.load(os.path.join(base, f\"{pid}_mri.npy\")).astype(np.float32)\n",
        "        tumor = np.load(os.path.join(base, f\"{pid}_tumor.npy\")).astype(\n",
        "            np.float32\n",
        "        )\n",
        "        mri = self._minmax(mri)\n",
        "        tumor = (tumor > 0.5).astype(np.float32)\n",
        "        sample = {\"patient_id\": pid, \"mri\": mri, \"tumor\": tumor}\n",
        "        if self.use_atlas:\n",
        "            regions = np.load(os.path.join(base, f\"{pid}_regions.npy\")).astype(\n",
        "                np.float32\n",
        "            )\n",
        "            regions = self._minmax(regions)\n",
        "            sample[\"regions\"] = regions\n",
        "        return sample\n",
        "\n",
        "\n",
        "# Custom collate function building batched tensors and patient IDs\n",
        "def image_only_collate_fn(batch, use_atlas=True):\n",
        "    mri = torch.stack([torch.tensor(it[\"mri\"]) for it in batch]).unsqueeze(1)\n",
        "    y = torch.stack([torch.tensor(it[\"tumor\"]) for it in batch]).unsqueeze(1)\n",
        "    if use_atlas:\n",
        "        regs = torch.stack(\n",
        "            [torch.tensor(it[\"regions\"]) for it in batch]\n",
        "        ).unsqueeze(1)\n",
        "        x = torch.cat([mri.float(), regs.float()], dim=1)\n",
        "    else:\n",
        "        x = mri.float()\n",
        "    pids = [it[\"patient_id\"] for it in batch]\n",
        "    return {\"x\": x, \"y\": y.float(), \"pid\": pids}\n",
        "\n",
        "\n",
        "# Compute basic metadata for a patient (tumor presence, area, dominant atlas region)\n",
        "def patient_meta(pid: str) -> Tuple[int, float, int]:\n",
        "    base = os.path.join(DATA_ROOT, pid)\n",
        "    tumor = np.load(os.path.join(base, f\"{pid}_tumor.npy\")).astype(np.float32)\n",
        "    mask = tumor > 0.5\n",
        "    area = float(mask.sum())\n",
        "    has_tumor = 1 if area >= 1 else 0\n",
        "\n",
        "    dom_region = -1\n",
        "    reg_path = os.path.join(base, f\"{pid}_regions.npy\")\n",
        "    if USE_ATLAS and os.path.isfile(reg_path) and mask.any():\n",
        "        regs = np.load(reg_path).astype(np.int32)\n",
        "        vals, counts = np.unique(regs[mask], return_counts=True)\n",
        "        if len(vals) > 0:\n",
        "            dom_region = int(vals[np.argmax(counts)])\n",
        "    return has_tumor, area, dom_region\n",
        "\n",
        "\n",
        "# Build a metadata DataFrame for all patients in the dataset\n",
        "def build_meta_for(dataset: ImageOnlyGliomaDataset) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for pid in dataset.patient_ids:\n",
        "        try:\n",
        "            ht, area, dom = patient_meta(pid)\n",
        "            rows.append({\"pid\": pid, \"has_tumor\": ht, \"area\": area, \"dom\": dom})\n",
        "        except Exception:\n",
        "            pass\n",
        "    meta = pd.DataFrame(rows)\n",
        "\n",
        "    # Compute tumor size bins for patients with tumor\n",
        "    meta[\"size_bin\"] = 0\n",
        "    m = meta[\"has_tumor\"] == 1\n",
        "    if m.any():\n",
        "        areas = meta.loc[m, \"area\"].values\n",
        "        qs = np.quantile(areas, np.linspace(0, 1, 4))\n",
        "        qs = np.unique(qs)\n",
        "        bins = (\n",
        "            np.digitize(areas, qs[1:-1], right=True)\n",
        "            if len(qs) > 2\n",
        "            else np.zeros_like(areas, dtype=int)\n",
        "        )\n",
        "        meta.loc[m, \"size_bin\"] = bins.astype(int)\n",
        "\n",
        "    # Full stratification label combining several attributes\n",
        "    meta[\"strat_label\"] = [\n",
        "        f\"{int(ht)}_{int(dom)}_{int(sb)}\"\n",
        "        for ht, dom, sb in zip(meta[\"has_tumor\"], meta[\"dom\"], meta[\"size_bin\"])\n",
        "    ]\n",
        "    return meta\n",
        "\n",
        "\n",
        "# Build a stratification label with different levels of granularity\n",
        "def build_label(meta_df: pd.DataFrame, level: str) -> np.ndarray:\n",
        "    if level == \"full\":\n",
        "        return np.array(\n",
        "            [\n",
        "                f\"{int(ht)}_{int(dom)}_{int(sb)}\"\n",
        "                for ht, dom, sb in zip(\n",
        "                    meta_df[\"has_tumor\"], meta_df[\"dom\"], meta_df[\"size_bin\"]\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "    if level == \"ht_dom\":\n",
        "        return np.array(\n",
        "            [\n",
        "                f\"{int(ht)}_{int(dom)}\"\n",
        "                for ht, dom in zip(meta_df[\"has_tumor\"], meta_df[\"dom\"])\n",
        "            ]\n",
        "        )\n",
        "    if level == \"ht_size\":\n",
        "        return np.array(\n",
        "            [\n",
        "                f\"{int(ht)}_{int(sb)}\"\n",
        "                for ht, sb in zip(meta_df[\"has_tumor\"], meta_df[\"size_bin\"])\n",
        "            ]\n",
        "        )\n",
        "    if level == \"ht\":\n",
        "        return meta_df[\"has_tumor\"].astype(str).values\n",
        "    raise ValueError(level)\n",
        "\n",
        "\n",
        "# Choose the stratification labels level for K-Fold client split\n",
        "def pick_strat_labels_for_kfold(meta_df: pd.DataFrame, n_splits: int):\n",
        "    for level in [\"full\", \"ht_dom\", \"ht_size\", \"ht\"]:\n",
        "        y = build_label(meta_df, level)\n",
        "        counts = pd.Series(y).value_counts()\n",
        "        if (counts >= n_splits).all():\n",
        "            print(f\"[INFO] Using K-Fold stratification level: {level}\")\n",
        "            return y, level\n",
        "        else:\n",
        "            rare = counts[counts < n_splits]\n",
        "            print(\n",
        "                f\"[WARN] Level '{level}' has rare classes (<{n_splits}): {rare.to_dict()} -> trying coarser...\"\n",
        "            )\n",
        "    print(\n",
        "        \"[WARN] Falling back to NON-stratified K-Fold (insufficient counts at all levels).\"\n",
        "    )\n",
        "    return None, \"none\"\n",
        "\n",
        "\n",
        "# Choose the stratification labels level for the train/val split inside each client\n",
        "def pick_strat_labels_for_client(\n",
        "    meta_client_df: pd.DataFrame, min_per_class: int = 2\n",
        "):\n",
        "    for level in [\"full\", \"ht_dom\", \"ht_size\", \"ht\"]:\n",
        "        y = build_label(meta_client_df, level)\n",
        "        counts = pd.Series(y).value_counts()\n",
        "        if (counts >= min_per_class).all():\n",
        "            print(\n",
        "                f\"[INFO] Using client train/val stratification level: {level}\"\n",
        "            )\n",
        "            return y, level\n",
        "        else:\n",
        "            rare = counts[counts < min_per_class]\n",
        "            print(\n",
        "                f\"[WARN] Client level '{level}' rare classes (<{min_per_class}): {rare.to_dict()} -> trying coarser...\"\n",
        "            )\n",
        "    print(\n",
        "        \"[WARN] Falling back to NON-stratified train/val (insufficient counts at all levels).\"\n",
        "    )\n",
        "    return None, \"none\"\n",
        "\n",
        "\n",
        "# Load dataset and metadata for all patients\n",
        "dataset = ImageOnlyGliomaDataset(\n",
        "    METADATA_DF_PATH,\n",
        "    DATA_ROOT,\n",
        "    use_atlas=USE_ATLAS,\n",
        "    exclude_ids=[\"PatientID_0191\"],\n",
        ")\n",
        "meta = build_meta_for(dataset)\n",
        "\n",
        "# Prepare global patient IDs and K-Fold labels\n",
        "X_all = meta[\"pid\"].values\n",
        "y_all_full, kfold_level = pick_strat_labels_for_kfold(meta, n_splits=N_CLIENTS)\n",
        "\n",
        "clients: List[Dict[str, List[str]]] = []\n",
        "client_metas = []\n",
        "\n",
        "# Create client splits and per-client train/val splits\n",
        "if kfold_level != \"none\":\n",
        "    skf = StratifiedKFold(n_splits=N_CLIENTS, shuffle=True, random_state=SEED)\n",
        "    for split_idx, (_, idx) in enumerate(skf.split(X_all, y_all_full)):\n",
        "        pids_client = X_all[idx]\n",
        "        meta_c = meta[meta[\"pid\"].isin(pids_client)].reset_index(drop=True)\n",
        "\n",
        "        y_client, tv_level = pick_strat_labels_for_client(\n",
        "            meta_c, min_per_class=2\n",
        "        )\n",
        "        Xc = meta_c[\"pid\"].values\n",
        "\n",
        "        if tv_level != \"none\":\n",
        "            sss = StratifiedShuffleSplit(\n",
        "                n_splits=1, test_size=VAL_FRAC_PER_CLIENT, random_state=SEED\n",
        "            )\n",
        "            ((tr_idx, va_idx),) = sss.split(Xc, y_client)\n",
        "            train_pids = Xc[tr_idx].tolist()\n",
        "            val_pids = Xc[va_idx].tolist()\n",
        "        else:\n",
        "            rng = np.random.RandomState(SEED)\n",
        "            perm = rng.permutation(len(Xc))\n",
        "            split_at = int((1.0 - VAL_FRAC_PER_CLIENT) * len(Xc))\n",
        "            train_pids = Xc[perm[:split_at]].tolist()\n",
        "            val_pids = Xc[perm[split_at:]].tolist()\n",
        "\n",
        "        cdir = os.path.join(CLIENT_DIR, f\"client_{split_idx}\")\n",
        "        os.makedirs(cdir, exist_ok=True)\n",
        "        with open(os.path.join(cdir, \"train_pids.json\"), \"w\") as f:\n",
        "            json.dump(train_pids, f, indent=2)\n",
        "        with open(os.path.join(cdir, \"val_pids.json\"), \"w\") as f:\n",
        "            json.dump(val_pids, f, indent=2)\n",
        "\n",
        "        clients.append({\"train\": train_pids, \"val\": val_pids})\n",
        "        client_metas.append(meta_c)\n",
        "else:\n",
        "    print(\"[INFO] Non-stratified 3-way split (deterministic) for clients.\")\n",
        "    rng = np.random.RandomState(SEED)\n",
        "    perm = rng.permutation(len(X_all))\n",
        "    sizes = [len(X_all) // N_CLIENTS] * N_CLIENTS\n",
        "    sizes[-1] += len(X_all) - sum(sizes)\n",
        "    start = 0\n",
        "    for split_idx, sz in enumerate(sizes):\n",
        "        pids_client = X_all[perm[start : start + sz]]\n",
        "        start += sz\n",
        "\n",
        "        meta_c = meta[meta[\"pid\"].isin(pids_client)].reset_index(drop=True)\n",
        "        Xc = meta_c[\"pid\"].values\n",
        "\n",
        "        perm_c = rng.permutation(len(Xc))\n",
        "        split_at = int((1.0 - VAL_FRAC_PER_CLIENT) * len(Xc))\n",
        "        train_pids = Xc[perm_c[:split_at]].tolist()\n",
        "        val_pids = Xc[perm_c[split_at:]].tolist()\n",
        "\n",
        "        cdir = os.path.join(CLIENT_DIR, f\"client_{split_idx}\")\n",
        "        os.makedirs(cdir, exist_ok=True)\n",
        "        with open(os.path.join(cdir, \"train_pids.json\"), \"w\") as f:\n",
        "            json.dump(train_pids, f, indent=2)\n",
        "        with open(os.path.join(cdir, \"val_pids.json\"), \"w\") as f:\n",
        "            json.dump(val_pids, f, indent=2)\n",
        "\n",
        "        clients.append({\"train\": train_pids, \"val\": val_pids})\n",
        "        client_metas.append(meta_c)\n",
        "\n",
        "# Save a small manifest describing the split configuration\n",
        "manifest = {\n",
        "    \"seed\": SEED,\n",
        "    \"use_atlas\": USE_ATLAS,\n",
        "    \"n_clients\": N_CLIENTS,\n",
        "    \"val_frac_per_client\": VAL_FRAC_PER_CLIENT,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"num_workers\": NUM_WORKERS,\n",
        "    \"kfold_level\": kfold_level,\n",
        "}\n",
        "with open(os.path.join(CLIENT_DIR, \"manifest.json\"), \"w\") as f:\n",
        "    json.dump(manifest, f, indent=2)\n",
        "\n",
        "\n",
        "# Dataset wrapper that restricts to a given list of patient IDs\n",
        "class SubsetByPIDs(Dataset):\n",
        "    def __init__(\n",
        "        self, full_dataset: ImageOnlyGliomaDataset, pid_list: List[str]\n",
        "    ):\n",
        "        self.ds = full_dataset\n",
        "        pid_to_idx = {pid: i for i, pid in enumerate(self.ds.patient_ids)}\n",
        "        self.indices = [pid_to_idx[p] for p in pid_list if p in pid_to_idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.ds[self.indices[i]]\n",
        "\n",
        "\n",
        "# Helper to create a DataLoader with the custom collate function\n",
        "def make_loader(ds, shuffle):\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "        collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "        generator=torch.Generator().manual_seed(SEED),\n",
        "    )\n",
        "\n",
        "\n",
        "# Optional preview loaders for quick sanity checks\n",
        "preview_loaders = []\n",
        "for cid, cl in enumerate(clients):\n",
        "    ds_tr = SubsetByPIDs(dataset, cl[\"train\"])\n",
        "    ds_va = SubsetByPIDs(dataset, cl[\"val\"])\n",
        "    ld_tr = make_loader(ds_tr, shuffle=True)\n",
        "    ld_va = make_loader(ds_va, shuffle=False)\n",
        "    preview_loaders.append((ld_tr, ld_va))\n",
        "\n",
        "\n",
        "# Summarize metadata distribution for a subset\n",
        "def summarize(meta_df: pd.DataFrame, name: str) -> Dict:\n",
        "    s = {\"name\": name, \"n\": int(len(meta_df))}\n",
        "    s[\"has_tumor_counts\"] = meta_df[\"has_tumor\"].value_counts().to_dict()\n",
        "    s[\"size_bin_counts\"] = meta_df[\"size_bin\"].value_counts().to_dict()\n",
        "    s[\"dom_region_top5\"] = meta_df[\"dom\"].value_counts().head(5).to_dict()\n",
        "    return s\n",
        "\n",
        "\n",
        "# Global metadata summary\n",
        "summary_all = summarize(meta, \"ALL\")\n",
        "print(\"\\n=== Global summary ===\")\n",
        "print(summary_all)\n",
        "\n",
        "# Per-client train/val metadata summaries\n",
        "per_client_summaries = []\n",
        "for cid, cl in enumerate(clients):\n",
        "    meta_c = client_metas[cid]\n",
        "    tr = meta_c[meta_c[\"pid\"].isin(cl[\"train\"])]\n",
        "    va = meta_c[meta_c[\"pid\"].isin(cl[\"val\"])]\n",
        "    s_client = {\n",
        "        \"client\": cid,\n",
        "        \"train\": summarize(tr, f\"client_{cid}_train\"),\n",
        "        \"val\": summarize(va, f\"client_{cid}_val\"),\n",
        "    }\n",
        "    per_client_summaries.append(s_client)\n",
        "    print(f\"\\n=== Client {cid} ===\")\n",
        "    print(s_client)\n",
        "\n",
        "# Save all summaries to disk\n",
        "with open(os.path.join(CLIENT_DIR, \"summary.json\"), \"w\") as f:\n",
        "    json.dump(\n",
        "        {\"global\": summary_all, \"per_client\": per_client_summaries}, f, indent=2\n",
        "    )\n",
        "\n",
        "print(f\"\\nSaved 3 client splits and summaries to: {CLIENT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfJlyrz0HEQT"
      },
      "source": [
        "**Data Distribution**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-m02RicJl5mC"
      },
      "outputs": [],
      "source": [
        "{\n",
        "    \"global\": {\n",
        "        \"name\": \"ALL\",\n",
        "        \"n\": 202,\n",
        "        \"has_tumor_counts\": {\"1\": 202},\n",
        "        \"size_bin_counts\": {\"0\": 68, \"1\": 67, \"2\": 67},\n",
        "        \"dom_region_top5\": {\"61\": 85, \"50\": 81, \"0\": 10, \"10\": 4, \"22\": 3},\n",
        "    },\n",
        "    \"per_client\": [\n",
        "        {\n",
        "            \"client\": 0,\n",
        "            \"train\": {\n",
        "                \"name\": \"client_0_train\",\n",
        "                \"n\": 54,\n",
        "                \"has_tumor_counts\": {\"1\": 54},\n",
        "                \"size_bin_counts\": {\"1\": 18, \"2\": 18, \"0\": 18},\n",
        "                \"dom_region_top5\": {\n",
        "                    \"61\": 29,\n",
        "                    \"50\": 15,\n",
        "                    \"0\": 4,\n",
        "                    \"7\": 1,\n",
        "                    \"17\": 1,\n",
        "                },\n",
        "            },\n",
        "            \"val\": {\n",
        "                \"name\": \"client_0_val\",\n",
        "                \"n\": 14,\n",
        "                \"has_tumor_counts\": {\"1\": 14},\n",
        "                \"size_bin_counts\": {\"0\": 5, \"1\": 5, \"2\": 4},\n",
        "                \"dom_region_top5\": {\n",
        "                    \"50\": 9,\n",
        "                    \"53\": 1,\n",
        "                    \"10\": 1,\n",
        "                    \"61\": 1,\n",
        "                    \"15\": 1,\n",
        "                },\n",
        "            },\n",
        "        },\n",
        "        {\n",
        "            \"client\": 1,\n",
        "            \"train\": {\n",
        "                \"name\": \"client_1_train\",\n",
        "                \"n\": 53,\n",
        "                \"has_tumor_counts\": {\"1\": 53},\n",
        "                \"size_bin_counts\": {\"1\": 18, \"2\": 18, \"0\": 17},\n",
        "                \"dom_region_top5\": {\n",
        "                    \"50\": 22,\n",
        "                    \"61\": 19,\n",
        "                    \"0\": 3,\n",
        "                    \"22\": 2,\n",
        "                    \"10\": 1,\n",
        "                },\n",
        "            },\n",
        "            \"val\": {\n",
        "                \"name\": \"client_1_val\",\n",
        "                \"n\": 14,\n",
        "                \"has_tumor_counts\": {\"1\": 14},\n",
        "                \"size_bin_counts\": {\"2\": 5, \"0\": 5, \"1\": 4},\n",
        "                \"dom_region_top5\": {\"50\": 6, \"61\": 6, \"10\": 1, \"22\": 1},\n",
        "            },\n",
        "        },\n",
        "        {\n",
        "            \"client\": 2,\n",
        "            \"train\": {\n",
        "                \"name\": \"client_2_train\",\n",
        "                \"n\": 53,\n",
        "                \"has_tumor_counts\": {\"1\": 53},\n",
        "                \"size_bin_counts\": {\"2\": 18, \"0\": 18, \"1\": 17},\n",
        "                \"dom_region_top5\": {\n",
        "                    \"61\": 25,\n",
        "                    \"50\": 22,\n",
        "                    \"23\": 1,\n",
        "                    \"31\": 1,\n",
        "                    \"62\": 1,\n",
        "                },\n",
        "            },\n",
        "            \"val\": {\n",
        "                \"name\": \"client_2_val\",\n",
        "                \"n\": 14,\n",
        "                \"has_tumor_counts\": {\"1\": 14},\n",
        "                \"size_bin_counts\": {\"0\": 5, \"1\": 5, \"2\": 4},\n",
        "                \"dom_region_top5\": {\"50\": 7, \"61\": 5, \"10\": 1, \"0\": 1},\n",
        "            },\n",
        "        },\n",
        "    ],\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV6kW5udsrGF"
      },
      "source": [
        "<h2>non-IID between 3 clients<h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1fCwCcXtNQk",
        "outputId": "d400a5fd-af59-4560-d0de-d8050a3411aa"
      },
      "outputs": [],
      "source": [
        "import os, json, pickle, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Global configuration and paths\n",
        "DATA_ROOT = \"Preprocessed-Data\"\n",
        "METADATA_DF_PATH = \"cleaned_df.pkl\"\n",
        "OUT_BASE = \"AITDM\"\n",
        "CLIENT_DIR = os.path.join(OUT_BASE, \"client\")\n",
        "os.makedirs(CLIENT_DIR, exist_ok=True)\n",
        "\n",
        "USE_ATLAS = True\n",
        "N_CLIENTS = 7\n",
        "VAL_FRAC_PER_CLIENT = 0.2\n",
        "BATCH_SIZE = 8\n",
        "NUM_WORKERS = 2\n",
        "SEED = 42\n",
        "\n",
        "# Dirichlet alpha controls degree of non-IID distribution\n",
        "DIRICHLET_ALPHA = 0.3\n",
        "\n",
        "\n",
        "# Ensure reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "set_seed(SEED)\n",
        "\n",
        "\n",
        "# Dataset class that loads MRI, tumor masks, and optional atlas data\n",
        "class ImageOnlyGliomaDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        metadata_df_path,\n",
        "        data_root,\n",
        "        use_atlas=True,\n",
        "        exclude_ids=None,\n",
        "        transform=None,\n",
        "    ):\n",
        "        with open(metadata_df_path, \"rb\") as f:\n",
        "            df = pickle.load(f)\n",
        "        if exclude_ids is None:\n",
        "            exclude_ids = [\"PatientID_0191\"]\n",
        "\n",
        "        self.df = df[~df[\"Patient_ID\"].isin(exclude_ids)].reset_index(drop=True)\n",
        "        self.data_root = data_root\n",
        "        self.use_atlas = use_atlas\n",
        "        self.transform = transform\n",
        "\n",
        "        # Collect only patients with required files\n",
        "        self.patient_ids = []\n",
        "        for pid in sorted(self.df[\"Patient_ID\"].tolist()):\n",
        "            mri_p = os.path.join(self.data_root, pid, f\"{pid}_mri.npy\")\n",
        "            tumor_p = os.path.join(self.data_root, pid, f\"{pid}_tumor.npy\")\n",
        "            if self.use_atlas:\n",
        "                reg_p = os.path.join(self.data_root, pid, f\"{pid}_regions.npy\")\n",
        "                if (\n",
        "                    os.path.isfile(mri_p)\n",
        "                    and os.path.isfile(tumor_p)\n",
        "                    and os.path.isfile(reg_p)\n",
        "                ):\n",
        "                    self.patient_ids.append(pid)\n",
        "            else:\n",
        "                if os.path.isfile(mri_p) and os.path.isfile(tumor_p):\n",
        "                    self.patient_ids.append(pid)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patient_ids)\n",
        "\n",
        "    # Min–max normalization\n",
        "    @staticmethod\n",
        "    def _minmax(x):\n",
        "        x = x.astype(np.float32)\n",
        "        mn, mx = np.min(x), np.max(x)\n",
        "        return (\n",
        "            (x - mn) / (mx - mn)\n",
        "            if mx > mn\n",
        "            else np.zeros_like(x, dtype=np.float32)\n",
        "        )\n",
        "\n",
        "    # Load a single patient's MRI + masks\n",
        "    def __getitem__(self, idx):\n",
        "        pid = self.patient_ids[idx]\n",
        "        base = os.path.join(self.data_root, pid)\n",
        "\n",
        "        mri = np.load(os.path.join(base, f\"{pid}_mri.npy\")).astype(np.float32)\n",
        "        tumor = np.load(os.path.join(base, f\"{pid}_tumor.npy\")).astype(\n",
        "            np.float32\n",
        "        )\n",
        "\n",
        "        mri = self._minmax(mri)\n",
        "        tumor = (tumor > 0.5).astype(np.float32)\n",
        "\n",
        "        sample = {\"patient_id\": pid, \"mri\": mri, \"tumor\": tumor}\n",
        "\n",
        "        if self.use_atlas:\n",
        "            regions = np.load(os.path.join(base, f\"{pid}_regions.npy\")).astype(\n",
        "                np.float32\n",
        "            )\n",
        "            regions = self._minmax(regions)\n",
        "            sample[\"regions\"] = regions\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "# Collate function used in DataLoaders\n",
        "def image_only_collate_fn(batch, use_atlas=True):\n",
        "    mri = torch.stack([torch.tensor(it[\"mri\"]) for it in batch]).unsqueeze(1)\n",
        "    y = torch.stack([torch.tensor(it[\"tumor\"]) for it in batch]).unsqueeze(1)\n",
        "\n",
        "    if use_atlas:\n",
        "        regs = torch.stack(\n",
        "            [torch.tensor(it[\"regions\"]) for it in batch]\n",
        "        ).unsqueeze(1)\n",
        "        x = torch.cat([mri.float(), regs.float()], dim=1)\n",
        "    else:\n",
        "        x = mri.float()\n",
        "\n",
        "    pids = [it[\"patient_id\"] for it in batch]\n",
        "    return {\"x\": x, \"y\": y.float(), \"pid\": pids}\n",
        "\n",
        "\n",
        "# Compute tumor presence, area and dominant atlas region\n",
        "def patient_meta(pid: str) -> Tuple[int, float, int]:\n",
        "    base = os.path.join(DATA_ROOT, pid)\n",
        "    tumor = np.load(os.path.join(base, f\"{pid}_tumor.npy\")).astype(np.float32)\n",
        "    mask = tumor > 0.5\n",
        "\n",
        "    area = float(mask.sum())\n",
        "    has_tumor = 1 if area >= 1 else 0\n",
        "\n",
        "    dom_region = -1\n",
        "    reg_path = os.path.join(base, f\"{pid}_regions.npy\")\n",
        "\n",
        "    if USE_ATLAS and os.path.isfile(reg_path) and mask.any():\n",
        "        regs = np.load(reg_path).astype(np.int32)\n",
        "        vals, counts = np.unique(regs[mask], return_counts=True)\n",
        "        if len(vals) > 0:\n",
        "            dom_region = int(vals[np.argmax(counts)])\n",
        "\n",
        "    return has_tumor, area, dom_region\n",
        "\n",
        "\n",
        "# Build metadata for all patients\n",
        "def build_meta_for(dataset: ImageOnlyGliomaDataset) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for pid in dataset.patient_ids:\n",
        "        try:\n",
        "            ht, area, dom = patient_meta(pid)\n",
        "            rows.append({\"pid\": pid, \"has_tumor\": ht, \"area\": area, \"dom\": dom})\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    meta = pd.DataFrame(rows)\n",
        "\n",
        "    # Create tumor size bins\n",
        "    meta[\"size_bin\"] = 0\n",
        "    m = meta[\"has_tumor\"] == 1\n",
        "    if m.any():\n",
        "        areas = meta.loc[m, \"area\"].values\n",
        "        qs = np.quantile(areas, np.linspace(0, 1, 4))\n",
        "        qs = np.unique(qs)\n",
        "        bins = (\n",
        "            np.digitize(areas, qs[1:-1], right=True)\n",
        "            if len(qs) > 2\n",
        "            else np.zeros_like(areas, int)\n",
        "        )\n",
        "        meta.loc[m, \"size_bin\"] = bins.astype(int)\n",
        "\n",
        "    return meta\n",
        "\n",
        "\n",
        "# Load dataset + metadata\n",
        "dataset = ImageOnlyGliomaDataset(\n",
        "    METADATA_DF_PATH,\n",
        "    DATA_ROOT,\n",
        "    use_atlas=USE_ATLAS,\n",
        "    exclude_ids=[\"PatientID_0191\"],\n",
        ")\n",
        "meta = build_meta_for(dataset)\n",
        "rng = np.random.RandomState(SEED)\n",
        "\n",
        "# Group patients by class (tumor size bin)\n",
        "class_to_pids = defaultdict(list)\n",
        "for _, row in meta.iterrows():\n",
        "    label = row[\"size_bin\"]\n",
        "    class_to_pids[label].append(row[\"pid\"])\n",
        "\n",
        "# Allocate patients to clients using Dirichlet sampling\n",
        "client_pid_sets = [[] for _ in range(N_CLIENTS)]\n",
        "\n",
        "for label, pids in class_to_pids.items():\n",
        "    pids = rng.permutation(pids)\n",
        "    proportions = rng.dirichlet([DIRICHLET_ALPHA] * N_CLIENTS)\n",
        "    counts = (proportions * len(pids)).astype(int)\n",
        "\n",
        "    # Adjust counts to cover all samples\n",
        "    while counts.sum() < len(pids):\n",
        "        counts[rng.randint(0, N_CLIENTS)] += 1\n",
        "\n",
        "    start = 0\n",
        "    for cid, count in enumerate(counts):\n",
        "        subset = pids[start : start + count]\n",
        "        client_pid_sets[cid].extend(subset)\n",
        "        start += count\n",
        "\n",
        "# Create clients + train/val splits\n",
        "clients = []\n",
        "client_metas = []\n",
        "\n",
        "for cid in range(N_CLIENTS):\n",
        "    Xc = np.array(client_pid_sets[cid])\n",
        "\n",
        "    rng_c = np.random.RandomState(SEED + cid)\n",
        "    perm_c = rng_c.permutation(len(Xc))\n",
        "\n",
        "    split_at = max(1, int((1.0 - VAL_FRAC_PER_CLIENT) * len(Xc)))\n",
        "    split_at = min(split_at, len(Xc) - 1)\n",
        "\n",
        "    train_pids = Xc[perm_c[:split_at]].tolist()\n",
        "    val_pids = Xc[perm_c[split_at:]].tolist()\n",
        "\n",
        "    meta_c = meta[meta[\"pid\"].isin(Xc)].reset_index(drop=True)\n",
        "\n",
        "    cdir = os.path.join(CLIENT_DIR, f\"client_{cid}\")\n",
        "    os.makedirs(cdir, exist_ok=True)\n",
        "\n",
        "    with open(os.path.join(cdir, \"train_pids.json\"), \"w\") as f:\n",
        "        json.dump(train_pids, f, indent=2)\n",
        "\n",
        "    with open(os.path.join(cdir, \"val_pids.json\"), \"w\") as f:\n",
        "        json.dump(val_pids, f, indent=2)\n",
        "\n",
        "    clients.append({\"train\": train_pids, \"val\": val_pids})\n",
        "    client_metas.append(meta_c)\n",
        "\n",
        "print(\n",
        "    f\"[INFO] Created {N_CLIENTS} non-IID clients using Dirichlet split (alpha={DIRICHLET_ALPHA}).\"\n",
        ")\n",
        "\n",
        "# Save manifest describing split configuration\n",
        "manifest = {\n",
        "    \"seed\": SEED,\n",
        "    \"use_atlas\": USE_ATLAS,\n",
        "    \"n_clients\": N_CLIENTS,\n",
        "    \"val_frac_per_client\": VAL_FRAC_PER_CLIENT,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"num_workers\": NUM_WORKERS,\n",
        "    \"split_type\": \"non-IID_Dirichlet\",\n",
        "    \"dirichlet_alpha\": DIRICHLET_ALPHA,\n",
        "}\n",
        "\n",
        "with open(os.path.join(CLIENT_DIR, \"manifest.json\"), \"w\") as f:\n",
        "    json.dump(manifest, f, indent=2)\n",
        "\n",
        "\n",
        "# Dataset wrapper for selecting specific patient IDs\n",
        "class SubsetByPIDs(Dataset):\n",
        "    def __init__(\n",
        "        self, full_dataset: ImageOnlyGliomaDataset, pid_list: List[str]\n",
        "    ):\n",
        "        self.ds = full_dataset\n",
        "        pid_to_idx = {pid: i for i, pid in enumerate(self.ds.patient_ids)}\n",
        "        self.indices = [pid_to_idx[p] for p in pid_list if p in pid_to_idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.ds[self.indices[i]]\n",
        "\n",
        "\n",
        "# Helper to build DataLoaders\n",
        "def make_loader(ds, shuffle):\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "        collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "        generator=torch.Generator().manual_seed(SEED),\n",
        "    )\n",
        "\n",
        "\n",
        "# Build preview loaders for validation\n",
        "preview_loaders = []\n",
        "for cid, cl in enumerate(clients):\n",
        "    ds_tr = SubsetByPIDs(dataset, cl[\"train\"])\n",
        "    ds_va = SubsetByPIDs(dataset, cl[\"val\"])\n",
        "    ld_tr = make_loader(ds_tr, shuffle=True)\n",
        "    ld_va = make_loader(ds_va, shuffle=False)\n",
        "    preview_loaders.append((ld_tr, ld_va))\n",
        "\n",
        "\n",
        "# Produce metadata summaries\n",
        "def summarize(meta_df: pd.DataFrame, name: str) -> Dict:\n",
        "    s = {\"name\": name, \"n\": int(len(meta_df))}\n",
        "    s[\"has_tumor_counts\"] = meta_df[\"has_tumor\"].value_counts().to_dict()\n",
        "    s[\"size_bin_counts\"] = meta_df[\"size_bin\"].value_counts().to_dict()\n",
        "    s[\"dom_region_top5\"] = meta_df[\"dom\"].value_counts().head(5).to_dict()\n",
        "    return s\n",
        "\n",
        "\n",
        "summary_all = summarize(meta, \"ALL\")\n",
        "print(\"\\n=== Global summary ===\")\n",
        "print(summary_all)\n",
        "\n",
        "per_client_summaries = []\n",
        "for cid, cl in enumerate(clients):\n",
        "    meta_c = client_metas[cid]\n",
        "    tr = meta_c[meta_c[\"pid\"].isin(cl[\"train\"])]\n",
        "    va = meta_c[meta_c[\"pid\"].isin(cl[\"val\"])]\n",
        "\n",
        "    s_client = {\n",
        "        \"client\": cid,\n",
        "        \"train\": summarize(tr, f\"client_{cid}_train\"),\n",
        "        \"val\": summarize(va, f\"client_{cid}_val\"),\n",
        "    }\n",
        "\n",
        "    per_client_summaries.append(s_client)\n",
        "    print(f\"\\n=== Client {cid} ===\")\n",
        "    print(s_client)\n",
        "\n",
        "# Save summaries\n",
        "with open(os.path.join(CLIENT_DIR, \"summary.json\"), \"w\") as f:\n",
        "    json.dump(\n",
        "        {\"global\": summary_all, \"per_client\": per_client_summaries}, f, indent=2\n",
        "    )\n",
        "\n",
        "print(\n",
        "    f\"\\nSaved {N_CLIENTS} non-IID client splits and summaries to: {CLIENT_DIR}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "OCx-KWaBtgbc",
        "outputId": "08ef5461-5c38-415a-ad68-ff7c4938fa14"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "CLIENT_DIR = \"AITDM/client\"\n",
        "summary_path = os.path.join(CLIENT_DIR, \"summary.json\")\n",
        "\n",
        "with open(summary_path, \"r\") as f:\n",
        "    summ = json.load(f)\n",
        "\n",
        "per_client = summ[\"per_client\"]\n",
        "N_CLIENTS = len(per_client)\n",
        "\n",
        "rows = []\n",
        "all_bins = set()\n",
        "\n",
        "for c in per_client:\n",
        "    train_bins = c[\"train\"][\"size_bin_counts\"]\n",
        "    val_bins = c[\"val\"][\"size_bin_counts\"]\n",
        "\n",
        "    merged = {}\n",
        "    for k, v in train_bins.items():\n",
        "        merged[int(k)] = merged.get(int(k), 0) + int(v)\n",
        "    for k, v in val_bins.items():\n",
        "        merged[int(k)] = merged.get(int(k), 0) + int(v)\n",
        "\n",
        "    all_bins |= set(merged.keys())\n",
        "\n",
        "    row = {f\"Bin {b}\": merged.get(b, 0) for b in merged}\n",
        "    row[\"train_n\"] = c[\"train\"][\"n\"]\n",
        "    row[\"val_n\"] = c[\"val\"][\"n\"]\n",
        "    rows.append(row)\n",
        "\n",
        "bins_sorted = sorted(all_bins)\n",
        "\n",
        "df = pd.DataFrame(rows).fillna(0).astype(int)\n",
        "df.index = [f\"Client {i}\" for i in range(N_CLIENTS)]\n",
        "\n",
        "for b in bins_sorted:\n",
        "    col = f\"Bin {b}\"\n",
        "    if col not in df.columns:\n",
        "        df[col] = 0\n",
        "bin_cols = [f\"Bin {b}\" for b in bins_sorted]\n",
        "\n",
        "ax = df[bin_cols].plot(kind=\"bar\", stacked=True, figsize=(10, 5))\n",
        "\n",
        "for i in range(N_CLIENTS):\n",
        "    total_height = df[bin_cols].iloc[i].sum()\n",
        "    y_offset = 0.8\n",
        "\n",
        "    ax.text(\n",
        "        i,\n",
        "        total_height + y_offset,\n",
        "        f\"train={df.loc[df.index[i], 'train_n']}\\nval={df.loc[df.index[i], 'val_n']}\",\n",
        "        ha=\"center\",\n",
        "        va=\"bottom\",\n",
        "        fontsize=9,\n",
        "    )\n",
        "\n",
        "ax.set_title(\"Client Data Distribution\")\n",
        "ax.set_xlabel(\"Client\")\n",
        "ax.set_ylabel(\"Number of samples\")\n",
        "ax.legend(title=\"Tumor size bin\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNV5HyCSpALX",
        "outputId": "326af203-f7c2-4499-8a6d-09f3c4fed4a6"
      },
      "outputs": [],
      "source": [
        "! zip -r client.zip AITDM/client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hy9_4kXHHgr"
      },
      "source": [
        "**Data Distribution**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xm0V8RS9l96o"
      },
      "outputs": [],
      "source": [
        "{\n",
        "    \"global\": {\n",
        "        \"name\": \"ALL\",\n",
        "        \"n\": 202,\n",
        "        \"has_tumor_counts\": {\"1\": 202},\n",
        "        \"size_bin_counts\": {\"0\": 68, \"1\": 67, \"2\": 67},\n",
        "        \"dom_region_top5\": {\"61\": 85, \"50\": 81, \"0\": 10, \"10\": 4, \"22\": 3},\n",
        "    },\n",
        "    \"per_client\": [\n",
        "        {\n",
        "            \"client\": 0,\n",
        "            \"train\": {\n",
        "                \"name\": \"client_0_train\",\n",
        "                \"n\": 35,\n",
        "                \"has_tumor_counts\": {\"1\": 35},\n",
        "                \"size_bin_counts\": {\"1\": 32, \"0\": 2, \"2\": 1},\n",
        "                \"dom_region_top5\": {\n",
        "                    \"50\": 18,\n",
        "                    \"61\": 13,\n",
        "                    \"7\": 1,\n",
        "                    \"62\": 1,\n",
        "                    \"10\": 1,\n",
        "                },\n",
        "            },\n",
        "            \"val\": {\n",
        "                \"name\": \"client_0_val\",\n",
        "                \"n\": 9,\n",
        "                \"has_tumor_counts\": {\"1\": 9},\n",
        "                \"size_bin_counts\": {\"1\": 8, \"0\": 1},\n",
        "                \"dom_region_top5\": {\"50\": 4, \"61\": 3, \"10\": 1, \"58\": 1},\n",
        "            },\n",
        "        },\n",
        "        {\n",
        "            \"client\": 1,\n",
        "            \"train\": {\n",
        "                \"name\": \"client_1_train\",\n",
        "                \"n\": 96,\n",
        "                \"has_tumor_counts\": {\"1\": 96},\n",
        "                \"size_bin_counts\": {\"2\": 49, \"0\": 27, \"1\": 20},\n",
        "                \"dom_region_top5\": {\n",
        "                    \"61\": 47,\n",
        "                    \"50\": 34,\n",
        "                    \"0\": 5,\n",
        "                    \"1\": 1,\n",
        "                    \"23\": 1,\n",
        "                },\n",
        "            },\n",
        "            \"val\": {\n",
        "                \"name\": \"client_1_val\",\n",
        "                \"n\": 25,\n",
        "                \"has_tumor_counts\": {\"1\": 25},\n",
        "                \"size_bin_counts\": {\"2\": 17, \"0\": 4, \"1\": 4},\n",
        "                \"dom_region_top5\": {\"61\": 13, \"50\": 11, \"0\": 1},\n",
        "            },\n",
        "        },\n",
        "        {\n",
        "            \"client\": 2,\n",
        "            \"train\": {\n",
        "                \"name\": \"client_2_train\",\n",
        "                \"n\": 29,\n",
        "                \"has_tumor_counts\": {\"1\": 29},\n",
        "                \"size_bin_counts\": {\"0\": 26, \"1\": 3},\n",
        "                \"dom_region_top5\": {\n",
        "                    \"50\": 12,\n",
        "                    \"61\": 8,\n",
        "                    \"22\": 2,\n",
        "                    \"0\": 2,\n",
        "                    \"31\": 1,\n",
        "                },\n",
        "            },\n",
        "            \"val\": {\n",
        "                \"name\": \"client_2_val\",\n",
        "                \"n\": 8,\n",
        "                \"has_tumor_counts\": {\"1\": 8},\n",
        "                \"size_bin_counts\": {\"0\": 8},\n",
        "                \"dom_region_top5\": {\n",
        "                    \"50\": 2,\n",
        "                    \"61\": 1,\n",
        "                    \"10\": 1,\n",
        "                    \"45\": 1,\n",
        "                    \"22\": 1,\n",
        "                },\n",
        "            },\n",
        "        },\n",
        "    ],\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPU64UV3E4v9"
      },
      "source": [
        "# **M1 - Baseline Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkngkfhZtbvV"
      },
      "source": [
        "<h2>Train a model for each client<h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mF02IxEM22gg"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/client.zip -d /content/client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6hGU2Q4FuQm"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/Preprocessed-Data.zip -d /content/Preprocessed-Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3117f186ea8f4403bba5499a1ffbd3d8",
            "746baebad78a4870904a3e11ae57081a",
            "2f2fea9499654a3ab85ebba399a5eae5",
            "ae7a43193675492c8f5a9d7f0517f73d",
            "81b79dac85eb44aaab0c89b3858fd70a",
            "4955b82bb8ef4cd4a23cf7e213d04b40",
            "d53d9a6f5e344ac08bc684c74dca1a54",
            "7498d427ce53446887252636c149da43",
            "47007b561c284a9ead6056d8d06f81b9",
            "33211f55587948ef8c631c8f5eb5f42d",
            "32424859b7944119b90eed837ce124d7",
            "d156bff5a12b4fd2a402f76fd45ae1a8",
            "c51731484e434818ad01f3ce139af20f",
            "2a7d5d99927547aba220065b78ce3baa",
            "76986cd4b96d49f48560a18268c337d0",
            "1de8b89d5f84448988da5a57ed013b87",
            "f834de8b38b344bab710da19e9765023",
            "2079106bec34430bac8b49b9c58c2b1c",
            "b4f64d81650e494d90b558da50c3c940",
            "d0ed26d8e9434b3495befb420c83050c",
            "190d9c431dad4fe6aeb719c53aae00e3",
            "43835fe3d6d740868ffd71ae83bec2ef"
          ]
        },
        "id": "RKHibbXVvLNG",
        "outputId": "0f1c38af-c06a-44cf-81e1-c5f244eca8ad"
      },
      "outputs": [],
      "source": [
        "import os, json, pickle, random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "from sklearn.metrics import jaccard_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "# Paths and I/O config\n",
        "DATA_ROOT = \"/content/Preprocessed-Data\"\n",
        "METADATA_DF_PATH = \"cleaned_df.pkl\"\n",
        "OUT_BASE = \"AITDM\"\n",
        "CLIENT_DIR = os.path.join(OUT_BASE, \"/content/client\")\n",
        "\n",
        "# Experiment config\n",
        "USE_ATLAS = True\n",
        "CLIENT_ID = 0\n",
        "BATCH_SIZE = 8\n",
        "LR = 1e-3\n",
        "EPOCHS = 50\n",
        "NUM_WORKERS = 2\n",
        "SEED = 42\n",
        "\n",
        "# Model / encoder config\n",
        "ENCODER_NAME = \"resnet34\"\n",
        "ENCODER_WEIGHTS = \"imagenet\"\n",
        "\n",
        "# Output directories\n",
        "OUT_MODELS_DIR = os.path.join(OUT_BASE, \"Models\", \"UNet_ImageOnly\")\n",
        "OUT_GRAPHS_DIR = os.path.join(OUT_BASE, \"Graphs\")\n",
        "os.makedirs(OUT_MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(OUT_GRAPHS_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
        "\n",
        "\n",
        "def worker_init_fn(worker_id):\n",
        "    worker_seed = SEED + worker_id\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "\n",
        "set_seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class ImageOnlyGliomaDataset(Dataset):\n",
        "    \"\"\"Dataset that loads MRI, tumor mask, and optional atlas regions per patient.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        metadata_df_path,\n",
        "        data_root,\n",
        "        use_atlas=True,\n",
        "        exclude_ids=None,\n",
        "        transform=None,\n",
        "    ):\n",
        "        import pickle, os, numpy as np, pandas as pd\n",
        "\n",
        "        # Load metadata dataframe\n",
        "        with open(metadata_df_path, \"rb\") as f:\n",
        "            df = pickle.load(f)\n",
        "\n",
        "        # Optionally exclude some patient IDs\n",
        "        if exclude_ids is None:\n",
        "            exclude_ids = [\"PatientID_0191\"]\n",
        "        self.df = df[~df[\"Patient_ID\"].isin(exclude_ids)].reset_index(drop=True)\n",
        "\n",
        "        self.data_root = data_root\n",
        "        self.use_atlas = use_atlas\n",
        "        self.transform = transform\n",
        "\n",
        "        # Collect patient IDs that have all required files\n",
        "        self.patient_ids = []\n",
        "        for pid in sorted(self.df[\"Patient_ID\"].tolist()):\n",
        "            mri_p = os.path.join(self.data_root, pid, f\"{pid}_mri.npy\")\n",
        "            tumor_p = os.path.join(self.data_root, pid, f\"{pid}_tumor.npy\")\n",
        "\n",
        "            if self.use_atlas:\n",
        "                reg_p = os.path.join(self.data_root, pid, f\"{pid}_regions.npy\")\n",
        "                if (\n",
        "                    os.path.isfile(mri_p)\n",
        "                    and os.path.isfile(tumor_p)\n",
        "                    and os.path.isfile(reg_p)\n",
        "                ):\n",
        "                    self.patient_ids.append(pid)\n",
        "            else:\n",
        "                if os.path.isfile(mri_p) and os.path.isfile(tumor_p):\n",
        "                    self.patient_ids.append(pid)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patient_ids)\n",
        "\n",
        "    @staticmethod\n",
        "    def _minmax(x):\n",
        "        \"\"\"Simple min-max normalization to [0, 1].\"\"\"\n",
        "        x = x.astype(np.float32)\n",
        "        mn, mx = np.min(x), np.max(x)\n",
        "        return (\n",
        "            (x - mn) / (mx - mn)\n",
        "            if mx > mn\n",
        "            else np.zeros_like(x, dtype=np.float32)\n",
        "        )\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Load and return one sample (dict) for a patient.\"\"\"\n",
        "        import os, numpy as np\n",
        "\n",
        "        pid = self.patient_ids[idx]\n",
        "        base = os.path.join(self.data_root, pid)\n",
        "\n",
        "        # Load MRI and tumor mask\n",
        "        mri = np.load(os.path.join(base, f\"{pid}_mri.npy\")).astype(np.float32)\n",
        "        tumor = np.load(os.path.join(base, f\"{pid}_tumor.npy\")).astype(\n",
        "            np.float32\n",
        "        )\n",
        "\n",
        "        # Normalize MRI and binarize tumor mask\n",
        "        mri = self._minmax(mri)\n",
        "        tumor = (tumor > 0.5).astype(np.float32)\n",
        "\n",
        "        sample = {\"patient_id\": pid, \"mri\": mri, \"tumor\": tumor}\n",
        "\n",
        "        # Optionally load regions/atlas\n",
        "        if self.use_atlas:\n",
        "            regions = np.load(os.path.join(base, f\"{pid}_regions.npy\")).astype(\n",
        "                np.float32\n",
        "            )\n",
        "            regions = self._minmax(regions)\n",
        "            sample[\"regions\"] = regions\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "def image_only_collate_fn(batch, use_atlas=True):\n",
        "    \"\"\"Custom collate: stack MRI (+ optional regions) and tumor into tensors.\"\"\"\n",
        "    mri = torch.stack([torch.tensor(it[\"mri\"]) for it in batch]).unsqueeze(1)\n",
        "    y = torch.stack([torch.tensor(it[\"tumor\"]) for it in batch]).unsqueeze(1)\n",
        "\n",
        "    if use_atlas:\n",
        "        regs = torch.stack(\n",
        "            [torch.tensor(it[\"regions\"]) for it in batch]\n",
        "        ).unsqueeze(1)\n",
        "        # Concatenate MRI and regions as channels\n",
        "        x = torch.cat([mri.float(), regs.float()], dim=1)\n",
        "    else:\n",
        "        x = mri.float()\n",
        "\n",
        "    return {\"x\": x, \"y\": y.float(), \"pid\": [it[\"patient_id\"] for it in batch]}\n",
        "\n",
        "\n",
        "class SubsetByPIDs(Dataset):\n",
        "    \"\"\"Wrap a dataset but keep only a subset of patient IDs.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, full_dataset: ImageOnlyGliomaDataset, pid_list: List[str]\n",
        "    ):\n",
        "        self.ds = full_dataset\n",
        "        # Map patient IDs to indices\n",
        "        pid_to_idx = {pid: i for i, pid in enumerate(self.ds.patient_ids)}\n",
        "        self.indices = [pid_to_idx[p] for p in pid_list if p in pid_to_idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.ds[self.indices[i]]\n",
        "\n",
        "\n",
        "# Load client-specific train/val patient IDs\n",
        "cdir = os.path.join(CLIENT_DIR, f\"client_{CLIENT_ID}\")\n",
        "with open(os.path.join(cdir, \"train_pids.json\"), \"r\") as f:\n",
        "    train_pids = json.load(f)\n",
        "with open(os.path.join(cdir, \"val_pids.json\"), \"r\") as f:\n",
        "    val_pids = json.load(f)\n",
        "\n",
        "# Build full dataset and then client subsets\n",
        "full_ds = ImageOnlyGliomaDataset(\n",
        "    METADATA_DF_PATH,\n",
        "    DATA_ROOT,\n",
        "    use_atlas=USE_ATLAS,\n",
        "    exclude_ids=[\"PatientID_0191\"],\n",
        ")\n",
        "train_dataset = SubsetByPIDs(full_ds, train_pids)\n",
        "val_dataset = SubsetByPIDs(full_ds, val_pids)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "    generator=torch.Generator().manual_seed(SEED),\n",
        "    worker_init_fn=worker_init_fn,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "    generator=torch.Generator().manual_seed(SEED),\n",
        "    worker_init_fn=worker_init_fn,\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"Loaded client_{CLIENT_ID}: train patients={len(train_dataset)}, val patients={len(val_dataset)}\"\n",
        ")\n",
        "\n",
        "# UNet model (from segmentation_models_pytorch)\n",
        "in_channels = 2 if USE_ATLAS else 1\n",
        "model = smp.Unet(\n",
        "    encoder_name=ENCODER_NAME,\n",
        "    encoder_weights=ENCODER_WEIGHTS,\n",
        "    in_channels=in_channels,\n",
        "    classes=1,\n",
        ").to(device)\n",
        "\n",
        "# Loss, optimizer, scheduler, and AMP scaler\n",
        "criterion = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
        "scheduler = optim.lr_scheduler.LinearLR(\n",
        "    optimizer, start_factor=1.0, end_factor=0.0, total_iters=EPOCHS\n",
        ")\n",
        "scaler = torch.amp.GradScaler(\"cuda\", enabled=(device.type == \"cuda\"))\n",
        "\n",
        "\n",
        "def calc_metrics(y_true, y_pred):\n",
        "    \"\"\"Compute Dice, IoU, and pixel accuracy.\"\"\"\n",
        "    y_true = y_true.astype(np.uint8).reshape(-1)\n",
        "    y_pred = y_pred.astype(np.uint8).reshape(-1)\n",
        "\n",
        "    inter = (y_true * y_pred).sum()\n",
        "    dice = (2.0 * inter) / (y_true.sum() + y_pred.sum() + 1e-8)\n",
        "\n",
        "    # IoU with sklearn, fallback if it fails\n",
        "    try:\n",
        "        iou = jaccard_score(y_true, y_pred, average=\"binary\")\n",
        "    except Exception:\n",
        "        union = y_true.sum() + y_pred.sum() - inter + 1e-8\n",
        "        iou = inter / union\n",
        "\n",
        "    acc = (y_true == y_pred).mean()\n",
        "    return float(dice), float(iou), float(acc)\n",
        "\n",
        "\n",
        "def plot_prediction(sample_imgs, save_path):\n",
        "    \"\"\"Plot MRI (+ optional regions), GT, and prediction for one batch.\"\"\"\n",
        "    if \"regions\" in sample_imgs:\n",
        "        fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
        "    else:\n",
        "        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # MRI\n",
        "    axs[0].imshow(sample_imgs[\"mri\"][0, 0], cmap=\"gray\")\n",
        "    axs[0].set_title(\"MRI\")\n",
        "    axs[0].axis(\"off\")\n",
        "\n",
        "    col = 1\n",
        "\n",
        "    # Regions (if available)\n",
        "    if \"regions\" in sample_imgs:\n",
        "        axs[col].imshow(sample_imgs[\"regions\"][0, 0], cmap=\"gray\")\n",
        "        axs[col].set_title(\"Regions\")\n",
        "        axs[col].axis(\"off\")\n",
        "        col += 1\n",
        "\n",
        "    # Ground truth\n",
        "    axs[col].imshow(sample_imgs[\"y_true\"][0, 0], cmap=\"gray\")\n",
        "    axs[col].set_title(\"Ground Truth\")\n",
        "    axs[col].axis(\"off\")\n",
        "    col += 1\n",
        "\n",
        "    # Prediction\n",
        "    axs[col].imshow(sample_imgs[\"y_pred\"][0, 0], cmap=\"gray\")\n",
        "    axs[col].set_title(\"Predicted\")\n",
        "    axs[col].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_metrics(history, save_path):\n",
        "    \"\"\"Plot training and validation curves for loss and metrics.\"\"\"\n",
        "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
        "\n",
        "    plt.figure(figsize=(16, 12))\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\")\n",
        "    plt.plot(epochs, history[\"val_loss\"], label=\"Val Loss\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Loss\")\n",
        "\n",
        "    # Dice\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(epochs, history[\"train_dice\"], label=\"Train Dice\")\n",
        "    plt.plot(epochs, history[\"val_dice\"], label=\"Val Dice\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Dice\")\n",
        "\n",
        "    # IoU\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(epochs, history[\"train_iou\"], label=\"Train IoU\")\n",
        "    plt.plot(epochs, history[\"val_iou\"], label=\"Val IoU\")\n",
        "    plt.legend()\n",
        "    plt.title(\"IoU\")\n",
        "\n",
        "    # Accuracy\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(epochs, history[\"train_acc\"], label=\"Train Acc\")\n",
        "    plt.plot(epochs, history[\"val_acc\"], label=\"Val Acc\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Accuracy\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion, scaler):\n",
        "    \"\"\"One training epoch over the dataloader.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = total_dice = total_iou = total_acc = 0.0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        x = batch[\"x\"].to(device)\n",
        "        y = batch[\"y\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Mixed precision forward pass\n",
        "        with torch.amp.autocast(\"cuda\", enabled=(device.type == \"cuda\")):\n",
        "            preds = model(x)\n",
        "            loss = criterion(preds, y)\n",
        "\n",
        "        # Backward with gradient scaling\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "        # Convert predictions to binary and compute metrics\n",
        "        preds_prob = (torch.sigmoid(preds).detach().cpu().numpy() > 0.5).astype(\n",
        "            np.uint8\n",
        "        )\n",
        "        y_np = (y.detach().cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        d, i, a = calc_metrics(y_np, preds_prob)\n",
        "\n",
        "        total_dice += d\n",
        "        total_iou += i\n",
        "        total_acc += a\n",
        "\n",
        "    n = len(dataloader)\n",
        "    return total_loss / n, total_dice / n, total_iou / n, total_acc / n\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_one_epoch(model, dataloader, criterion, keep_last_batch=True):\n",
        "    \"\"\"Evaluation loop over the validation dataloader.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = total_dice = total_iou = total_acc = 0.0\n",
        "    last_batch_imgs = None\n",
        "\n",
        "    for batch in dataloader:\n",
        "        x = batch[\"x\"].to(device)\n",
        "        y = batch[\"y\"].to(device)\n",
        "\n",
        "        preds = model(x)\n",
        "        loss = criterion(preds, y)\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "        preds_prob = (torch.sigmoid(preds).cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        y_np = (y.cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        d, i, a = calc_metrics(y_np, preds_prob)\n",
        "\n",
        "        total_dice += d\n",
        "        total_iou += i\n",
        "        total_acc += a\n",
        "\n",
        "        # Optionally keep last batch for visualization\n",
        "        if keep_last_batch:\n",
        "            ch0 = x[:, 0:1].cpu().numpy()\n",
        "            imgs = {\"mri\": ch0, \"y_true\": y.cpu().numpy(), \"y_pred\": preds_prob}\n",
        "            if x.shape[1] == 2:\n",
        "                ch1 = x[:, 1:2].cpu().numpy()\n",
        "                imgs[\"regions\"] = ch1\n",
        "            last_batch_imgs = imgs\n",
        "\n",
        "    n = len(dataloader)\n",
        "    return (\n",
        "        total_loss / n,\n",
        "        total_dice / n,\n",
        "        total_iou / n,\n",
        "        total_acc / n,\n",
        "        last_batch_imgs,\n",
        "    )\n",
        "\n",
        "\n",
        "def infer_and_visualize_best(\n",
        "    model,\n",
        "    val_dataset,\n",
        "    use_atlas: bool,\n",
        "    out_dir: str,\n",
        "    client_id: int,\n",
        "    best_ckpt_path: str,\n",
        "    k_samples: int = 3,\n",
        "    threshold: float = 0.5,\n",
        "):\n",
        "    \"\"\"Load best checkpoint, run inference on a few validation samples, and save visualizations.\"\"\"\n",
        "    import random\n",
        "\n",
        "    # Make sampling deterministic\n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    if os.path.isfile(best_ckpt_path):\n",
        "        model.load_state_dict(torch.load(best_ckpt_path, map_location=device))\n",
        "        model.eval()\n",
        "        print(f\"[Best Model] Loaded: {best_ckpt_path}\\n\")\n",
        "    else:\n",
        "        print(f\"[Best Model] Missing checkpoint: {best_ckpt_path}\\n\")\n",
        "        return\n",
        "\n",
        "    k = min(k_samples, len(val_dataset))\n",
        "    if k == 0:\n",
        "        print(\"[Best Model] Empty val dataset.\\n\")\n",
        "        return\n",
        "\n",
        "    # Deterministic random subset of validation indices\n",
        "    idxs = random.sample(range(len(val_dataset)), k)\n",
        "\n",
        "    def _predict_one(sample):\n",
        "        \"\"\"Run model on one sample and return inputs and binarized prediction.\"\"\"\n",
        "        mri = (\n",
        "            torch.tensor(sample[\"mri\"])\n",
        "            .unsqueeze(0)\n",
        "            .unsqueeze(0)\n",
        "            .float()\n",
        "            .to(device)\n",
        "        )\n",
        "\n",
        "        if use_atlas and (\"regions\" in sample):\n",
        "            regs = (\n",
        "                torch.tensor(sample[\"regions\"])\n",
        "                .unsqueeze(0)\n",
        "                .unsqueeze(0)\n",
        "                .float()\n",
        "                .to(device)\n",
        "            )\n",
        "            x = torch.cat([mri, regs], dim=1)\n",
        "        else:\n",
        "            x = mri\n",
        "\n",
        "        with torch.no_grad():\n",
        "            prob = torch.sigmoid(model(x)).cpu().numpy()\n",
        "            pred_bin = (prob > threshold).astype(np.uint8)\n",
        "\n",
        "        return x.cpu().numpy(), pred_bin\n",
        "\n",
        "    saved_paths = []\n",
        "\n",
        "    # Save individual sample figures\n",
        "    for i, idx in enumerate(idxs, 1):\n",
        "        sample = val_dataset[idx]\n",
        "        pid = sample.get(\"patient_id\", f\"val_{idx}\")\n",
        "        x_np, pred_bin = _predict_one(sample)\n",
        "\n",
        "        imgs = {\n",
        "            \"mri\": x_np[:, 0:1],\n",
        "            \"y_true\": np.expand_dims(\n",
        "                np.expand_dims(sample[\"tumor\"], 0), 0\n",
        "            ).astype(np.float32),\n",
        "            \"y_pred\": pred_bin.astype(np.float32),\n",
        "        }\n",
        "\n",
        "        if use_atlas and (\"regions\" in sample):\n",
        "            imgs[\"regions\"] = x_np[:, 1:2]\n",
        "\n",
        "        out_path = os.path.join(\n",
        "            out_dir, f\"best_val_sample_{i}_client{client_id}_{pid}.png\"\n",
        "        )\n",
        "        plot_prediction(imgs, out_path)\n",
        "        saved_paths.append(out_path)\n",
        "\n",
        "    print(\"[Best Model] Saved individual figures:\")\n",
        "    for p in saved_paths:\n",
        "        print(\" -\", p)\n",
        "    print()\n",
        "\n",
        "    # Save grid figure\n",
        "    cols = 4 if use_atlas else 3\n",
        "    fig, axs = plt.subplots(k, cols, figsize=(5 * cols, 4 * k))\n",
        "\n",
        "    if k == 1:\n",
        "        axs = np.expand_dims(axs, 0)\n",
        "\n",
        "    for row, idx in enumerate(idxs):\n",
        "        sample = val_dataset[idx]\n",
        "        x_np, pred_bin = _predict_one(sample)\n",
        "\n",
        "        mri = x_np[0, 0]\n",
        "        gt = sample[\"tumor\"]\n",
        "        col = 0\n",
        "\n",
        "        axs[row, col].imshow(mri, cmap=\"gray\")\n",
        "        axs[row, col].set_title(\"MRI\")\n",
        "        axs[row, col].axis(\"off\")\n",
        "        col += 1\n",
        "\n",
        "        if use_atlas and (\"regions\" in sample):\n",
        "            regs = x_np[0, 1]\n",
        "            axs[row, col].imshow(regs, cmap=\"gray\")\n",
        "            axs[row, col].set_title(\"Regions\")\n",
        "            axs[row, col].axis(\"off\")\n",
        "            col += 1\n",
        "\n",
        "        axs[row, col].imshow(gt, cmap=\"gray\")\n",
        "        axs[row, col].set_title(\"Ground Truth\")\n",
        "        axs[row, col].axis(\"off\")\n",
        "        col += 1\n",
        "\n",
        "        axs[row, col].imshow(pred_bin[0, 0], cmap=\"gray\")\n",
        "        axs[row, col].set_title(\"Predicted (τ=0.5)\")\n",
        "        axs[row, col].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    grid_path = os.path.join(\n",
        "        out_dir, f\"best_model_val_grid_client{client_id}.png\"\n",
        "    )\n",
        "    plt.savefig(grid_path, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    print(f\"[Best Model] Saved grid -> {grid_path}\\n\")\n",
        "\n",
        "\n",
        "# History containers for training curves\n",
        "history = {\n",
        "    \"train_loss\": [],\n",
        "    \"train_dice\": [],\n",
        "    \"train_iou\": [],\n",
        "    \"train_acc\": [],\n",
        "    \"val_loss\": [],\n",
        "    \"val_dice\": [],\n",
        "    \"val_iou\": [],\n",
        "    \"val_acc\": [],\n",
        "}\n",
        "\n",
        "# Tracking best validation metrics\n",
        "best_val_iou = 0.0\n",
        "best_val_dice = -float(\"inf\")\n",
        "best_val_loss = float(\"inf\")\n",
        "best_path = os.path.join(OUT_MODELS_DIR, f\"best_unet_client{CLIENT_ID}.pth\")\n",
        "\n",
        "log_rows = []\n",
        "\n",
        "# Main training loop\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    # Train one epoch\n",
        "    trL, trD, trI, trA = train_one_epoch(\n",
        "        model, train_loader, optimizer, criterion, scaler\n",
        "    )\n",
        "    # Validate\n",
        "    vaL, vaD, vaI, vaA, _ = eval_one_epoch(\n",
        "        model, val_loader, criterion, keep_last_batch=True\n",
        "    )\n",
        "\n",
        "    # Update scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    # Save metrics for plots\n",
        "    history[\"train_loss\"].append(trL)\n",
        "    history[\"train_dice\"].append(trD)\n",
        "    history[\"train_iou\"].append(trI)\n",
        "    history[\"train_acc\"].append(trA)\n",
        "\n",
        "    history[\"val_loss\"].append(vaL)\n",
        "    history[\"val_dice\"].append(vaD)\n",
        "    history[\"val_iou\"].append(vaI)\n",
        "    history[\"val_acc\"].append(vaA)\n",
        "\n",
        "    print(\n",
        "        f\"[Epoch {epoch:03d}/{EPOCHS}] \"\n",
        "        f\"Train — Loss {trL:.4f} | Dice {trD:.4f} | IoU {trI:.4f} | Accuracy {trA:.4f} || \"\n",
        "        f\"Val — Loss {vaL:.4f} | Dice {vaD:.4f} | IoU {vaI:.4f} | Accuracy {vaA:.4f}\\n\"\n",
        "    )\n",
        "\n",
        "    # Save checkpoint if Dice improves and loss decreases\n",
        "    saved_ckpt = False\n",
        "    if (vaD > best_val_dice) and (vaL < best_val_loss):\n",
        "        best_val_dice = vaD\n",
        "        best_val_loss = vaL\n",
        "        torch.save(model.state_dict(), best_path)\n",
        "        saved_ckpt = True\n",
        "        print(\n",
        "            f\"Saved best model (Val Dice↑ {best_val_dice:.4f} & Val Loss↓ {best_val_loss:.4f}) -> {best_path}\\n\"\n",
        "        )\n",
        "\n",
        "    # Log row for CSV\n",
        "    log_rows.append(\n",
        "        {\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": trL,\n",
        "            \"train_dice\": trD,\n",
        "            \"train_iou\": trI,\n",
        "            \"train_acc\": trA,\n",
        "            \"val_loss\": vaL,\n",
        "            \"val_dice\": vaD,\n",
        "            \"val_iou\": vaI,\n",
        "            \"val_acc\": vaA,\n",
        "            \"saved_ckpt\": saved_ckpt,\n",
        "            \"marker\": \"X\" if saved_ckpt else \"\",\n",
        "        }\n",
        "    )\n",
        "\n",
        "# Save metrics as CSV\n",
        "metrics_csv = os.path.join(OUT_GRAPHS_DIR, f\"metrics_client{CLIENT_ID}.csv\")\n",
        "pd.DataFrame(log_rows).to_csv(metrics_csv, index=False)\n",
        "print(f\"[Log] Wrote per-epoch metrics CSV -> {metrics_csv}\\n\")\n",
        "\n",
        "# Plot training curves\n",
        "plot_metrics(\n",
        "    history,\n",
        "    os.path.join(OUT_GRAPHS_DIR, f\"training_curves_client{CLIENT_ID}.png\"),\n",
        ")\n",
        "\n",
        "# Run inference with best model and visualize a few validation samples\n",
        "infer_and_visualize_best(\n",
        "    model=model,\n",
        "    val_dataset=val_dataset,\n",
        "    use_atlas=USE_ATLAS,\n",
        "    out_dir=OUT_GRAPHS_DIR,\n",
        "    client_id=CLIENT_ID,\n",
        "    best_ckpt_path=best_path,\n",
        "    k_samples=3,\n",
        "    threshold=0.5,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeATNHiwE86m"
      },
      "source": [
        "# **M1 - Federated Learning Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0BCxaFfYUhz"
      },
      "source": [
        "<h2>Federated Experiment<h2>\n",
        "\n",
        "**with Flower**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bU6gtvgwWuNm",
        "outputId": "fd6ca049-b9d0-4853-f035-4d17061a32d2"
      },
      "outputs": [],
      "source": [
        "%%writefile seg_data.py\n",
        "import os, pickle, numpy as np, torch\n",
        "from typing import List\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Global paths and configuration\n",
        "DATA_ROOT = \"/content/Preprocessed-Data\"\n",
        "METADATA_DF_PATH = \"cleaned_df.pkl\"\n",
        "USE_ATLAS = True\n",
        "EXCLUDE_IDS = [\"PatientID_0191\"]\n",
        "\n",
        "\n",
        "# Dataset that loads MRI, tumor mask and optional atlas for each patient\n",
        "class ImageOnlyGliomaDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        metadata_df_path=METADATA_DF_PATH,\n",
        "        data_root=DATA_ROOT,\n",
        "        use_atlas=USE_ATLAS,\n",
        "        exclude_ids=None,\n",
        "        transform=None,\n",
        "    ):\n",
        "        with open(metadata_df_path, \"rb\") as f:\n",
        "            df = pickle.load(f)\n",
        "\n",
        "        # Optionally exclude specific patients\n",
        "        if exclude_ids is None:\n",
        "            exclude_ids = EXCLUDE_IDS\n",
        "\n",
        "        # Keep only non-excluded patient rows\n",
        "        self.df = df[~df[\"Patient_ID\"].isin(exclude_ids)].reset_index(drop=True)\n",
        "        self.data_root = data_root\n",
        "        self.use_atlas = use_atlas\n",
        "        self.transform = transform\n",
        "\n",
        "        # Collect patient IDs that have all required .npy files\n",
        "        self.patient_ids = []\n",
        "        for pid in sorted(self.df[\"Patient_ID\"].tolist()):\n",
        "            base = os.path.join(self.data_root, pid)\n",
        "            mri_p = os.path.join(base, f\"{pid}_mri.npy\")\n",
        "            tumor_p = os.path.join(base, f\"{pid}_tumor.npy\")\n",
        "            if self.use_atlas:\n",
        "                reg_p = os.path.join(base, f\"{pid}_regions.npy\")\n",
        "                ok = (\n",
        "                    os.path.isfile(mri_p)\n",
        "                    and os.path.isfile(tumor_p)\n",
        "                    and os.path.isfile(reg_p)\n",
        "                )\n",
        "            else:\n",
        "                ok = os.path.isfile(mri_p) and os.path.isfile(tumor_p)\n",
        "            if ok:\n",
        "                self.patient_ids.append(pid)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patient_ids)\n",
        "\n",
        "    # Simple min–max normalization\n",
        "    @staticmethod\n",
        "    def _minmax(x):\n",
        "        x = x.astype(np.float32)\n",
        "        mn, mx = np.min(x), np.max(x)\n",
        "        return (\n",
        "            (x - mn) / (mx - mn)\n",
        "            if mx > mn\n",
        "            else np.zeros_like(x, dtype=np.float32)\n",
        "        )\n",
        "\n",
        "    # Load a single sample: MRI, tumor mask, and optional atlas\n",
        "    def __getitem__(self, idx):\n",
        "        pid = self.patient_ids[idx]\n",
        "        base = os.path.join(self.data_root, pid)\n",
        "\n",
        "        mri = np.load(os.path.join(base, f\"{pid}_mri.npy\")).astype(np.float32)\n",
        "        tumor = np.load(os.path.join(base, f\"{pid}_tumor.npy\")).astype(\n",
        "            np.float32\n",
        "        )\n",
        "\n",
        "        mri = self._minmax(mri)\n",
        "        tumor = (tumor > 0.5).astype(np.float32)\n",
        "\n",
        "        sample = {\"patient_id\": pid, \"mri\": mri, \"tumor\": tumor}\n",
        "\n",
        "        if self.use_atlas:\n",
        "            regions = np.load(os.path.join(base, f\"{pid}_regions.npy\")).astype(\n",
        "                np.float32\n",
        "            )\n",
        "            regions = self._minmax(regions)\n",
        "            sample[\"regions\"] = regions\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "# Collate function to build batched tensors and patient ID list\n",
        "def image_only_collate_fn(batch, use_atlas=USE_ATLAS):\n",
        "    mri = torch.stack([torch.tensor(it[\"mri\"]) for it in batch]).unsqueeze(1)\n",
        "    y = torch.stack([torch.tensor(it[\"tumor\"]) for it in batch]).unsqueeze(1)\n",
        "\n",
        "    if use_atlas:\n",
        "        regs = torch.stack(\n",
        "            [torch.tensor(it[\"regions\"]) for it in batch]\n",
        "        ).unsqueeze(1)\n",
        "        x = torch.cat([mri.float(), regs.float()], dim=1)\n",
        "    else:\n",
        "        x = mri.float()\n",
        "\n",
        "    return {\"x\": x, \"y\": y.float(), \"pid\": [it[\"patient_id\"] for it in batch]}\n",
        "\n",
        "\n",
        "# Dataset wrapper that restricts to a subset of patient IDs\n",
        "class SubsetByPIDs(Dataset):\n",
        "    def __init__(\n",
        "        self, full_dataset: ImageOnlyGliomaDataset, pid_list: List[str]\n",
        "    ):\n",
        "        self.ds = full_dataset\n",
        "        pid_to_idx = {pid: i for i, pid in enumerate(self.ds.patient_ids)}\n",
        "        self.indices = [pid_to_idx[p] for p in pid_list if p in pid_to_idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.ds[self.indices[i]]\n",
        "\n",
        "\n",
        "# Compute Dice, IoU and accuracy for binary masks\n",
        "def calc_metrics(y_true, y_pred):\n",
        "    y_true = y_true.astype(np.uint8).reshape(-1)\n",
        "    y_pred = y_pred.astype(np.uint8).reshape(-1)\n",
        "\n",
        "    inter = (y_true & y_pred).sum()\n",
        "    dice = (2.0 * inter) / (y_true.sum() + y_pred.sum() + 1e-8)\n",
        "    union = y_true.sum() + y_pred.sum() - inter + 1e-8\n",
        "    iou = inter / union\n",
        "    acc = (y_true == y_pred).mean()\n",
        "\n",
        "    return float(dice), float(iou), float(acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFmBJsv8Fr23",
        "outputId": "d601255e-d12a-4ae7-8ff3-c6b4cf34a368"
      },
      "outputs": [],
      "source": [
        "%%writefile fl_client.py\n",
        "import argparse, os, json, numpy as np, torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import flwr as fl\n",
        "import copy\n",
        "import segmentation_models_pytorch as smp\n",
        "from seg_data import (\n",
        "    ImageOnlyGliomaDataset,\n",
        "    SubsetByPIDs,\n",
        "    image_only_collate_fn,\n",
        "    calc_metrics,\n",
        "    DATA_ROOT,\n",
        "    METADATA_DF_PATH,\n",
        "    USE_ATLAS,\n",
        ")\n",
        "\n",
        "# Base directory for client data splits\n",
        "CLIENT_DIR = \"/content/client\"\n",
        "\n",
        "# Data/loading config\n",
        "BATCH_SIZE = 8\n",
        "NUM_WORKERS = 2\n",
        "SEED = 42\n",
        "\n",
        "# Device and model config\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "ENCODER_NAME = \"resnet34\"\n",
        "ENCODER_WEIGHTS = \"imagenet\"\n",
        "\n",
        "# Local checkpoint directory (per client best models)\n",
        "CKPT_DIR = os.path.join(\"AITDM\", \"checkpoints\")\n",
        "os.makedirs(CKPT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "def get_model():\n",
        "    \"\"\"Create and return a UNet segmentation model.\"\"\"\n",
        "    in_ch = 2 if USE_ATLAS else 1\n",
        "    model = smp.Unet(\n",
        "        encoder_name=ENCODER_NAME,\n",
        "        encoder_weights=ENCODER_WEIGHTS,\n",
        "        in_channels=in_ch,\n",
        "        classes=1,\n",
        "    )\n",
        "    return model.to(DEVICE)\n",
        "\n",
        "\n",
        "def get_loaders(cid: int):\n",
        "    \"\"\"Build train/val dataloaders for a given client ID.\"\"\"\n",
        "    full = ImageOnlyGliomaDataset(\n",
        "        METADATA_DF_PATH,\n",
        "        DATA_ROOT,\n",
        "        use_atlas=USE_ATLAS,\n",
        "        exclude_ids=[\"PatientID_0191\"],\n",
        "    )\n",
        "\n",
        "    # Load client-specific patient IDs\n",
        "    with open(\n",
        "        os.path.join(CLIENT_DIR, f\"client_{cid}\", \"train_pids.json\")\n",
        "    ) as f:\n",
        "        tr_p = json.load(f)\n",
        "    with open(os.path.join(CLIENT_DIR, f\"client_{cid}\", \"val_pids.json\")) as f:\n",
        "        va_p = json.load(f)\n",
        "\n",
        "    g = torch.Generator().manual_seed(SEED)\n",
        "\n",
        "    # Subset datasets\n",
        "    ds_tr = SubsetByPIDs(full, tr_p)\n",
        "    ds_va = SubsetByPIDs(full, va_p)\n",
        "\n",
        "    # Train loader\n",
        "    ld_tr = DataLoader(\n",
        "        ds_tr,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "        collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "        generator=g,\n",
        "    )\n",
        "\n",
        "    # Validation loader\n",
        "    ld_va = DataLoader(\n",
        "        ds_va,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "        collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "        generator=g,\n",
        "    )\n",
        "\n",
        "    return ld_tr, ld_va, len(ds_tr), len(ds_va)\n",
        "\n",
        "\n",
        "def get_parameters(model):\n",
        "    \"\"\"Convert model parameters to a list of NumPy arrays (for Flower).\"\"\"\n",
        "    return [p.detach().cpu().numpy() for _, p in model.state_dict().items()]\n",
        "\n",
        "\n",
        "def set_parameters(model, params):\n",
        "    \"\"\"Load model parameters from a list of NumPy arrays (from Flower).\"\"\"\n",
        "    sd = model.state_dict()\n",
        "    for k, v in zip(sd.keys(), params):\n",
        "        sd[k] = torch.tensor(v)\n",
        "    model.load_state_dict(sd, strict=True)\n",
        "\n",
        "\n",
        "# Loss functions used for combined criterion\n",
        "bce = nn.BCEWithLogitsLoss()\n",
        "dice_loss = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
        "\n",
        "\n",
        "def criterion(pred, y):\n",
        "    \"\"\"Hybrid loss: BCE + Dice.\"\"\"\n",
        "    return 0.5 * bce(pred, y) + 0.5 * dice_loss(pred, y)\n",
        "\n",
        "\n",
        "def maybe_save_best(cid, val_loss, val_dice, best_epoch, rnd, model):\n",
        "    \"\"\"Save best local model (per client) based on validation loss and Dice.\"\"\"\n",
        "    best_json = os.path.join(CKPT_DIR, f\"client_{cid}_best.json\")\n",
        "    best_pt = os.path.join(CKPT_DIR, f\"client_{cid}_best.pt\")\n",
        "\n",
        "    # Default previous best values\n",
        "    prev = {\"val_loss\": float(\"inf\"), \"val_dice\": -1.0}\n",
        "    if os.path.isfile(best_json):\n",
        "        try:\n",
        "            with open(best_json, \"r\") as f:\n",
        "                prev = json.load(f)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Check if current model improved both loss and Dice\n",
        "    improved = (val_loss < prev.get(\"val_loss\", float(\"inf\"))) and (\n",
        "        val_dice > prev.get(\"val_dice\", -1.0)\n",
        "    )\n",
        "\n",
        "    if improved:\n",
        "        # Save state dict and metadata\n",
        "        torch.save(model.state_dict(), best_pt)\n",
        "        with open(best_json, \"w\") as f:\n",
        "            json.dump(\n",
        "                {\n",
        "                    \"round\": int(rnd),\n",
        "                    \"epoch\": int(best_epoch),\n",
        "                    \"val_loss\": float(val_loss),\n",
        "                    \"val_dice\": float(val_dice),\n",
        "                },\n",
        "                f,\n",
        "            )\n",
        "\n",
        "\n",
        "class SegClient(fl.client.NumPyClient):\n",
        "    \"\"\"Flower NumPyClient for federated glioma segmentation.\"\"\"\n",
        "\n",
        "    def __init__(self, cid: int):\n",
        "        self.cid = cid\n",
        "        self.model = get_model()\n",
        "        self.train_loader, self.val_loader, self.ntr, self.nva = get_loaders(\n",
        "            cid\n",
        "        )\n",
        "\n",
        "    def get_parameters(self, config):\n",
        "        \"\"\"Return current local model parameters.\"\"\"\n",
        "        return get_parameters(self.model)\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        \"\"\"Local training for a number of epochs, then return updated parameters and metrics.\"\"\"\n",
        "        # Load global parameters\n",
        "        set_parameters(self.model, parameters)\n",
        "\n",
        "        # Read fit configuration from server\n",
        "        epochs = int(config.get(\"local_epochs\", 1))\n",
        "        lr = float(config.get(\"lr\", 1e-3))\n",
        "        rnd = int(config.get(\"round\", 0))\n",
        "\n",
        "        # Optimizer and AMP scaler\n",
        "        opt = optim.AdamW(self.model.parameters(), lr=lr)\n",
        "        scaler = torch.amp.GradScaler(\"cuda\", enabled=(DEVICE.type == \"cuda\"))\n",
        "\n",
        "        # Track best validation performance\n",
        "        best_state = None\n",
        "        best_val_loss = float(\"inf\")\n",
        "        best_val_dice = -1.0\n",
        "        best_epoch_idx = -1\n",
        "\n",
        "        # Track best train metrics for that epoch (optional, for debugging)\n",
        "        best_train_loss = float(\"inf\")\n",
        "        best_train_dice = -1.0\n",
        "        best_train_iou = 0.0\n",
        "        best_train_acc = 0.0\n",
        "\n",
        "        # Per-epoch logs (will be sent to server)\n",
        "        epoch_logs = []\n",
        "\n",
        "        for epoch_idx in range(1, epochs + 1):\n",
        "            # ----- Training phase -----\n",
        "            self.model.train()\n",
        "            tot_tr_loss = tot_tr_d = tot_tr_i = tot_tr_a = 0.0\n",
        "            nb_tr = 0\n",
        "\n",
        "            for batch in self.train_loader:\n",
        "                x = batch[\"x\"].to(DEVICE)\n",
        "                y = batch[\"y\"].to(DEVICE)\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "\n",
        "                # Forward pass with mixed precision\n",
        "                with torch.amp.autocast(\n",
        "                    \"cuda\", enabled=(DEVICE.type == \"cuda\")\n",
        "                ):\n",
        "                    pred = self.model(x)\n",
        "                    loss = criterion(pred, y)\n",
        "\n",
        "                # Backward pass and optimizer step\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(opt)\n",
        "                scaler.update()\n",
        "\n",
        "                # Compute train metrics on this batch\n",
        "                with torch.no_grad():\n",
        "                    y_hat = (\n",
        "                        torch.sigmoid(pred).detach().cpu().numpy() > 0.5\n",
        "                    ).astype(np.uint8)\n",
        "                    y_np = (y.detach().cpu().numpy() > 0.5).astype(np.uint8)\n",
        "                    d, i, a = calc_metrics(y_np, y_hat)\n",
        "\n",
        "                tot_tr_loss += float(loss.item())\n",
        "                tot_tr_d += d\n",
        "                tot_tr_i += i\n",
        "                tot_tr_a += a\n",
        "                nb_tr += 1\n",
        "\n",
        "            nb_tr = max(nb_tr, 1)\n",
        "            epoch_tr_loss = tot_tr_loss / nb_tr\n",
        "            epoch_tr_dice = tot_tr_d / nb_tr\n",
        "            epoch_tr_iou = tot_tr_i / nb_tr\n",
        "            epoch_tr_acc = tot_tr_a / nb_tr\n",
        "\n",
        "            # ----- Validation phase -----\n",
        "            self.model.eval()\n",
        "            tot_val_loss = tot_val_d = tot_val_i = tot_val_a = 0.0\n",
        "            nb_val = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in self.val_loader:\n",
        "                    x = batch[\"x\"].to(DEVICE)\n",
        "                    y = batch[\"y\"].to(DEVICE)\n",
        "                    pred = self.model(x)\n",
        "\n",
        "                    v_loss = float(criterion(pred, y).item())\n",
        "                    y_hat = (torch.sigmoid(pred).cpu().numpy() > 0.5).astype(\n",
        "                        np.uint8\n",
        "                    )\n",
        "                    y_np = (y.cpu().numpy() > 0.5).astype(np.uint8)\n",
        "                    d, i, a = calc_metrics(y_np, y_hat)\n",
        "\n",
        "                    tot_val_loss += v_loss\n",
        "                    tot_val_d += d\n",
        "                    tot_val_i += i\n",
        "                    tot_val_a += a\n",
        "                    nb_val += 1\n",
        "\n",
        "            nb_val = max(nb_val, 1)\n",
        "            epoch_val_loss = tot_val_loss / nb_val\n",
        "            epoch_val_dice = tot_val_d / nb_val\n",
        "            epoch_val_iou = tot_val_i / nb_val\n",
        "            epoch_val_acc = tot_val_a / nb_val\n",
        "\n",
        "            # Log per-epoch metrics\n",
        "            epoch_logs.append(\n",
        "                {\n",
        "                    \"epoch\": int(epoch_idx),\n",
        "                    \"train_loss\": float(epoch_tr_loss),\n",
        "                    \"train_dice\": float(epoch_tr_dice),\n",
        "                    \"train_iou\": float(epoch_tr_iou),\n",
        "                    \"train_acc\": float(epoch_tr_acc),\n",
        "                    \"val_loss\": float(epoch_val_loss),\n",
        "                    \"val_dice\": float(epoch_val_dice),\n",
        "                    \"val_iou\": float(epoch_val_iou),\n",
        "                    \"val_acc\": float(epoch_val_acc),\n",
        "                }\n",
        "            )\n",
        "\n",
        "            # Update best model based on validation metrics\n",
        "            if (epoch_val_loss < best_val_loss) and (\n",
        "                epoch_val_dice > best_val_dice\n",
        "            ):\n",
        "                best_val_loss = epoch_val_loss\n",
        "                best_val_dice = epoch_val_dice\n",
        "                best_state = copy.deepcopy(self.model.state_dict())\n",
        "                best_epoch_idx = epoch_idx\n",
        "\n",
        "                best_train_loss = epoch_tr_loss\n",
        "                best_train_dice = epoch_tr_dice\n",
        "                best_train_iou = epoch_tr_iou\n",
        "                best_train_acc = epoch_tr_acc\n",
        "\n",
        "        # Load best local model state if available\n",
        "        if best_state is not None:\n",
        "            self.model.load_state_dict(best_state)\n",
        "\n",
        "        # Mark best epoch inside logs\n",
        "        for ep in epoch_logs:\n",
        "            ep[\"best_epoch\"] = ep[\"epoch\"] == best_epoch_idx\n",
        "\n",
        "        # Metrics sent back to server\n",
        "        train_metrics = {\n",
        "            \"cid\": int(self.cid),\n",
        "            \"best_epoch\": int(best_epoch_idx),\n",
        "            \"best_val_loss\": float(best_val_loss),\n",
        "            \"best_val_dice\": float(best_val_dice),\n",
        "            \"per_epoch\": json.dumps(epoch_logs),\n",
        "        }\n",
        "\n",
        "        # Save best local checkpoint\n",
        "        maybe_save_best(\n",
        "            self.cid,\n",
        "            best_val_loss,\n",
        "            best_val_dice,\n",
        "            best_epoch_idx,\n",
        "            rnd,\n",
        "            self.model,\n",
        "        )\n",
        "\n",
        "        # Return updated parameters, number of train examples, and metrics\n",
        "        return get_parameters(self.model), self.ntr, train_metrics\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "        \"\"\"Evaluate global parameters on local validation set.\"\"\"\n",
        "        # Load global parameters\n",
        "        set_parameters(self.model, parameters)\n",
        "        self.model.eval()\n",
        "\n",
        "        tot_loss = tot_d = tot_i = tot_a = 0.0\n",
        "        nb = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in self.val_loader:\n",
        "                x = batch[\"x\"].to(DEVICE)\n",
        "                y = batch[\"y\"].to(DEVICE)\n",
        "                pred = self.model(x)\n",
        "\n",
        "                loss = float(criterion(pred, y).item())\n",
        "                y_hat = (torch.sigmoid(pred).cpu().numpy() > 0.5).astype(\n",
        "                    np.uint8\n",
        "                )\n",
        "                y_np = (y.cpu().numpy() > 0.5).astype(np.uint8)\n",
        "                d, i, a = calc_metrics(y_np, y_hat)\n",
        "\n",
        "                tot_loss += loss\n",
        "                tot_d += d\n",
        "                tot_i += i\n",
        "                tot_a += a\n",
        "                nb += 1\n",
        "\n",
        "        nb = max(nb, 1)\n",
        "        metrics = {\n",
        "            \"loss\": tot_loss / nb,\n",
        "            \"dice\": tot_d / nb,\n",
        "            \"iou\": tot_i / nb,\n",
        "            \"acc\": tot_a / nb,\n",
        "            \"cid\": int(self.cid),\n",
        "        }\n",
        "\n",
        "        # Flower expects (loss, num_examples, metrics)\n",
        "        return metrics[\"loss\"], self.nva, metrics\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Standalone client entry point (for non-simulation setups)\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--cid\", type=int, required=True)\n",
        "    parser.add_argument(\"--server\", default=\"0.0.0.0:8080\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(\n",
        "        f\"[SegClient {args.cid}] device={DEVICE}, cuda={torch.cuda.is_available()}\"\n",
        "    )\n",
        "\n",
        "    fl.client.start_numpy_client(\n",
        "        server_address=args.server,\n",
        "        client=SegClient(args.cid),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZxs3HOEF4Wj",
        "outputId": "03fe587e-2816-442a-d442-0f390b69636d"
      },
      "outputs": [],
      "source": [
        "%%writefile fl_sim_colab.py\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "import torch\n",
        "import flwr as fl\n",
        "from flwr.common import FitIns\n",
        "from fl_client import SegClient\n",
        "\n",
        "# Base directory for outputs\n",
        "BASE_DIR = \"AITDM\"\n",
        "# Directory where per-client metrics will be saved\n",
        "METRICS_DIR = os.path.join(BASE_DIR, \"metrics\")\n",
        "os.makedirs(METRICS_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "def client_fn(cid: str):\n",
        "    \"\"\"Create a Flower client for a given client ID.\"\"\"\n",
        "    return SegClient(int(cid)).to_client()\n",
        "\n",
        "\n",
        "def ensure_csv(path: str, header: list[str]):\n",
        "    \"\"\"Create a CSV file with header if it does not exist.\"\"\"\n",
        "    if not os.path.isfile(path):\n",
        "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "        with open(path, \"w\", newline=\"\") as f:\n",
        "            csv.writer(f).writerow(header)\n",
        "\n",
        "\n",
        "def append_row(path: str, row: list):\n",
        "    \"\"\"Append a single row to a CSV file.\"\"\"\n",
        "    with open(path, \"a\", newline=\"\") as f:\n",
        "        csv.writer(f).writerow(row)\n",
        "\n",
        "\n",
        "class PerClientLoggingFedAvg(fl.server.strategy.FedAvg):\n",
        "    \"\"\"Custom FedAvg strategy that logs per-client metrics to CSV.\"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # CSV header for logged metrics\n",
        "        self.header = [\n",
        "            \"round\",\n",
        "            \"epoch\",\n",
        "            \"train_loss\",\n",
        "            \"train_dice\",\n",
        "            \"train_iou\",\n",
        "            \"train_acc\",\n",
        "            \"val_loss\",\n",
        "            \"val_dice\",\n",
        "            \"val_iou\",\n",
        "            \"val_acc\",\n",
        "            \"best_epoch\",\n",
        "        ]\n",
        "\n",
        "    def configure_fit(self, server_round, parameters, client_manager):\n",
        "        \"\"\"Inject current round number into fit configuration for each client.\"\"\"\n",
        "        items = super().configure_fit(server_round, parameters, client_manager)\n",
        "        out = []\n",
        "        for it in items:\n",
        "            if isinstance(it, tuple):\n",
        "                client, fitins = it\n",
        "            else:\n",
        "                client, fitins = None, it\n",
        "\n",
        "            cfg = dict(fitins.config)\n",
        "            cfg[\"round\"] = server_round\n",
        "            new_fitins = FitIns(fitins.parameters, cfg)\n",
        "\n",
        "            out.append(\n",
        "                (client, new_fitins) if client is not None else new_fitins\n",
        "            )\n",
        "        return out\n",
        "\n",
        "    def aggregate_fit(self, rnd, results, failures):\n",
        "        \"\"\"Aggregate fit results and log per-epoch metrics for each client.\"\"\"\n",
        "        agg = super().aggregate_fit(rnd, results, failures)\n",
        "\n",
        "        for client_proxy, fit_res in results:\n",
        "            m = fit_res.metrics or {}\n",
        "            cid = str(m.get(\"cid\", client_proxy.cid))\n",
        "\n",
        "            client_csv = os.path.join(METRICS_DIR, f\"metrics_client_{cid}.csv\")\n",
        "            ensure_csv(client_csv, self.header)\n",
        "\n",
        "            best_epoch = int(m.get(\"best_epoch\", -1))\n",
        "            per_epoch_raw = m.get(\"per_epoch\", \"[]\")\n",
        "\n",
        "            # Parse per-epoch metrics sent from the client\n",
        "            try:\n",
        "                per_epoch = json.loads(per_epoch_raw)\n",
        "            except Exception:\n",
        "                per_epoch = []\n",
        "\n",
        "            # Write one row per local epoch\n",
        "            for ep in per_epoch:\n",
        "                epoch = ep.get(\"epoch\", \"\")\n",
        "                row = [\n",
        "                    rnd,\n",
        "                    epoch,\n",
        "                    ep.get(\"train_loss\", \"\"),\n",
        "                    ep.get(\"train_dice\", \"\"),\n",
        "                    ep.get(\"train_iou\", \"\"),\n",
        "                    ep.get(\"train_acc\", \"\"),\n",
        "                    ep.get(\"val_loss\", \"\"),\n",
        "                    ep.get(\"val_dice\", \"\"),\n",
        "                    ep.get(\"val_iou\", \"\"),\n",
        "                    ep.get(\"val_acc\", \"\"),\n",
        "                    \"x\" if int(epoch) == best_epoch else \"\",\n",
        "                ]\n",
        "                append_row(client_csv, row)\n",
        "\n",
        "        return agg\n",
        "\n",
        "\n",
        "# Federated learning strategy configuration\n",
        "strategy = PerClientLoggingFedAvg(\n",
        "    fraction_fit=1.0,\n",
        "    fraction_evaluate=1.0,\n",
        "    min_fit_clients=3,\n",
        "    min_evaluate_clients=3,\n",
        "    min_available_clients=3,\n",
        "    on_fit_config_fn=lambda rnd: {\"local_epochs\": 5, \"lr\": 1e-3},\n",
        ")\n",
        "\n",
        "# Resource configuration (GPU if available)\n",
        "use_gpu = torch.cuda.is_available()\n",
        "client_resources = {\"num_cpus\": 1, \"num_gpus\": 1.0 if use_gpu else 0.0}\n",
        "\n",
        "# Start Flower simulation with 3 clients and 5 communication rounds\n",
        "fl.simulation.start_simulation(\n",
        "    client_fn=client_fn,\n",
        "    num_clients=3,\n",
        "    config=fl.server.ServerConfig(num_rounds=5),\n",
        "    strategy=strategy,\n",
        "    client_resources=client_resources,\n",
        "    ray_init_args={\"include_dashboard\": False},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3dlt740MYbP",
        "outputId": "0f4d7393-46ef-42a5-f9d1-d8a780b35063"
      },
      "outputs": [],
      "source": [
        "!python fl_sim_colab.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzRm-EHu8iCx"
      },
      "source": [
        "# **M2 - Clip Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA8ci1ByLTbg"
      },
      "source": [
        "**We train UNet for tumor segmentation, train BioClinicalBERT for region prediction in reports, then train a multimodal CLIP-like model that aligns text–image embeddings and, simultaneously, preserves segmentation through a combined loss.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9EatFoMKv7p"
      },
      "source": [
        "**<h2>UNet on images<h2>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwxW3U1PKUbv"
      },
      "outputs": [],
      "source": [
        "import os, json, pickle, random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "from sklearn.metrics import jaccard_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "# Paths and I/O config\n",
        "DATA_ROOT = \"/content/Preprocessed-Data\"\n",
        "METADATA_DF_PATH = \"cleaned_df.pkl\"\n",
        "OUT_BASE = \"AITDM\"\n",
        "CLIENT_DIR = os.path.join(OUT_BASE, \"/content/client\")\n",
        "\n",
        "# Experiment config\n",
        "USE_ATLAS = True\n",
        "CLIENT_ID = 0\n",
        "BATCH_SIZE = 8\n",
        "LR = 1e-3\n",
        "EPOCHS = 50\n",
        "NUM_WORKERS = 2\n",
        "SEED = 42\n",
        "\n",
        "# Model / encoder config\n",
        "ENCODER_NAME = \"resnet34\"\n",
        "ENCODER_WEIGHTS = \"imagenet\"\n",
        "\n",
        "# Output directories\n",
        "OUT_MODELS_DIR = os.path.join(OUT_BASE, \"Models\", \"UNet_ImageOnly\")\n",
        "OUT_GRAPHS_DIR = os.path.join(OUT_BASE, \"Graphs\")\n",
        "os.makedirs(OUT_MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(OUT_GRAPHS_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
        "\n",
        "\n",
        "def worker_init_fn(worker_id):\n",
        "    worker_seed = SEED + worker_id\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "\n",
        "set_seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class ImageOnlyGliomaDataset(Dataset):\n",
        "    \"\"\"Dataset that loads MRI, tumor mask, and optional atlas regions per patient.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        metadata_df_path,\n",
        "        data_root,\n",
        "        use_atlas=True,\n",
        "        exclude_ids=None,\n",
        "        transform=None,\n",
        "    ):\n",
        "        import pickle, os, numpy as np, pandas as pd\n",
        "\n",
        "        # Load metadata dataframe\n",
        "        with open(metadata_df_path, \"rb\") as f:\n",
        "            df = pickle.load(f)\n",
        "\n",
        "        # Optionally exclude some patient IDs\n",
        "        if exclude_ids is None:\n",
        "            exclude_ids = [\"PatientID_0191\"]\n",
        "        self.df = df[~df[\"Patient_ID\"].isin(exclude_ids)].reset_index(drop=True)\n",
        "\n",
        "        self.data_root = data_root\n",
        "        self.use_atlas = use_atlas\n",
        "        self.transform = transform\n",
        "\n",
        "        # Collect patient IDs that have all required files\n",
        "        self.patient_ids = []\n",
        "        for pid in sorted(self.df[\"Patient_ID\"].tolist()):\n",
        "            mri_p = os.path.join(self.data_root, pid, f\"{pid}_mri.npy\")\n",
        "            tumor_p = os.path.join(self.data_root, pid, f\"{pid}_tumor.npy\")\n",
        "\n",
        "            if self.use_atlas:\n",
        "                reg_p = os.path.join(self.data_root, pid, f\"{pid}_regions.npy\")\n",
        "                if (\n",
        "                    os.path.isfile(mri_p)\n",
        "                    and os.path.isfile(tumor_p)\n",
        "                    and os.path.isfile(reg_p)\n",
        "                ):\n",
        "                    self.patient_ids.append(pid)\n",
        "            else:\n",
        "                if os.path.isfile(mri_p) and os.path.isfile(tumor_p):\n",
        "                    self.patient_ids.append(pid)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patient_ids)\n",
        "\n",
        "    @staticmethod\n",
        "    def _minmax(x):\n",
        "        \"\"\"Simple min-max normalization to [0, 1].\"\"\"\n",
        "        x = x.astype(np.float32)\n",
        "        mn, mx = np.min(x), np.max(x)\n",
        "        return (\n",
        "            (x - mn) / (mx - mn)\n",
        "            if mx > mn\n",
        "            else np.zeros_like(x, dtype=np.float32)\n",
        "        )\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Load and return one sample (dict) for a patient.\"\"\"\n",
        "        import os, numpy as np\n",
        "\n",
        "        pid = self.patient_ids[idx]\n",
        "        base = os.path.join(self.data_root, pid)\n",
        "\n",
        "        # Load MRI and tumor mask\n",
        "        mri = np.load(os.path.join(base, f\"{pid}_mri.npy\")).astype(np.float32)\n",
        "        tumor = np.load(os.path.join(base, f\"{pid}_tumor.npy\")).astype(\n",
        "            np.float32\n",
        "        )\n",
        "\n",
        "        # Normalize MRI and binarize tumor mask\n",
        "        mri = self._minmax(mri)\n",
        "        tumor = (tumor > 0.5).astype(np.float32)\n",
        "\n",
        "        sample = {\"patient_id\": pid, \"mri\": mri, \"tumor\": tumor}\n",
        "\n",
        "        # Optionally load regions/atlas\n",
        "        if self.use_atlas:\n",
        "            regions = np.load(os.path.join(base, f\"{pid}_regions.npy\")).astype(\n",
        "                np.float32\n",
        "            )\n",
        "            regions = self._minmax(regions)\n",
        "            sample[\"regions\"] = regions\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "def image_only_collate_fn(batch, use_atlas=True):\n",
        "    \"\"\"Custom collate: stack MRI (+ optional regions) and tumor into tensors.\"\"\"\n",
        "    mri = torch.stack([torch.tensor(it[\"mri\"]) for it in batch]).unsqueeze(1)\n",
        "    y = torch.stack([torch.tensor(it[\"tumor\"]) for it in batch]).unsqueeze(1)\n",
        "\n",
        "    if use_atlas:\n",
        "        regs = torch.stack(\n",
        "            [torch.tensor(it[\"regions\"]) for it in batch]\n",
        "        ).unsqueeze(1)\n",
        "        # Concatenate MRI and regions as channels\n",
        "        x = torch.cat([mri.float(), regs.float()], dim=1)\n",
        "    else:\n",
        "        x = mri.float()\n",
        "\n",
        "    return {\"x\": x, \"y\": y.float(), \"pid\": [it[\"patient_id\"] for it in batch]}\n",
        "\n",
        "\n",
        "class SubsetByPIDs(Dataset):\n",
        "    \"\"\"Wrap a dataset but keep only a subset of patient IDs.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, full_dataset: ImageOnlyGliomaDataset, pid_list: List[str]\n",
        "    ):\n",
        "        self.ds = full_dataset\n",
        "        # Map patient IDs to indices\n",
        "        pid_to_idx = {pid: i for i, pid in enumerate(self.ds.patient_ids)}\n",
        "        self.indices = [pid_to_idx[p] for p in pid_list if p in pid_to_idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.ds[self.indices[i]]\n",
        "\n",
        "\n",
        "# Load client-specific train/val patient IDs\n",
        "cdir = os.path.join(CLIENT_DIR, f\"client_{CLIENT_ID}\")\n",
        "with open(os.path.join(cdir, \"train_pids.json\"), \"r\") as f:\n",
        "    train_pids = json.load(f)\n",
        "with open(os.path.join(cdir, \"val_pids.json\"), \"r\") as f:\n",
        "    val_pids = json.load(f)\n",
        "\n",
        "# Build full dataset and then client subsets\n",
        "full_ds = ImageOnlyGliomaDataset(\n",
        "    METADATA_DF_PATH,\n",
        "    DATA_ROOT,\n",
        "    use_atlas=USE_ATLAS,\n",
        "    exclude_ids=[\"PatientID_0191\"],\n",
        ")\n",
        "train_dataset = SubsetByPIDs(full_ds, train_pids)\n",
        "val_dataset = SubsetByPIDs(full_ds, val_pids)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "    generator=torch.Generator().manual_seed(SEED),\n",
        "    worker_init_fn=worker_init_fn,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "    generator=torch.Generator().manual_seed(SEED),\n",
        "    worker_init_fn=worker_init_fn,\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"Loaded client_{CLIENT_ID}: train patients={len(train_dataset)}, val patients={len(val_dataset)}\"\n",
        ")\n",
        "\n",
        "# UNet model (from segmentation_models_pytorch)\n",
        "in_channels = 2 if USE_ATLAS else 1\n",
        "model = smp.Unet(\n",
        "    encoder_name=ENCODER_NAME,\n",
        "    encoder_weights=ENCODER_WEIGHTS,\n",
        "    in_channels=in_channels,\n",
        "    classes=1,\n",
        ").to(device)\n",
        "\n",
        "# Loss, optimizer, scheduler, and AMP scaler\n",
        "criterion = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
        "scheduler = optim.lr_scheduler.LinearLR(\n",
        "    optimizer, start_factor=1.0, end_factor=0.0, total_iters=EPOCHS\n",
        ")\n",
        "scaler = torch.amp.GradScaler(\"cuda\", enabled=(device.type == \"cuda\"))\n",
        "\n",
        "\n",
        "def calc_metrics(y_true, y_pred):\n",
        "    \"\"\"Compute Dice, IoU, and pixel accuracy.\"\"\"\n",
        "    y_true = y_true.astype(np.uint8).reshape(-1)\n",
        "    y_pred = y_pred.astype(np.uint8).reshape(-1)\n",
        "\n",
        "    inter = (y_true * y_pred).sum()\n",
        "    dice = (2.0 * inter) / (y_true.sum() + y_pred.sum() + 1e-8)\n",
        "\n",
        "    # IoU with sklearn, fallback if it fails\n",
        "    try:\n",
        "        iou = jaccard_score(y_true, y_pred, average=\"binary\")\n",
        "    except Exception:\n",
        "        union = y_true.sum() + y_pred.sum() - inter + 1e-8\n",
        "        iou = inter / union\n",
        "\n",
        "    acc = (y_true == y_pred).mean()\n",
        "    return float(dice), float(iou), float(acc)\n",
        "\n",
        "\n",
        "def plot_prediction(sample_imgs, save_path):\n",
        "    \"\"\"Plot MRI (+ optional regions), GT, and prediction for one batch.\"\"\"\n",
        "    if \"regions\" in sample_imgs:\n",
        "        fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
        "    else:\n",
        "        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # MRI\n",
        "    axs[0].imshow(sample_imgs[\"mri\"][0, 0], cmap=\"gray\")\n",
        "    axs[0].set_title(\"MRI\")\n",
        "    axs[0].axis(\"off\")\n",
        "\n",
        "    col = 1\n",
        "\n",
        "    # Regions (if available)\n",
        "    if \"regions\" in sample_imgs:\n",
        "        axs[col].imshow(sample_imgs[\"regions\"][0, 0], cmap=\"gray\")\n",
        "        axs[col].set_title(\"Regions\")\n",
        "        axs[col].axis(\"off\")\n",
        "        col += 1\n",
        "\n",
        "    # Ground truth\n",
        "    axs[col].imshow(sample_imgs[\"y_true\"][0, 0], cmap=\"gray\")\n",
        "    axs[col].set_title(\"Ground Truth\")\n",
        "    axs[col].axis(\"off\")\n",
        "    col += 1\n",
        "\n",
        "    # Prediction\n",
        "    axs[col].imshow(sample_imgs[\"y_pred\"][0, 0], cmap=\"gray\")\n",
        "    axs[col].set_title(\"Predicted\")\n",
        "    axs[col].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_metrics(history, save_path):\n",
        "    \"\"\"Plot training and validation curves for loss and metrics.\"\"\"\n",
        "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
        "\n",
        "    plt.figure(figsize=(16, 12))\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\")\n",
        "    plt.plot(epochs, history[\"val_loss\"], label=\"Val Loss\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Loss\")\n",
        "\n",
        "    # Dice\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(epochs, history[\"train_dice\"], label=\"Train Dice\")\n",
        "    plt.plot(epochs, history[\"val_dice\"], label=\"Val Dice\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Dice\")\n",
        "\n",
        "    # IoU\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(epochs, history[\"train_iou\"], label=\"Train IoU\")\n",
        "    plt.plot(epochs, history[\"val_iou\"], label=\"Val IoU\")\n",
        "    plt.legend()\n",
        "    plt.title(\"IoU\")\n",
        "\n",
        "    # Accuracy\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(epochs, history[\"train_acc\"], label=\"Train Acc\")\n",
        "    plt.plot(epochs, history[\"val_acc\"], label=\"Val Acc\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Accuracy\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion, scaler):\n",
        "    \"\"\"One training epoch over the dataloader.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = total_dice = total_iou = total_acc = 0.0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        x = batch[\"x\"].to(device)\n",
        "        y = batch[\"y\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Mixed precision forward pass\n",
        "        with torch.amp.autocast(\"cuda\", enabled=(device.type == \"cuda\")):\n",
        "            preds = model(x)\n",
        "            loss = criterion(preds, y)\n",
        "\n",
        "        # Backward with gradient scaling\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "        # Convert predictions to binary and compute metrics\n",
        "        preds_prob = (torch.sigmoid(preds).detach().cpu().numpy() > 0.5).astype(\n",
        "            np.uint8\n",
        "        )\n",
        "        y_np = (y.detach().cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        d, i, a = calc_metrics(y_np, preds_prob)\n",
        "\n",
        "        total_dice += d\n",
        "        total_iou += i\n",
        "        total_acc += a\n",
        "\n",
        "    n = len(dataloader)\n",
        "    return total_loss / n, total_dice / n, total_iou / n, total_acc / n\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_one_epoch(model, dataloader, criterion, keep_last_batch=True):\n",
        "    \"\"\"Evaluation loop over the validation dataloader.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = total_dice = total_iou = total_acc = 0.0\n",
        "    last_batch_imgs = None\n",
        "\n",
        "    for batch in dataloader:\n",
        "        x = batch[\"x\"].to(device)\n",
        "        y = batch[\"y\"].to(device)\n",
        "\n",
        "        preds = model(x)\n",
        "        loss = criterion(preds, y)\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "        preds_prob = (torch.sigmoid(preds).cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        y_np = (y.cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        d, i, a = calc_metrics(y_np, preds_prob)\n",
        "\n",
        "        total_dice += d\n",
        "        total_iou += i\n",
        "        total_acc += a\n",
        "\n",
        "        # Optionally keep last batch for visualization\n",
        "        if keep_last_batch:\n",
        "            ch0 = x[:, 0:1].cpu().numpy()\n",
        "            imgs = {\"mri\": ch0, \"y_true\": y.cpu().numpy(), \"y_pred\": preds_prob}\n",
        "            if x.shape[1] == 2:\n",
        "                ch1 = x[:, 1:2].cpu().numpy()\n",
        "                imgs[\"regions\"] = ch1\n",
        "            last_batch_imgs = imgs\n",
        "\n",
        "    n = len(dataloader)\n",
        "    return (\n",
        "        total_loss / n,\n",
        "        total_dice / n,\n",
        "        total_iou / n,\n",
        "        total_acc / n,\n",
        "        last_batch_imgs,\n",
        "    )\n",
        "\n",
        "\n",
        "def infer_and_visualize_best(\n",
        "    model,\n",
        "    val_dataset,\n",
        "    use_atlas: bool,\n",
        "    out_dir: str,\n",
        "    client_id: int,\n",
        "    best_ckpt_path: str,\n",
        "    k_samples: int = 3,\n",
        "    threshold: float = 0.5,\n",
        "):\n",
        "    \"\"\"Load best checkpoint, run inference on a few validation samples, and save visualizations.\"\"\"\n",
        "    import random\n",
        "\n",
        "    # Make sampling deterministic\n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    if os.path.isfile(best_ckpt_path):\n",
        "        model.load_state_dict(torch.load(best_ckpt_path, map_location=device))\n",
        "        model.eval()\n",
        "        print(f\"[Best Model] Loaded: {best_ckpt_path}\\n\")\n",
        "    else:\n",
        "        print(f\"[Best Model] Missing checkpoint: {best_ckpt_path}\\n\")\n",
        "        return\n",
        "\n",
        "    k = min(k_samples, len(val_dataset))\n",
        "    if k == 0:\n",
        "        print(\"[Best Model] Empty val dataset.\\n\")\n",
        "        return\n",
        "\n",
        "    # Deterministic random subset of validation indices\n",
        "    idxs = random.sample(range(len(val_dataset)), k)\n",
        "\n",
        "    def _predict_one(sample):\n",
        "        \"\"\"Run model on one sample and return inputs and binarized prediction.\"\"\"\n",
        "        mri = (\n",
        "            torch.tensor(sample[\"mri\"])\n",
        "            .unsqueeze(0)\n",
        "            .unsqueeze(0)\n",
        "            .float()\n",
        "            .to(device)\n",
        "        )\n",
        "\n",
        "        if use_atlas and (\"regions\" in sample):\n",
        "            regs = (\n",
        "                torch.tensor(sample[\"regions\"])\n",
        "                .unsqueeze(0)\n",
        "                .unsqueeze(0)\n",
        "                .float()\n",
        "                .to(device)\n",
        "            )\n",
        "            x = torch.cat([mri, regs], dim=1)\n",
        "        else:\n",
        "            x = mri\n",
        "\n",
        "        with torch.no_grad():\n",
        "            prob = torch.sigmoid(model(x)).cpu().numpy()\n",
        "            pred_bin = (prob > threshold).astype(np.uint8)\n",
        "\n",
        "        return x.cpu().numpy(), pred_bin\n",
        "\n",
        "    saved_paths = []\n",
        "\n",
        "    # Save individual sample figures\n",
        "    for i, idx in enumerate(idxs, 1):\n",
        "        sample = val_dataset[idx]\n",
        "        pid = sample.get(\"patient_id\", f\"val_{idx}\")\n",
        "        x_np, pred_bin = _predict_one(sample)\n",
        "\n",
        "        imgs = {\n",
        "            \"mri\": x_np[:, 0:1],\n",
        "            \"y_true\": np.expand_dims(\n",
        "                np.expand_dims(sample[\"tumor\"], 0), 0\n",
        "            ).astype(np.float32),\n",
        "            \"y_pred\": pred_bin.astype(np.float32),\n",
        "        }\n",
        "\n",
        "        if use_atlas and (\"regions\" in sample):\n",
        "            imgs[\"regions\"] = x_np[:, 1:2]\n",
        "\n",
        "        out_path = os.path.join(\n",
        "            out_dir, f\"best_val_sample_{i}_client{client_id}_{pid}.png\"\n",
        "        )\n",
        "        plot_prediction(imgs, out_path)\n",
        "        saved_paths.append(out_path)\n",
        "\n",
        "    print(\"[Best Model] Saved individual figures:\")\n",
        "    for p in saved_paths:\n",
        "        print(\" -\", p)\n",
        "    print()\n",
        "\n",
        "    # Save grid figure\n",
        "    cols = 4 if use_atlas else 3\n",
        "    fig, axs = plt.subplots(k, cols, figsize=(5 * cols, 4 * k))\n",
        "\n",
        "    if k == 1:\n",
        "        axs = np.expand_dims(axs, 0)\n",
        "\n",
        "    for row, idx in enumerate(idxs):\n",
        "        sample = val_dataset[idx]\n",
        "        x_np, pred_bin = _predict_one(sample)\n",
        "\n",
        "        mri = x_np[0, 0]\n",
        "        gt = sample[\"tumor\"]\n",
        "        col = 0\n",
        "\n",
        "        axs[row, col].imshow(mri, cmap=\"gray\")\n",
        "        axs[row, col].set_title(\"MRI\")\n",
        "        axs[row, col].axis(\"off\")\n",
        "        col += 1\n",
        "\n",
        "        if use_atlas and (\"regions\" in sample):\n",
        "            regs = x_np[0, 1]\n",
        "            axs[row, col].imshow(regs, cmap=\"gray\")\n",
        "            axs[row, col].set_title(\"Regions\")\n",
        "            axs[row, col].axis(\"off\")\n",
        "            col += 1\n",
        "\n",
        "        axs[row, col].imshow(gt, cmap=\"gray\")\n",
        "        axs[row, col].set_title(\"Ground Truth\")\n",
        "        axs[row, col].axis(\"off\")\n",
        "        col += 1\n",
        "\n",
        "        axs[row, col].imshow(pred_bin[0, 0], cmap=\"gray\")\n",
        "        axs[row, col].set_title(\"Predicted (τ=0.5)\")\n",
        "        axs[row, col].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    grid_path = os.path.join(\n",
        "        out_dir, f\"best_model_val_grid_client{client_id}.png\"\n",
        "    )\n",
        "    plt.savefig(grid_path, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    print(f\"[Best Model] Saved grid -> {grid_path}\\n\")\n",
        "\n",
        "\n",
        "# History containers for training curves\n",
        "history = {\n",
        "    \"train_loss\": [],\n",
        "    \"train_dice\": [],\n",
        "    \"train_iou\": [],\n",
        "    \"train_acc\": [],\n",
        "    \"val_loss\": [],\n",
        "    \"val_dice\": [],\n",
        "    \"val_iou\": [],\n",
        "    \"val_acc\": [],\n",
        "}\n",
        "\n",
        "# Tracking best validation metrics\n",
        "best_val_iou = 0.0\n",
        "best_val_dice = -float(\"inf\")\n",
        "best_val_loss = float(\"inf\")\n",
        "best_path = os.path.join(OUT_MODELS_DIR, f\"best_unet_client{CLIENT_ID}.pth\")\n",
        "\n",
        "log_rows = []\n",
        "\n",
        "# Main training loop\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    # Train one epoch\n",
        "    trL, trD, trI, trA = train_one_epoch(\n",
        "        model, train_loader, optimizer, criterion, scaler\n",
        "    )\n",
        "    # Validate\n",
        "    vaL, vaD, vaI, vaA, _ = eval_one_epoch(\n",
        "        model, val_loader, criterion, keep_last_batch=True\n",
        "    )\n",
        "\n",
        "    # Update scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    # Save metrics for plots\n",
        "    history[\"train_loss\"].append(trL)\n",
        "    history[\"train_dice\"].append(trD)\n",
        "    history[\"train_iou\"].append(trI)\n",
        "    history[\"train_acc\"].append(trA)\n",
        "\n",
        "    history[\"val_loss\"].append(vaL)\n",
        "    history[\"val_dice\"].append(vaD)\n",
        "    history[\"val_iou\"].append(vaI)\n",
        "    history[\"val_acc\"].append(vaA)\n",
        "\n",
        "    print(\n",
        "        f\"[Epoch {epoch:03d}/{EPOCHS}] \"\n",
        "        f\"Train — Loss {trL:.4f} | Dice {trD:.4f} | IoU {trI:.4f} | Accuracy {trA:.4f} || \"\n",
        "        f\"Val — Loss {vaL:.4f} | Dice {vaD:.4f} | IoU {vaI:.4f} | Accuracy {vaA:.4f}\\n\"\n",
        "    )\n",
        "\n",
        "    # Save checkpoint if Dice improves and loss decreases\n",
        "    saved_ckpt = False\n",
        "    if (vaD > best_val_dice) and (vaL < best_val_loss):\n",
        "        best_val_dice = vaD\n",
        "        best_val_loss = vaL\n",
        "        torch.save(model.state_dict(), best_path)\n",
        "        saved_ckpt = True\n",
        "        print(\n",
        "            f\"Saved best model (Val Dice↑ {best_val_dice:.4f} & Val Loss↓ {best_val_loss:.4f}) -> {best_path}\\n\"\n",
        "        )\n",
        "\n",
        "    # Log row for CSV\n",
        "    log_rows.append(\n",
        "        {\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": trL,\n",
        "            \"train_dice\": trD,\n",
        "            \"train_iou\": trI,\n",
        "            \"train_acc\": trA,\n",
        "            \"val_loss\": vaL,\n",
        "            \"val_dice\": vaD,\n",
        "            \"val_iou\": vaI,\n",
        "            \"val_acc\": vaA,\n",
        "            \"saved_ckpt\": saved_ckpt,\n",
        "            \"marker\": \"X\" if saved_ckpt else \"\",\n",
        "        }\n",
        "    )\n",
        "\n",
        "# Save metrics as CSV\n",
        "metrics_csv = os.path.join(OUT_GRAPHS_DIR, f\"metrics_client{CLIENT_ID}.csv\")\n",
        "pd.DataFrame(log_rows).to_csv(metrics_csv, index=False)\n",
        "print(f\"[Log] Wrote per-epoch metrics CSV -> {metrics_csv}\\n\")\n",
        "\n",
        "# Plot training curves\n",
        "plot_metrics(\n",
        "    history,\n",
        "    os.path.join(OUT_GRAPHS_DIR, f\"training_curves_client{CLIENT_ID}.png\"),\n",
        ")\n",
        "\n",
        "# Run inference with best model and visualize a few validation samples\n",
        "infer_and_visualize_best(\n",
        "    model=model,\n",
        "    val_dataset=val_dataset,\n",
        "    use_atlas=USE_ATLAS,\n",
        "    out_dir=OUT_GRAPHS_DIR,\n",
        "    client_id=CLIENT_ID,\n",
        "    best_ckpt_path=best_path,\n",
        "    k_samples=3,\n",
        "    threshold=0.5,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKYalDg9K4tg"
      },
      "source": [
        "**<h2>BioClinicalBERT on reports<h2>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LtOH3OE4UST0",
        "outputId": "86f8dc02-56fa-4acd-ec8b-cdc965b18e39"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import random\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "\n",
        "warnings.simplefilter(\"ignore\", category=UndefinedMetricWarning)\n",
        "\n",
        "# ---- Reproducibility ----\n",
        "SEED = 42\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
        "\n",
        "# ---- Paths / Config ----\n",
        "METADATA_DF_PATH = \"/content/cleaned_df.pkl\"\n",
        "LABELS_PATH = \"/content/labels_list.pkl\"\n",
        "OUT_BASE = \"/content/AITDM\"\n",
        "CLIENT_BASE_DIR = \"/content/client\"\n",
        "\n",
        "CLIENT_ID = 0\n",
        "BATCH_SIZE = 8\n",
        "LR = 2e-5\n",
        "EPOCHS = 20\n",
        "NUM_WORKERS = 2\n",
        "MAX_LEN = 512\n",
        "WARMUP_RATIO = 0.1\n",
        "\n",
        "BERT_MODEL_NAME = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "DROPOUT = 0.3\n",
        "\n",
        "TOPK_PRED = 5\n",
        "USE_POS_WEIGHT = True\n",
        "REMOVE_BACKGROUND_LABEL = True\n",
        "\n",
        "OUT_MODELS_DIR = os.path.join(OUT_BASE, \"Models\", \"BioClinicalBert\")\n",
        "OUT_GRAPHS_DIR = os.path.join(OUT_BASE, \"Graphs\")\n",
        "os.makedirs(OUT_MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(OUT_GRAPHS_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "def set_seed(seed: int = 42) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
        "\n",
        "\n",
        "def worker_init_fn(worker_id: int) -> None:\n",
        "    s = SEED + worker_id\n",
        "    random.seed(s)\n",
        "    np.random.seed(s)\n",
        "\n",
        "\n",
        "set_seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class ReportsGliomaDataset(Dataset):\n",
        "    \"\"\"Multi-label dataset: report text -> set of region labels.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        metadata_df_path: str,\n",
        "        labels_path: str,\n",
        "        exclude_ids: Optional[List[str]] = None,\n",
        "        remove_background: bool = False,\n",
        "    ):\n",
        "        with open(metadata_df_path, \"rb\") as f:\n",
        "            df = pickle.load(f)\n",
        "\n",
        "        with open(labels_path, \"rb\") as f:\n",
        "            labels = list(pickle.load(f))\n",
        "\n",
        "        if remove_background:\n",
        "            labels = [\n",
        "                l for l in labels if str(l).strip().lower() != \"background\"\n",
        "            ]\n",
        "\n",
        "        self.labels: List[str] = labels\n",
        "        self.label_to_idx: Dict[str, int] = {\n",
        "            lab: i for i, lab in enumerate(self.labels)\n",
        "        }\n",
        "\n",
        "        if exclude_ids is None:\n",
        "            exclude_ids = [\"PatientID_0191\"]\n",
        "\n",
        "        df = df[~df[\"Patient_ID\"].isin(exclude_ids)].reset_index(drop=True)\n",
        "        df = df[df[\"Report\"].notna() & df[\"Top 5 Regions\"].notna()].reset_index(\n",
        "            drop=True\n",
        "        )\n",
        "\n",
        "        self.df = df\n",
        "        self.patient_ids = df[\"Patient_ID\"].tolist()\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.patient_ids)\n",
        "\n",
        "    def _make_multilabel_target(self, regions_list) -> np.ndarray:\n",
        "        y = np.zeros(len(self.labels), dtype=np.float32)\n",
        "        if not isinstance(regions_list, (list, tuple)):\n",
        "            return y\n",
        "        for reg in regions_list:\n",
        "            idx = self.label_to_idx.get(reg, None)\n",
        "            if idx is not None:\n",
        "                y[idx] = 1.0\n",
        "        return y\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, object]:\n",
        "        row = self.df.iloc[idx]\n",
        "        pid = row[\"Patient_ID\"]\n",
        "        report = str(row[\"Report\"])\n",
        "        top5 = row[\"Top 5 Regions\"]\n",
        "        target = self._make_multilabel_target(top5)\n",
        "        return {\"patient_id\": pid, \"report\": report, \"target\": target}\n",
        "\n",
        "\n",
        "class SubsetByPIDs(Dataset):\n",
        "    \"\"\"Filter a dataset by a provided list of patient IDs.\"\"\"\n",
        "\n",
        "    def __init__(self, full_dataset: ReportsGliomaDataset, pid_list: List[str]):\n",
        "        self.ds = full_dataset\n",
        "        pid_to_idx = {pid: i for i, pid in enumerate(self.ds.patient_ids)}\n",
        "        self.indices = [pid_to_idx[p] for p in pid_list if p in pid_to_idx]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, i: int) -> Dict[str, object]:\n",
        "        return self.ds[self.indices[i]]\n",
        "\n",
        "\n",
        "def reports_collate_fn(batch: List[Dict[str, object]]) -> Dict[str, object]:\n",
        "    reports = [b[\"report\"] for b in batch]\n",
        "    targets = torch.from_numpy(np.stack([b[\"target\"] for b in batch])).float()\n",
        "    pids = [b[\"patient_id\"] for b in batch]\n",
        "    return {\"report\": reports, \"target\": targets, \"pid\": pids}\n",
        "\n",
        "\n",
        "class BioBERTMultiLabelClassifier(nn.Module):\n",
        "    \"\"\"BioClinicalBERT + linear head for multi-label prediction.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str, num_labels: int, dropout: float = 0.3):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(\n",
        "        self, input_ids: torch.Tensor, attention_mask: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled = outputs.pooler_output\n",
        "        if pooled is None:\n",
        "            pooled = outputs.last_hidden_state[:, 0]  # CLS\n",
        "        x = self.dropout(pooled)\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
        "\n",
        "\n",
        "def tokenize_reports(\n",
        "    reports: List[str], max_len: int = 512\n",
        ") -> Dict[str, torch.Tensor]:\n",
        "    return tokenizer(\n",
        "        reports,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_len,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "\n",
        "def compute_pos_weight(train_dataset: Dataset, num_labels: int) -> torch.Tensor:\n",
        "    ys = np.stack(\n",
        "        [train_dataset[i][\"target\"] for i in range(len(train_dataset))]\n",
        "    ).astype(np.float32)\n",
        "    pos = ys.sum(axis=0)\n",
        "    neg = ys.shape[0] - pos\n",
        "    pw = (neg + 1e-6) / (pos + 1e-6)\n",
        "    pw = np.clip(pw, 1.0, 100.0)\n",
        "    return torch.tensor(pw, dtype=torch.float32)\n",
        "\n",
        "\n",
        "def preds_topk(probs: np.ndarray, k: int) -> np.ndarray:\n",
        "    k = max(1, min(k, probs.shape[1]))\n",
        "    pred = np.zeros_like(probs, dtype=np.int32)\n",
        "    topk_idx = np.argsort(-probs, axis=1)[:, :k]\n",
        "    rows = np.arange(probs.shape[0])[:, None]\n",
        "    pred[rows, topk_idx] = 1\n",
        "    return pred\n",
        "\n",
        "\n",
        "def precision_recall_at_k(\n",
        "    y_true: np.ndarray, probs: np.ndarray, k: int\n",
        ") -> Tuple[float, float]:\n",
        "    pred = preds_topk(probs, k)\n",
        "    tp = (pred * y_true).sum(axis=1)\n",
        "    prec = (tp / (k + 1e-8)).mean()\n",
        "    true_pos = y_true.sum(axis=1)\n",
        "    rec = (tp / (true_pos + 1e-8)).mean()\n",
        "    return float(prec), float(rec)\n",
        "\n",
        "\n",
        "def safe_auc_per_class(y_true: np.ndarray, y_score: np.ndarray) -> np.ndarray:\n",
        "    num_labels = y_true.shape[1]\n",
        "    out = np.full(num_labels, np.nan, dtype=np.float32)\n",
        "    for j in range(num_labels):\n",
        "        col = y_true[:, j]\n",
        "        if col.min() == col.max():\n",
        "            continue\n",
        "        try:\n",
        "            out[j] = float(roc_auc_score(col, y_score[:, j]))\n",
        "        except Exception:\n",
        "            out[j] = np.nan\n",
        "    return out\n",
        "\n",
        "\n",
        "def plot_history(\n",
        "    history: Dict[str, List[float]], save_dir: str, client_id: int, topk: int\n",
        ") -> None:\n",
        "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
        "\n",
        "    def _plot(tr_key: str, va_key: str, title: str, fname: str, ylabel: str):\n",
        "        plt.figure()\n",
        "        plt.plot(epochs, history[tr_key], label=\"Train\")\n",
        "        plt.plot(epochs, history[va_key], label=\"Val\")\n",
        "        plt.title(title)\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(ylabel)\n",
        "        plt.legend()\n",
        "        plt.savefig(os.path.join(save_dir, fname), bbox_inches=\"tight\")\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "    _plot(\n",
        "        \"train_loss\",\n",
        "        \"val_loss\",\n",
        "        \"BioClinicalBERT Loss\",\n",
        "        f\"biobert_loss_client{client_id}.png\",\n",
        "        \"Loss\",\n",
        "    )\n",
        "    _plot(\n",
        "        \"train_f1_macro_topk\",\n",
        "        \"val_f1_macro_topk\",\n",
        "        f\"F1 Macro (Top-{topk})\",\n",
        "        f\"biobert_f1macro_topk_client{client_id}.png\",\n",
        "        \"F1\",\n",
        "    )\n",
        "    _plot(\n",
        "        \"train_f1_micro_topk\",\n",
        "        \"val_f1_micro_topk\",\n",
        "        f\"F1 Micro (Top-{topk})\",\n",
        "        f\"biobert_f1micro_topk_client{client_id}.png\",\n",
        "        \"F1\",\n",
        "    )\n",
        "    _plot(\n",
        "        \"train_exact_match_topk\",\n",
        "        \"val_exact_match_topk\",\n",
        "        f\"Exact Match (Top-{topk})\",\n",
        "        f\"biobert_exactmatch_topk_client{client_id}.png\",\n",
        "        \"Exact Match\",\n",
        "    )\n",
        "\n",
        "\n",
        "def train_eval_biobert(\n",
        "    model: nn.Module,\n",
        "    train_loader: DataLoader,\n",
        "    val_loader: DataLoader,\n",
        "    label_names: List[str],\n",
        "    train_dataset: Dataset,\n",
        "    epochs: int = 20,\n",
        "    lr: float = 2e-5,\n",
        "    warmup_ratio: float = 0.1,\n",
        "    max_len: int = 512,\n",
        "    client_id: int = 0,\n",
        "    topk: int = 5,\n",
        "    use_pos_weight: bool = True,\n",
        ") -> Tuple[Dict[str, List[float]], str]:\n",
        "    num_labels = len(label_names)\n",
        "\n",
        "    pos_weight = (\n",
        "        compute_pos_weight(train_dataset, num_labels).to(device)\n",
        "        if use_pos_weight\n",
        "        else None\n",
        "    )\n",
        "    criterion = (\n",
        "        nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "        if pos_weight is not None\n",
        "        else nn.BCEWithLogitsLoss()\n",
        "    )\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    total_steps = len(train_loader) * epochs\n",
        "    warmup_steps = int(warmup_ratio * total_steps)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=warmup_steps,\n",
        "        num_training_steps=total_steps,\n",
        "    )\n",
        "\n",
        "    scaler = torch.amp.GradScaler(\"cuda\", enabled=(device.type == \"cuda\"))\n",
        "    model.to(device)\n",
        "\n",
        "    best_val_f1 = -1.0\n",
        "    best_path = os.path.join(\n",
        "        OUT_MODELS_DIR, f\"best_biobert_client{client_id}.pt\"\n",
        "    )\n",
        "\n",
        "    history: Dict[str, List[float]] = {\n",
        "        \"train_loss\": [],\n",
        "        \"val_loss\": [],\n",
        "        \"train_f1_macro_topk\": [],\n",
        "        \"val_f1_macro_topk\": [],\n",
        "        \"train_f1_micro_topk\": [],\n",
        "        \"val_f1_micro_topk\": [],\n",
        "        \"train_f1_samples_topk\": [],\n",
        "        \"val_f1_samples_topk\": [],\n",
        "        \"train_exact_match_topk\": [],\n",
        "        \"val_exact_match_topk\": [],\n",
        "        \"train_p_at_k\": [],\n",
        "        \"val_p_at_k\": [],\n",
        "        \"train_r_at_k\": [],\n",
        "        \"val_r_at_k\": [],\n",
        "        \"train_f1_macro_thr\": [],\n",
        "        \"val_f1_macro_thr\": [],\n",
        "    }\n",
        "\n",
        "    log_rows: List[Dict[str, object]] = []\n",
        "\n",
        "    def _metrics(y_true: np.ndarray, probs: np.ndarray) -> Dict[str, float]:\n",
        "        pred_topk = preds_topk(probs, topk)\n",
        "        f1_macro = f1_score(y_true, pred_topk, average=\"macro\", zero_division=0)\n",
        "        f1_micro = f1_score(y_true, pred_topk, average=\"micro\", zero_division=0)\n",
        "        f1_samples = f1_score(\n",
        "            y_true, pred_topk, average=\"samples\", zero_division=0\n",
        "        )\n",
        "        exact_match = float((y_true == pred_topk).all(axis=1).mean())\n",
        "        p_at_k, r_at_k = precision_recall_at_k(y_true, probs, topk)\n",
        "        pred_thr = (probs > 0.5).astype(np.int32)\n",
        "        f1_macro_thr = f1_score(\n",
        "            y_true, pred_thr, average=\"macro\", zero_division=0\n",
        "        )\n",
        "        return {\n",
        "            \"f1_macro_topk\": float(f1_macro),\n",
        "            \"f1_micro_topk\": float(f1_micro),\n",
        "            \"f1_samples_topk\": float(f1_samples),\n",
        "            \"exact_match_topk\": float(exact_match),\n",
        "            \"p_at_k\": float(p_at_k),\n",
        "            \"r_at_k\": float(r_at_k),\n",
        "            \"f1_macro_thr\": float(f1_macro_thr),\n",
        "        }\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        # ---- Train ----\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        y_true_train, y_prob_train = [], []\n",
        "\n",
        "        for batch in train_loader:\n",
        "            reports = batch[\"report\"]\n",
        "            labels = batch[\"target\"].to(device, non_blocking=True)\n",
        "\n",
        "            enc = tokenize_reports(reports, max_len=max_len)\n",
        "            input_ids = enc[\"input_ids\"].to(device, non_blocking=True)\n",
        "            attention_mask = enc[\"attention_mask\"].to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            with torch.amp.autocast(\"cuda\", enabled=(device.type == \"cuda\")):\n",
        "                logits = model(\n",
        "                    input_ids=input_ids, attention_mask=attention_mask\n",
        "                )\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += float(loss.item())\n",
        "            probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "            y_prob_train.append(probs)\n",
        "            y_true_train.append(labels.detach().cpu().numpy())\n",
        "\n",
        "        y_true_train = np.concatenate(y_true_train, axis=0).astype(np.int32)\n",
        "        y_prob_train = np.concatenate(y_prob_train, axis=0)\n",
        "        trm = _metrics(y_true_train, y_prob_train)\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss / max(1, len(train_loader)))\n",
        "        history[\"train_f1_macro_topk\"].append(trm[\"f1_macro_topk\"])\n",
        "        history[\"train_f1_micro_topk\"].append(trm[\"f1_micro_topk\"])\n",
        "        history[\"train_f1_samples_topk\"].append(trm[\"f1_samples_topk\"])\n",
        "        history[\"train_exact_match_topk\"].append(trm[\"exact_match_topk\"])\n",
        "        history[\"train_p_at_k\"].append(trm[\"p_at_k\"])\n",
        "        history[\"train_r_at_k\"].append(trm[\"r_at_k\"])\n",
        "        history[\"train_f1_macro_thr\"].append(trm[\"f1_macro_thr\"])\n",
        "\n",
        "        # ---- Validation ----\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        y_true_val, y_prob_val = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                reports = batch[\"report\"]\n",
        "                labels = batch[\"target\"].to(device, non_blocking=True)\n",
        "\n",
        "                enc = tokenize_reports(reports, max_len=max_len)\n",
        "                input_ids = enc[\"input_ids\"].to(device, non_blocking=True)\n",
        "                attention_mask = enc[\"attention_mask\"].to(\n",
        "                    device, non_blocking=True\n",
        "                )\n",
        "\n",
        "                logits = model(\n",
        "                    input_ids=input_ids, attention_mask=attention_mask\n",
        "                )\n",
        "                loss = criterion(logits, labels)\n",
        "                val_loss += float(loss.item())\n",
        "\n",
        "                probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "                y_prob_val.append(probs)\n",
        "                y_true_val.append(labels.detach().cpu().numpy())\n",
        "\n",
        "        y_true_val = np.concatenate(y_true_val, axis=0).astype(np.int32)\n",
        "        y_prob_val = np.concatenate(y_prob_val, axis=0)\n",
        "        vam = _metrics(y_true_val, y_prob_val)\n",
        "\n",
        "        history[\"val_loss\"].append(val_loss / max(1, len(val_loader)))\n",
        "        history[\"val_f1_macro_topk\"].append(vam[\"f1_macro_topk\"])\n",
        "        history[\"val_f1_micro_topk\"].append(vam[\"f1_micro_topk\"])\n",
        "        history[\"val_f1_samples_topk\"].append(vam[\"f1_samples_topk\"])\n",
        "        history[\"val_exact_match_topk\"].append(vam[\"exact_match_topk\"])\n",
        "        history[\"val_p_at_k\"].append(vam[\"p_at_k\"])\n",
        "        history[\"val_r_at_k\"].append(vam[\"r_at_k\"])\n",
        "        history[\"val_f1_macro_thr\"].append(vam[\"f1_macro_thr\"])\n",
        "\n",
        "        pred_topk_val = preds_topk(y_prob_val, topk)\n",
        "        f1_per_class = f1_score(\n",
        "            y_true_val, pred_topk_val, average=None, zero_division=0\n",
        "        )\n",
        "        auc_per_class = safe_auc_per_class(y_true_val, y_prob_val)\n",
        "\n",
        "        print(f\"[Epoch {epoch:03d}/{epochs}]\")\n",
        "        print(\n",
        "            f\"Train — Loss {history['train_loss'][-1]:.4f} | \"\n",
        "            f\"F1macro@{topk} {trm['f1_macro_topk']:.4f} | F1micro@{topk} {trm['f1_micro_topk']:.4f} | \"\n",
        "            f\"P@{topk} {trm['p_at_k']:.4f} | R@{topk} {trm['r_at_k']:.4f} | \"\n",
        "            f\"Exact {trm['exact_match_topk']:.4f} | F1macro(thr0.5) {trm['f1_macro_thr']:.4f}\"\n",
        "        )\n",
        "        print(\n",
        "            f\"Val   — Loss {history['val_loss'][-1]:.4f} | \"\n",
        "            f\"F1macro@{topk} {vam['f1_macro_topk']:.4f} | F1micro@{topk} {vam['f1_micro_topk']:.4f} | \"\n",
        "            f\"P@{topk} {vam['p_at_k']:.4f} | R@{topk} {vam['r_at_k']:.4f} | \"\n",
        "            f\"Exact {vam['exact_match_topk']:.4f} | F1macro(thr0.5) {vam['f1_macro_thr']:.4f}\"\n",
        "        )\n",
        "\n",
        "        valid_auc_mask = ~np.isnan(auc_per_class)\n",
        "        valid_idx = np.where(valid_auc_mask)[0]\n",
        "        print(\n",
        "            f\"AUC valid for {int(valid_auc_mask.sum())}/{len(auc_per_class)} classes in validation.\"\n",
        "        )\n",
        "        print(\"Val per-class F1/AUC (first 5 VALID):\")\n",
        "        for j in valid_idx[:5]:\n",
        "            print(\n",
        "                f\"  {label_names[j]}: F1={float(f1_per_class[j]):.4f}, AUC={float(auc_per_class[j]):.4f}\"\n",
        "            )\n",
        "        if len(valid_idx) == 0:\n",
        "            print(\"  (No valid AUC classes in this validation split.)\")\n",
        "        print()\n",
        "\n",
        "        saved_ckpt = False\n",
        "        if vam[\"f1_macro_topk\"] > best_val_f1:\n",
        "            best_val_f1 = float(vam[\"f1_macro_topk\"])\n",
        "            torch.save(model.state_dict(), best_path)\n",
        "            saved_ckpt = True\n",
        "            print(\n",
        "                f\"[Checkpoint] Saved best BioBERT (Val F1macro@{topk}={best_val_f1:.4f}) -> {best_path}\\n\"\n",
        "            )\n",
        "\n",
        "        log_rows.append(\n",
        "            {\n",
        "                \"epoch\": epoch,\n",
        "                \"train_loss\": history[\"train_loss\"][-1],\n",
        "                \"val_loss\": history[\"val_loss\"][-1],\n",
        "                \"train_f1_macro_topk\": history[\"train_f1_macro_topk\"][-1],\n",
        "                \"val_f1_macro_topk\": history[\"val_f1_macro_topk\"][-1],\n",
        "                \"train_f1_micro_topk\": history[\"train_f1_micro_topk\"][-1],\n",
        "                \"val_f1_micro_topk\": history[\"val_f1_micro_topk\"][-1],\n",
        "                \"train_f1_samples_topk\": history[\"train_f1_samples_topk\"][-1],\n",
        "                \"val_f1_samples_topk\": history[\"val_f1_samples_topk\"][-1],\n",
        "                \"train_exact_match_topk\": history[\"train_exact_match_topk\"][-1],\n",
        "                \"val_exact_match_topk\": history[\"val_exact_match_topk\"][-1],\n",
        "                \"train_p_at_k\": history[\"train_p_at_k\"][-1],\n",
        "                \"val_p_at_k\": history[\"val_p_at_k\"][-1],\n",
        "                \"train_r_at_k\": history[\"train_r_at_k\"][-1],\n",
        "                \"val_r_at_k\": history[\"val_r_at_k\"][-1],\n",
        "                \"train_f1_macro_thr\": history[\"train_f1_macro_thr\"][-1],\n",
        "                \"val_f1_macro_thr\": history[\"val_f1_macro_thr\"][-1],\n",
        "                \"saved_ckpt\": saved_ckpt,\n",
        "                \"marker\": \"X\" if saved_ckpt else \"\",\n",
        "            }\n",
        "        )\n",
        "\n",
        "    metrics_csv = os.path.join(\n",
        "        OUT_GRAPHS_DIR, f\"biobert_metrics_client{client_id}.csv\"\n",
        "    )\n",
        "    pd.DataFrame(log_rows).to_csv(metrics_csv, index=False)\n",
        "    print(f\"[Log] Wrote BioBERT per-epoch metrics CSV -> {metrics_csv}\\n\")\n",
        "\n",
        "    plot_history(history, OUT_GRAPHS_DIR, client_id, topk)\n",
        "    return history, best_path\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load client split\n",
        "    cdir = os.path.join(CLIENT_BASE_DIR, f\"client_{CLIENT_ID}\")\n",
        "    with open(os.path.join(cdir, \"train_pids.json\"), \"r\") as f:\n",
        "        train_pids = json.load(f)\n",
        "    with open(os.path.join(cdir, \"val_pids.json\"), \"r\") as f:\n",
        "        val_pids = json.load(f)\n",
        "\n",
        "    # Build datasets\n",
        "    full_ds = ReportsGliomaDataset(\n",
        "        METADATA_DF_PATH,\n",
        "        LABELS_PATH,\n",
        "        exclude_ids=[\"PatientID_0191\"],\n",
        "        remove_background=REMOVE_BACKGROUND_LABEL,\n",
        "    )\n",
        "    label_names = full_ds.labels\n",
        "\n",
        "    train_dataset = SubsetByPIDs(full_ds, train_pids)\n",
        "    val_dataset = SubsetByPIDs(full_ds, val_pids)\n",
        "\n",
        "    # DataLoaders\n",
        "    g = torch.Generator().manual_seed(SEED)\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "        collate_fn=reports_collate_fn,\n",
        "        worker_init_fn=worker_init_fn,\n",
        "        generator=g,\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "        collate_fn=reports_collate_fn,\n",
        "        worker_init_fn=worker_init_fn,\n",
        "        generator=g,\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"Loaded client_{CLIENT_ID}: \"\n",
        "        f\"train patients={len(train_dataset)}, val patients={len(val_dataset)}, num_labels={len(label_names)}\"\n",
        "    )\n",
        "\n",
        "    # Model + training\n",
        "    model = BioBERTMultiLabelClassifier(\n",
        "        BERT_MODEL_NAME,\n",
        "        num_labels=len(label_names),\n",
        "        dropout=DROPOUT,\n",
        "    )\n",
        "\n",
        "    history, best_model_path = train_eval_biobert(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        label_names=label_names,\n",
        "        train_dataset=train_dataset,\n",
        "        epochs=EPOCHS,\n",
        "        lr=LR,\n",
        "        warmup_ratio=WARMUP_RATIO,\n",
        "        max_len=MAX_LEN,\n",
        "        client_id=CLIENT_ID,\n",
        "        topk=TOPK_PRED,\n",
        "        use_pos_weight=USE_POS_WEIGHT,\n",
        "    )\n",
        "\n",
        "    print(f\"[Done] Best BioBERT model saved at: {best_model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGF1_XihLEBQ"
      },
      "source": [
        "**<h2>Clip-like model<h2>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e4SPJvVHVzde",
        "outputId": "c10bb036-2556-4847-94be-bd4a87513e64"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import random\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "\n",
        "import segmentation_models_pytorch as smp\n",
        "from sklearn.metrics import jaccard_score\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# Global config\n",
        "# ------------------------\n",
        "SEED = 42\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# Reproducibility helpers\n",
        "# ------------------------\n",
        "def set_seed(seed: int = 42) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
        "\n",
        "\n",
        "def worker_init_fn(worker_id: int) -> None:\n",
        "    # Make each dataloader worker deterministic\n",
        "    s = SEED + worker_id\n",
        "    np.random.seed(s)\n",
        "    random.seed(s)\n",
        "\n",
        "\n",
        "set_seed(SEED)\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# Small utilities\n",
        "# ------------------------\n",
        "def minmax01(x: np.ndarray) -> np.ndarray:\n",
        "    # Min-max normalize to [0, 1]\n",
        "    x = x.astype(np.float32)\n",
        "    mn = float(np.min(x))\n",
        "    mx = float(np.max(x))\n",
        "    if mx <= mn:\n",
        "        return np.zeros_like(x, dtype=np.float32)\n",
        "    return (x - mn) / (mx - mn)\n",
        "\n",
        "\n",
        "def find_first_existing(paths: List[str]) -> str:\n",
        "    # Pick the first path that exists\n",
        "    for p in paths:\n",
        "        if p and os.path.exists(p):\n",
        "            return p\n",
        "    raise FileNotFoundError(\"None of these paths exist:\\n\" + \"\\n\".join(paths))\n",
        "\n",
        "\n",
        "def ensure_dir(p: str) -> None:\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# Dataset: loads MRI + atlas regions + tumor mask + report text\n",
        "# Also builds a multi-label target from \"Top 5 Regions\"\n",
        "# ------------------------\n",
        "class GliomaDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        metadata_df_path: str,\n",
        "        labels_path: str,\n",
        "        data_root: str,\n",
        "        exclude_ids: Optional[List[str]] = None,\n",
        "        remove_background: bool = False,\n",
        "    ):\n",
        "        # Load metadata dataframe (contains report + labels per patient)\n",
        "        with open(metadata_df_path, \"rb\") as f:\n",
        "            df = pickle.load(f)\n",
        "\n",
        "        if exclude_ids is None:\n",
        "            exclude_ids = [\"PatientID_0191\"]\n",
        "\n",
        "        # Filter invalid rows and excluded patients\n",
        "        df = df[~df[\"Patient_ID\"].isin(exclude_ids)].reset_index(drop=True)\n",
        "        df = df[df[\"Report\"].notna() & df[\"Top 5 Regions\"].notna()].reset_index(\n",
        "            drop=True\n",
        "        )\n",
        "\n",
        "        # Load label list\n",
        "        with open(labels_path, \"rb\") as f:\n",
        "            labels = list(pickle.load(f))\n",
        "\n",
        "        if remove_background:\n",
        "            labels = [\n",
        "                l for l in labels if str(l).strip().lower() != \"background\"\n",
        "            ]\n",
        "\n",
        "        self.labels = labels\n",
        "        self.label_to_idx = {lab: i for i, lab in enumerate(self.labels)}\n",
        "\n",
        "        self.data_root = data_root\n",
        "        self.df = df\n",
        "\n",
        "        # Encode some categorical columns\n",
        "        self.categorical_cols = [\n",
        "            \"Sex at Birth\",\n",
        "            \"Race\",\n",
        "            \"Primary Diagnosis\",\n",
        "            \"Previous Brain Tumor\",\n",
        "            \"Type of previous brain tumor\",\n",
        "            \"Age Range\",\n",
        "        ]\n",
        "        self.code_maps: Dict[str, Dict[int, Any]] = {}\n",
        "        for col in self.categorical_cols:\n",
        "            cat = pd.Categorical(self.df[col])\n",
        "            self.df[col + \"_code\"] = cat.codes.astype(np.int64)\n",
        "            self.code_maps[col] = dict(enumerate(cat.categories))\n",
        "\n",
        "        # Keep only patients that have all required .npy files\n",
        "        self.patient_ids: List[str] = []\n",
        "        for pid in self.df[\"Patient_ID\"].tolist():\n",
        "            base = os.path.join(self.data_root, pid)\n",
        "            mri_p = os.path.join(base, f\"{pid}_mri.npy\")\n",
        "            reg_p = os.path.join(base, f\"{pid}_regions.npy\")\n",
        "            tumor_p = os.path.join(base, f\"{pid}_tumor.npy\")\n",
        "            if (\n",
        "                os.path.isfile(mri_p)\n",
        "                and os.path.isfile(reg_p)\n",
        "                and os.path.isfile(tumor_p)\n",
        "            ):\n",
        "                self.patient_ids.append(pid)\n",
        "\n",
        "        self.df = self.df[\n",
        "            self.df[\"Patient_ID\"].isin(self.patient_ids)\n",
        "        ].reset_index(drop=True)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.df)\n",
        "\n",
        "    def _make_target_regions(self, regions_list: Any) -> np.ndarray:\n",
        "        # Multi-label vector over all region labels\n",
        "        y = np.zeros(len(self.labels), dtype=np.float32)\n",
        "        if not isinstance(regions_list, (list, tuple)):\n",
        "            return y\n",
        "        for reg in regions_list:\n",
        "            idx = self.label_to_idx.get(reg, None)\n",
        "            if idx is not None:\n",
        "                y[idx] = 1.0\n",
        "        return y\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
        "        row = self.df.iloc[idx]\n",
        "        pid = row[\"Patient_ID\"]\n",
        "        base = os.path.join(self.data_root, pid)\n",
        "\n",
        "        # Load arrays\n",
        "        mri = np.load(os.path.join(base, f\"{pid}_mri.npy\")).astype(np.float32)\n",
        "        regions = np.load(os.path.join(base, f\"{pid}_regions.npy\")).astype(\n",
        "            np.float32\n",
        "        )\n",
        "        tumor = np.load(os.path.join(base, f\"{pid}_tumor.npy\")).astype(\n",
        "            np.float32\n",
        "        )\n",
        "\n",
        "        # Normalize inputs, binarize mask\n",
        "        mri = minmax01(mri)\n",
        "        regions = minmax01(regions)\n",
        "        tumor = (tumor > 0.5).astype(np.float32)\n",
        "\n",
        "        target_regions = self._make_target_regions(row[\"Top 5 Regions\"])\n",
        "\n",
        "        return {\n",
        "            \"patient_id\": pid,\n",
        "            \"mri\": mri,\n",
        "            \"regions\": regions,\n",
        "            \"tumor\": tumor,\n",
        "            \"report\": str(row[\"Report\"]),\n",
        "            \"target_regions\": target_regions,\n",
        "            # Extra metadata\n",
        "            \"sex\": int(row[\"Sex at Birth_code\"]),\n",
        "            \"race\": int(row[\"Race_code\"]),\n",
        "            \"age\": float(row[\"Age at diagnosis\"]),\n",
        "            \"primary_diagnosis\": int(row[\"Primary Diagnosis_code\"]),\n",
        "            \"h3_3a_mutation\": float(row[\"H3-3A mutation\"]),\n",
        "            \"pten_mutation\": float(row[\"PTEN mutation\"]),\n",
        "            \"CDKN2A_B_deletion\": float(row[\"CDKN2A/B deletion\"]),\n",
        "            \"TP53_alteration\": float(row[\"TP53 alteration\"]),\n",
        "            \"other_mutations_alterations\": row[\"Other mutations/alterations\"],\n",
        "            \"previous_brain_tumor\": int(row[\"Previous Brain Tumor_code\"]),\n",
        "            \"type_of_previous_brain_tumor\": int(\n",
        "                row[\"Type of previous brain tumor_code\"]\n",
        "            ),\n",
        "            \"age_range\": int(row[\"Age Range_code\"]),\n",
        "        }\n",
        "\n",
        "\n",
        "def glioma_collate_fn(batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "    # Build tensors for MRI/regions/mask and keep report text as list\n",
        "    mri = (\n",
        "        torch.from_numpy(np.stack([b[\"mri\"] for b in batch]))\n",
        "        .float()\n",
        "        .unsqueeze(1)\n",
        "    )\n",
        "    regions = (\n",
        "        torch.from_numpy(np.stack([b[\"regions\"] for b in batch]))\n",
        "        .float()\n",
        "        .unsqueeze(1)\n",
        "    )\n",
        "    tumor = (\n",
        "        torch.from_numpy(np.stack([b[\"tumor\"] for b in batch]))\n",
        "        .float()\n",
        "        .unsqueeze(1)\n",
        "    )\n",
        "    target_regions = torch.from_numpy(\n",
        "        np.stack([b[\"target_regions\"] for b in batch])\n",
        "    ).float()\n",
        "\n",
        "    return {\n",
        "        \"patient_id\": [b[\"patient_id\"] for b in batch],\n",
        "        \"report\": [b[\"report\"] for b in batch],\n",
        "        \"mri\": mri,\n",
        "        \"regions\": regions,\n",
        "        \"tumor\": tumor,\n",
        "        \"target_regions\": target_regions,\n",
        "    }\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# Text encoder: BioClinicalBERT + linear head (head not used later)\n",
        "# ------------------------\n",
        "class BioBERTMultiLabelClassifier(nn.Module):\n",
        "    def __init__(self, model_name: str, num_labels: int, dropout: float = 0.3):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(\n",
        "        self, input_ids: torch.Tensor, attention_mask: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled = out.pooler_output\n",
        "        if pooled is None:\n",
        "            pooled = out.last_hidden_state[:, 0]  # CLS\n",
        "        x = self.dropout(pooled)\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "def tokenize_reports(\n",
        "    tokenizer: AutoTokenizer, reports: List[str], max_len: int = 512\n",
        ") -> Dict[str, torch.Tensor]:\n",
        "    # Tokenize a list of report texts for BERT\n",
        "    return tokenizer(\n",
        "        reports,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_len,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "\n",
        "def dice_iou_acc(\n",
        "    y_true: np.ndarray, y_pred: np.ndarray\n",
        ") -> Tuple[float, float, float]:\n",
        "    # Compute Dice, IoU and pixel accuracy for segmentation masks\n",
        "    y_true = y_true.astype(np.uint8).reshape(-1)\n",
        "    y_pred = y_pred.astype(np.uint8).reshape(-1)\n",
        "    inter = int((y_true * y_pred).sum())\n",
        "    dice = (2.0 * inter) / (float(y_true.sum() + y_pred.sum()) + 1e-8)\n",
        "    try:\n",
        "        iou = float(jaccard_score(y_true, y_pred, average=\"binary\"))\n",
        "    except Exception:\n",
        "        union = float(y_true.sum() + y_pred.sum() - inter) + 1e-8\n",
        "        iou = float(inter / union)\n",
        "    acc = float((y_true == y_pred).mean())\n",
        "    return float(dice), float(iou), float(acc)\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# CLIP-like model: aligns text embedding with image embedding\n",
        "# and also predicts segmentation mask with UNet\n",
        "# ------------------------\n",
        "class ClipModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        bert_backbone: AutoModel,\n",
        "        tokenizer: AutoTokenizer,\n",
        "        unet_model: nn.Module,\n",
        "        embed_dim: int = 512,\n",
        "        max_len: int = 512,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.bert_backbone = bert_backbone\n",
        "        self.tokenizer = tokenizer\n",
        "        self.unet_model = unet_model\n",
        "        self.max_len = max_len\n",
        "\n",
        "        # Projection layers to a shared embedding space\n",
        "        self.text_projection = nn.Linear(\n",
        "            self.bert_backbone.config.hidden_size, embed_dim\n",
        "        )\n",
        "        self.image_projection = nn.Linear(512, embed_dim)\n",
        "\n",
        "        # CLIP temperature parameter and uncertainty weights for multitask loss\n",
        "        self.logit_scale = nn.Parameter(\n",
        "            torch.tensor(np.log(1 / 0.07), dtype=torch.float32)\n",
        "        )\n",
        "        self.log_sigma_clip = nn.Parameter(torch.tensor(0.0))\n",
        "        self.log_sigma_seg = nn.Parameter(torch.tensor(0.0))\n",
        "\n",
        "    def encode_text(self, texts: List[str]) -> torch.Tensor:\n",
        "        # Encode reports -> normalized text embeddings\n",
        "        enc = tokenize_reports(self.tokenizer, texts, max_len=self.max_len)\n",
        "        input_ids = enc[\"input_ids\"].to(DEVICE, non_blocking=True)\n",
        "        attention_mask = enc[\"attention_mask\"].to(DEVICE, non_blocking=True)\n",
        "\n",
        "        out = self.bert_backbone(\n",
        "            input_ids=input_ids, attention_mask=attention_mask\n",
        "        )\n",
        "        pooled = out.pooler_output\n",
        "        if pooled is None:\n",
        "            pooled = out.last_hidden_state[:, 0]\n",
        "\n",
        "        t = self.text_projection(pooled)\n",
        "        return F.normalize(t, dim=-1)\n",
        "\n",
        "    def encode_image(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # Encode image -> normalized image embeddings\n",
        "        feats = self.unet_model.encoder(x)[-1]\n",
        "        v = feats.mean(dim=(2, 3))\n",
        "        v = self.image_projection(v)\n",
        "        return F.normalize(v, dim=-1)\n",
        "\n",
        "    def forward(\n",
        "        self, texts: List[str], mri: torch.Tensor, regions: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        # Build 2-channel input (MRI + atlas regions)\n",
        "        x = torch.cat([mri, regions], dim=1).to(DEVICE, non_blocking=True)\n",
        "\n",
        "        # CLIP embeddings + segmentation logits\n",
        "        t = self.encode_text(texts)\n",
        "        v = self.encode_image(x)\n",
        "        seg_logits = self.unet_model(x)\n",
        "        return t, v, seg_logits\n",
        "\n",
        "    def clip_contrastive_loss(\n",
        "        self, t: torch.Tensor, v: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        # Standard CLIP-style symmetric cross-entropy loss over batch similarities\n",
        "        logit_scale = self.logit_scale.exp().clamp(1e-3, 100.0)\n",
        "        logits = (t @ v.t()) * logit_scale\n",
        "        labels = torch.arange(t.size(0), device=logits.device)\n",
        "        return 0.5 * (\n",
        "            F.cross_entropy(logits, labels)\n",
        "            + F.cross_entropy(logits.t(), labels)\n",
        "        )\n",
        "\n",
        "    def combined_loss(\n",
        "        self,\n",
        "        t: torch.Tensor,\n",
        "        v: torch.Tensor,\n",
        "        seg_logits: torch.Tensor,\n",
        "        seg_target: torch.Tensor,\n",
        "        seg_criterion: nn.Module,\n",
        "    ) -> torch.Tensor:\n",
        "        # Multi-task loss with learned uncertainty weighting\n",
        "        clip_loss = self.clip_contrastive_loss(t, v)\n",
        "        seg_loss = seg_criterion(\n",
        "            seg_logits,\n",
        "            seg_target.to(seg_logits.device, non_blocking=True).float(),\n",
        "        )\n",
        "        return (\n",
        "            (1.0 / (2.0 * torch.exp(self.log_sigma_clip) ** 2)) * clip_loss\n",
        "            + (1.0 / (2.0 * torch.exp(self.log_sigma_seg) ** 2)) * seg_loss\n",
        "            + self.log_sigma_clip\n",
        "            + self.log_sigma_seg\n",
        "        )\n",
        "\n",
        "\n",
        "def load_biobert_backbone_only(\n",
        "    bert_wrapper: BioBERTMultiLabelClassifier, ckpt_path: str\n",
        ") -> None:\n",
        "    # Load only BERT weights (skip classifier head)\n",
        "    sd = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "    sd = {k: v for k, v in sd.items() if not k.startswith(\"classifier.\")}\n",
        "    bert_wrapper.load_state_dict(sd, strict=False)\n",
        "\n",
        "\n",
        "def build_unet(\n",
        "    in_channels: int = 2,\n",
        "    encoder_name: str = \"resnet34\",\n",
        "    encoder_weights: Optional[str] = None,\n",
        ") -> nn.Module:\n",
        "    # UNet used both for segmentation and as image feature encoder\n",
        "    return smp.Unet(\n",
        "        encoder_name=encoder_name,\n",
        "        encoder_weights=encoder_weights,\n",
        "        in_channels=in_channels,\n",
        "        classes=1,\n",
        "        activation=None,\n",
        "    )\n",
        "\n",
        "\n",
        "def train_one_epoch_clip(\n",
        "    model: ClipModel,\n",
        "    loader: DataLoader,\n",
        "    optimizer: optim.Optimizer,\n",
        "    seg_criterion: nn.Module,\n",
        "    scaler: torch.amp.GradScaler,\n",
        ") -> Dict[str, float]:\n",
        "    # One epoch of training (contrastive + segmentation)\n",
        "    model.train()\n",
        "    tot_loss = tot_dice = tot_iou = tot_acc = 0.0\n",
        "    n = 0\n",
        "\n",
        "    for batch in loader:\n",
        "        texts = batch[\"report\"]\n",
        "        mri = batch[\"mri\"].to(DEVICE, non_blocking=True)\n",
        "        regions = batch[\"regions\"].to(DEVICE, non_blocking=True)\n",
        "        tumor = batch[\"tumor\"].to(DEVICE, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with torch.amp.autocast(\"cuda\", enabled=(DEVICE.type == \"cuda\")):\n",
        "            t, v, seg_logits = model(texts, mri, regions)\n",
        "            loss = model.combined_loss(t, v, seg_logits, tumor, seg_criterion)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            preds = (\n",
        "                torch.sigmoid(seg_logits).detach().cpu().numpy() > 0.5\n",
        "            ).astype(np.uint8)\n",
        "            y_np = (tumor.detach().cpu().numpy() > 0.5).astype(np.uint8)\n",
        "            d, i, a = dice_iou_acc(y_np, preds)\n",
        "\n",
        "        tot_loss += float(loss.item())\n",
        "        tot_dice += d\n",
        "        tot_iou += i\n",
        "        tot_acc += a\n",
        "        n += 1\n",
        "\n",
        "    return {\n",
        "        \"loss\": tot_loss / max(1, n),\n",
        "        \"dice\": tot_dice / max(1, n),\n",
        "        \"iou\": tot_iou / max(1, n),\n",
        "        \"acc\": tot_acc / max(1, n),\n",
        "    }\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_one_epoch_clip(\n",
        "    model: ClipModel, loader: DataLoader, seg_criterion: nn.Module\n",
        ") -> Dict[str, float]:\n",
        "    # Validation epoch\n",
        "    model.eval()\n",
        "    tot_loss = tot_dice = tot_iou = tot_acc = 0.0\n",
        "    n = 0\n",
        "\n",
        "    for batch in loader:\n",
        "        texts = batch[\"report\"]\n",
        "        mri = batch[\"mri\"].to(DEVICE, non_blocking=True)\n",
        "        regions = batch[\"regions\"].to(DEVICE, non_blocking=True)\n",
        "        tumor = batch[\"tumor\"].to(DEVICE, non_blocking=True)\n",
        "\n",
        "        t, v, seg_logits = model(texts, mri, regions)\n",
        "        loss = model.combined_loss(t, v, seg_logits, tumor, seg_criterion)\n",
        "\n",
        "        preds = (torch.sigmoid(seg_logits).detach().cpu().numpy() > 0.5).astype(\n",
        "            np.uint8\n",
        "        )\n",
        "        y_np = (tumor.detach().cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        d, i, a = dice_iou_acc(y_np, preds)\n",
        "\n",
        "        tot_loss += float(loss.item())\n",
        "        tot_dice += d\n",
        "        tot_iou += i\n",
        "        tot_acc += a\n",
        "        n += 1\n",
        "\n",
        "    return {\n",
        "        \"loss\": tot_loss / max(1, n),\n",
        "        \"dice\": tot_dice / max(1, n),\n",
        "        \"iou\": tot_iou / max(1, n),\n",
        "        \"acc\": tot_acc / max(1, n),\n",
        "    }\n",
        "\n",
        "\n",
        "def main():\n",
        "    candidates_metadata = [\n",
        "        \"/content/drive/MyDrive/PKG - MU-Glioma-Post/cleaned_df.pkl\",\n",
        "        \"/content/cleaned_df.pkl\",\n",
        "    ]\n",
        "    candidates_labels = [\n",
        "        \"/content/drive/MyDrive/PKG - MU-Glioma-Post/labels_list.pkl\",\n",
        "        \"/content/labels_list.pkl\",\n",
        "    ]\n",
        "    candidates_data_root = [\n",
        "        \"/content/drive/MyDrive/PKG - MU-Glioma-Post/Preprocessed-Data\",\n",
        "        \"/content/Preprocessed-Data\",\n",
        "    ]\n",
        "\n",
        "    METADATA_DF_PATH = find_first_existing(candidates_metadata)\n",
        "    LABELS_PATH = find_first_existing(candidates_labels)\n",
        "    DATA_ROOT = find_first_existing(candidates_data_root)\n",
        "\n",
        "    BERT_MODEL_NAME = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "\n",
        "    # Optional pretrained checkpoints\n",
        "    BIOBERT_BEST_CAND = [\n",
        "        \"/content/drive/MyDrive/PKG - MU-Glioma-Post/Models/BioClinicalBert/best.pt\",\n",
        "        \"/content/AITDM/Models/BioClinicalBert/best_biobert_client0.pt\",\n",
        "    ]\n",
        "    UNET_BEST_CAND = [\n",
        "        \"/content/drive/MyDrive/PKG - MU-Glioma-Post/Models/UNet/best_unet_model.pth\",\n",
        "        \"/content/AITDM/Models/UNet_ImageOnly/best_unet_client0.pth\",\n",
        "        \"/content/AITDM/Models/UNet_ImageOnly/best_unet_client0.pth\",\n",
        "    ]\n",
        "\n",
        "    BIOBERT_BEST = next((p for p in BIOBERT_BEST_CAND if os.path.isfile(p)), \"\")\n",
        "    UNET_BEST = next((p for p in UNET_BEST_CAND if os.path.isfile(p)), \"\")\n",
        "\n",
        "    # Output checkpoint for the CLIP-like model\n",
        "    CLIP_SAVE = \"/content/drive/MyDrive/PKG - MU-Glioma-Post/Models/CLIP_Based/best_clip_model.pth\"\n",
        "    ensure_dir(os.path.dirname(CLIP_SAVE))\n",
        "\n",
        "    # Training hyperparams\n",
        "    BATCH_SIZE = 4\n",
        "    EPOCHS = 50\n",
        "    LR = 1e-4\n",
        "    MAX_LEN = 512\n",
        "\n",
        "    print(\"Resolved paths:\")\n",
        "    print(\"  metadata:\", METADATA_DF_PATH)\n",
        "    print(\"  labels  :\", LABELS_PATH)\n",
        "    print(\"  data_root:\", DATA_ROOT)\n",
        "    print(\n",
        "        \"  biobert ckpt:\",\n",
        "        BIOBERT_BEST if BIOBERT_BEST else \"(not found, will use base)\",\n",
        "    )\n",
        "    print(\n",
        "        \"  unet ckpt  :\",\n",
        "        UNET_BEST if UNET_BEST else \"(not found, will use random init)\",\n",
        "    )\n",
        "\n",
        "    # Build dataset and split train/test\n",
        "    dataset = GliomaDataset(\n",
        "        metadata_df_path=METADATA_DF_PATH,\n",
        "        labels_path=LABELS_PATH,\n",
        "        data_root=DATA_ROOT,\n",
        "        exclude_ids=[\"PatientID_0191\"],\n",
        "        remove_background=False,\n",
        "    )\n",
        "\n",
        "    test_size = int(0.2 * len(dataset))\n",
        "    train_size = len(dataset) - test_size\n",
        "    train_dataset, test_dataset = random_split(\n",
        "        dataset,\n",
        "        [train_size, test_size],\n",
        "        generator=torch.Generator().manual_seed(SEED),\n",
        "    )\n",
        "\n",
        "    # Dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        collate_fn=glioma_collate_fn,\n",
        "        worker_init_fn=worker_init_fn,\n",
        "        drop_last=True,\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        collate_fn=glioma_collate_fn,\n",
        "        worker_init_fn=worker_init_fn,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    # Tokenizer + BERT backbone\n",
        "    tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
        "    bert_wrapper = BioBERTMultiLabelClassifier(\n",
        "        BERT_MODEL_NAME, num_labels=len(dataset.labels), dropout=0.3\n",
        "    )\n",
        "    if BIOBERT_BEST:\n",
        "        try:\n",
        "            bert_wrapper.load_state_dict(\n",
        "                torch.load(BIOBERT_BEST, map_location=\"cpu\")\n",
        "            )\n",
        "        except RuntimeError:\n",
        "            load_biobert_backbone_only(bert_wrapper, BIOBERT_BEST)\n",
        "    bert_backbone = bert_wrapper.bert.to(DEVICE)\n",
        "\n",
        "    # UNet\n",
        "    unet_model = build_unet(\n",
        "        in_channels=2, encoder_name=\"resnet34\", encoder_weights=None\n",
        "    )\n",
        "    if UNET_BEST:\n",
        "        unet_model.load_state_dict(torch.load(UNET_BEST, map_location=\"cpu\"))\n",
        "    unet_model = unet_model.to(DEVICE)\n",
        "\n",
        "    # Build CLIP-like multimodal model\n",
        "    clip_model = ClipModel(\n",
        "        bert_backbone=bert_backbone,\n",
        "        tokenizer=tokenizer,\n",
        "        unet_model=unet_model,\n",
        "        embed_dim=512,\n",
        "        max_len=MAX_LEN,\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    # Loss/optimizer/scheduler\n",
        "    seg_criterion = smp.losses.DiceLoss(\n",
        "        smp.losses.BINARY_MODE, from_logits=True\n",
        "    )\n",
        "    optimizer = optim.AdamW(clip_model.parameters(), lr=LR)\n",
        "\n",
        "    total_steps = len(train_loader) * EPOCHS\n",
        "    warmup_steps = int(0.1 * total_steps)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    scaler = torch.amp.GradScaler(\"cuda\", enabled=(DEVICE.type == \"cuda\"))\n",
        "\n",
        "    # Track best checkpoint by (Dice up) and (loss down)\n",
        "    best_val_dice = -1.0\n",
        "    best_val_loss = float(\"inf\")\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        tr = train_one_epoch_clip(\n",
        "            clip_model, train_loader, optimizer, seg_criterion, scaler\n",
        "        )\n",
        "        va = eval_one_epoch_clip(clip_model, test_loader, seg_criterion)\n",
        "\n",
        "        # Step LR scheduler once per training step (done here in a loop)\n",
        "        for _ in range(len(train_loader)):\n",
        "            scheduler.step()\n",
        "\n",
        "        print(\n",
        "            f\"[Epoch {epoch:03d}/{EPOCHS}] \"\n",
        "            f\"Train loss {tr['loss']:.4f} dice {tr['dice']:.4f} iou {tr['iou']:.4f} acc {tr['acc']:.4f} || \"\n",
        "            f\"Val loss {va['loss']:.4f} dice {va['dice']:.4f} iou {va['iou']:.4f} acc {va['acc']:.4f}\"\n",
        "        )\n",
        "\n",
        "        # Save best model\n",
        "        if (va[\"dice\"] > best_val_dice) and (va[\"loss\"] < best_val_loss):\n",
        "            best_val_dice = va[\"dice\"]\n",
        "            best_val_loss = va[\"loss\"]\n",
        "            torch.save(clip_model.state_dict(), CLIP_SAVE)\n",
        "            print(\n",
        "                f\"Saved best model -> {CLIP_SAVE} (val dice {best_val_dice:.4f}, val loss {best_val_loss:.4f})\"\n",
        "            )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXoXxVZ4Lig_"
      },
      "source": [
        "# **M2 - Ensemble on Images only**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgL9BfPx8seu",
        "outputId": "aac8a9e4-5f6f-4cd8-a71a-7fe797cb1e47"
      },
      "outputs": [],
      "source": [
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "print(list(smp.encoders.get_encoder_names()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtwsveySU7ft"
      },
      "source": [
        "**<h2>UNet - \"resnet50\"<h2>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BSXYT79OULsR",
        "outputId": "fdf8d893-4262-4820-cb57-0c57b2c1f88a"
      },
      "outputs": [],
      "source": [
        "import os, json, pickle, random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "from sklearn.metrics import jaccard_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "# Paths and I/O config\n",
        "DATA_ROOT = \"/content/Preprocessed-Data\"\n",
        "METADATA_DF_PATH = \"cleaned_df.pkl\"\n",
        "OUT_BASE = \"AITDM\"\n",
        "CLIENT_DIR = os.path.join(OUT_BASE, \"/content/client\")\n",
        "\n",
        "# Experiment config\n",
        "USE_ATLAS = True\n",
        "CLIENT_ID = 2\n",
        "BATCH_SIZE = 8\n",
        "LR = 1e-3\n",
        "EPOCHS = 50\n",
        "NUM_WORKERS = 2\n",
        "SEED = 42\n",
        "\n",
        "# Model / encoder config\n",
        "ENCODER_NAME = \"resnet50\"\n",
        "ENCODER_WEIGHTS = \"imagenet\"\n",
        "\n",
        "# Output directories\n",
        "OUT_MODELS_DIR = os.path.join(OUT_BASE, \"Models\", \"UNet_ImageOnly\")\n",
        "OUT_GRAPHS_DIR = os.path.join(OUT_BASE, \"Graphs\")\n",
        "os.makedirs(OUT_MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(OUT_GRAPHS_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "def sanitize(s: str) -> str:\n",
        "    return str(s).replace(\"/\", \"-\").replace(\" \", \"_\")\n",
        "\n",
        "\n",
        "encoder_tag = sanitize(ENCODER_NAME)\n",
        "weights_tag = (\n",
        "    sanitize(ENCODER_WEIGHTS) if ENCODER_WEIGHTS is not None else \"none\"\n",
        ")\n",
        "atlas_tag = \"atlas\" if USE_ATLAS else \"img\"\n",
        "\n",
        "run_tag = f\"unet_{encoder_tag}_{weights_tag}_{atlas_tag}\"\n",
        "\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
        "\n",
        "\n",
        "def worker_init_fn(worker_id):\n",
        "    worker_seed = SEED + worker_id\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "\n",
        "set_seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class ImageOnlyGliomaDataset(Dataset):\n",
        "    \"\"\"Dataset that loads MRI, tumor mask, and optional atlas regions per patient.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        metadata_df_path,\n",
        "        data_root,\n",
        "        use_atlas=True,\n",
        "        exclude_ids=None,\n",
        "        transform=None,\n",
        "    ):\n",
        "        import pickle, os, numpy as np, pandas as pd\n",
        "\n",
        "        # Load metadata dataframe\n",
        "        with open(metadata_df_path, \"rb\") as f:\n",
        "            df = pickle.load(f)\n",
        "\n",
        "        # Optionally exclude some patient IDs\n",
        "        if exclude_ids is None:\n",
        "            exclude_ids = [\"PatientID_0191\"]\n",
        "        self.df = df[~df[\"Patient_ID\"].isin(exclude_ids)].reset_index(drop=True)\n",
        "\n",
        "        self.data_root = data_root\n",
        "        self.use_atlas = use_atlas\n",
        "        self.transform = transform\n",
        "\n",
        "        # Collect patient IDs that have all required files\n",
        "        self.patient_ids = []\n",
        "        for pid in sorted(self.df[\"Patient_ID\"].tolist()):\n",
        "            mri_p = os.path.join(self.data_root, pid, f\"{pid}_mri.npy\")\n",
        "            tumor_p = os.path.join(self.data_root, pid, f\"{pid}_tumor.npy\")\n",
        "\n",
        "            if self.use_atlas:\n",
        "                reg_p = os.path.join(self.data_root, pid, f\"{pid}_regions.npy\")\n",
        "                if (\n",
        "                    os.path.isfile(mri_p)\n",
        "                    and os.path.isfile(tumor_p)\n",
        "                    and os.path.isfile(reg_p)\n",
        "                ):\n",
        "                    self.patient_ids.append(pid)\n",
        "            else:\n",
        "                if os.path.isfile(mri_p) and os.path.isfile(tumor_p):\n",
        "                    self.patient_ids.append(pid)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patient_ids)\n",
        "\n",
        "    @staticmethod\n",
        "    def _minmax(x):\n",
        "        \"\"\"Simple min-max normalization to [0, 1].\"\"\"\n",
        "        x = x.astype(np.float32)\n",
        "        mn, mx = np.min(x), np.max(x)\n",
        "        return (\n",
        "            (x - mn) / (mx - mn)\n",
        "            if mx > mn\n",
        "            else np.zeros_like(x, dtype=np.float32)\n",
        "        )\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Load and return one sample (dict) for a patient.\"\"\"\n",
        "        import os, numpy as np\n",
        "\n",
        "        pid = self.patient_ids[idx]\n",
        "        base = os.path.join(self.data_root, pid)\n",
        "\n",
        "        # Load MRI and tumor mask\n",
        "        mri = np.load(os.path.join(base, f\"{pid}_mri.npy\")).astype(np.float32)\n",
        "        tumor = np.load(os.path.join(base, f\"{pid}_tumor.npy\")).astype(\n",
        "            np.float32\n",
        "        )\n",
        "\n",
        "        # Normalize MRI and binarize tumor mask\n",
        "        mri = self._minmax(mri)\n",
        "        tumor = (tumor > 0.5).astype(np.float32)\n",
        "\n",
        "        sample = {\"patient_id\": pid, \"mri\": mri, \"tumor\": tumor}\n",
        "\n",
        "        # Optionally load regions/atlas\n",
        "        if self.use_atlas:\n",
        "            regions = np.load(os.path.join(base, f\"{pid}_regions.npy\")).astype(\n",
        "                np.float32\n",
        "            )\n",
        "            regions = self._minmax(regions)\n",
        "            sample[\"regions\"] = regions\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "def image_only_collate_fn(batch, use_atlas=True):\n",
        "    \"\"\"Custom collate: stack MRI (+ optional regions) and tumor into tensors.\"\"\"\n",
        "    mri = torch.stack([torch.tensor(it[\"mri\"]) for it in batch]).unsqueeze(1)\n",
        "    y = torch.stack([torch.tensor(it[\"tumor\"]) for it in batch]).unsqueeze(1)\n",
        "\n",
        "    if use_atlas:\n",
        "        regs = torch.stack(\n",
        "            [torch.tensor(it[\"regions\"]) for it in batch]\n",
        "        ).unsqueeze(1)\n",
        "        # Concatenate MRI and regions as channels\n",
        "        x = torch.cat([mri.float(), regs.float()], dim=1)\n",
        "    else:\n",
        "        x = mri.float()\n",
        "\n",
        "    return {\"x\": x, \"y\": y.float(), \"pid\": [it[\"patient_id\"] for it in batch]}\n",
        "\n",
        "\n",
        "class SubsetByPIDs(Dataset):\n",
        "    \"\"\"Wrap a dataset but keep only a subset of patient IDs.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, full_dataset: ImageOnlyGliomaDataset, pid_list: List[str]\n",
        "    ):\n",
        "        self.ds = full_dataset\n",
        "        # Map patient IDs to indices\n",
        "        pid_to_idx = {pid: i for i, pid in enumerate(self.ds.patient_ids)}\n",
        "        self.indices = [pid_to_idx[p] for p in pid_list if p in pid_to_idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.ds[self.indices[i]]\n",
        "\n",
        "\n",
        "# Load client-specific train/val patient IDs\n",
        "cdir = os.path.join(CLIENT_DIR, f\"client_{CLIENT_ID}\")\n",
        "with open(os.path.join(cdir, \"train_pids.json\"), \"r\") as f:\n",
        "    train_pids = json.load(f)\n",
        "with open(os.path.join(cdir, \"val_pids.json\"), \"r\") as f:\n",
        "    val_pids = json.load(f)\n",
        "\n",
        "# Build full dataset and then client subsets\n",
        "full_ds = ImageOnlyGliomaDataset(\n",
        "    METADATA_DF_PATH,\n",
        "    DATA_ROOT,\n",
        "    use_atlas=USE_ATLAS,\n",
        "    exclude_ids=[\"PatientID_0191\"],\n",
        ")\n",
        "train_dataset = SubsetByPIDs(full_ds, train_pids)\n",
        "val_dataset = SubsetByPIDs(full_ds, val_pids)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "    generator=torch.Generator().manual_seed(SEED),\n",
        "    worker_init_fn=worker_init_fn,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "    generator=torch.Generator().manual_seed(SEED),\n",
        "    worker_init_fn=worker_init_fn,\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"Loaded client_{CLIENT_ID}: train patients={len(train_dataset)}, val patients={len(val_dataset)}\"\n",
        ")\n",
        "\n",
        "# UNet model (from segmentation_models_pytorch)\n",
        "in_channels = 2 if USE_ATLAS else 1\n",
        "model = smp.Unet(\n",
        "    encoder_name=ENCODER_NAME,\n",
        "    encoder_weights=ENCODER_WEIGHTS,\n",
        "    in_channels=in_channels,\n",
        "    classes=1,\n",
        ").to(device)\n",
        "\n",
        "# Loss, optimizer, scheduler, and AMP scaler\n",
        "criterion = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
        "scheduler = optim.lr_scheduler.LinearLR(\n",
        "    optimizer, start_factor=1.0, end_factor=0.0, total_iters=EPOCHS\n",
        ")\n",
        "scaler = torch.amp.GradScaler(\"cuda\", enabled=(device.type == \"cuda\"))\n",
        "\n",
        "\n",
        "def calc_metrics(y_true, y_pred):\n",
        "    \"\"\"Compute Dice, IoU, and pixel accuracy.\"\"\"\n",
        "    y_true = y_true.astype(np.uint8).reshape(-1)\n",
        "    y_pred = y_pred.astype(np.uint8).reshape(-1)\n",
        "\n",
        "    inter = (y_true * y_pred).sum()\n",
        "    dice = (2.0 * inter) / (y_true.sum() + y_pred.sum() + 1e-8)\n",
        "\n",
        "    # IoU with sklearn, fallback if it fails\n",
        "    try:\n",
        "        iou = jaccard_score(y_true, y_pred, average=\"binary\")\n",
        "    except Exception:\n",
        "        union = y_true.sum() + y_pred.sum() - inter + 1e-8\n",
        "        iou = inter / union\n",
        "\n",
        "    acc = (y_true == y_pred).mean()\n",
        "    return float(dice), float(iou), float(acc)\n",
        "\n",
        "\n",
        "def plot_prediction(sample_imgs, save_path):\n",
        "    \"\"\"Plot MRI (+ optional regions), GT, and prediction for one batch.\"\"\"\n",
        "    if \"regions\" in sample_imgs:\n",
        "        fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
        "    else:\n",
        "        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # MRI\n",
        "    axs[0].imshow(sample_imgs[\"mri\"][0, 0], cmap=\"gray\")\n",
        "    axs[0].set_title(\"MRI\")\n",
        "    axs[0].axis(\"off\")\n",
        "\n",
        "    col = 1\n",
        "\n",
        "    # Regions (if available)\n",
        "    if \"regions\" in sample_imgs:\n",
        "        axs[col].imshow(sample_imgs[\"regions\"][0, 0], cmap=\"gray\")\n",
        "        axs[col].set_title(\"Regions\")\n",
        "        axs[col].axis(\"off\")\n",
        "        col += 1\n",
        "\n",
        "    # Ground truth\n",
        "    axs[col].imshow(sample_imgs[\"y_true\"][0, 0], cmap=\"gray\")\n",
        "    axs[col].set_title(\"Ground Truth\")\n",
        "    axs[col].axis(\"off\")\n",
        "    col += 1\n",
        "\n",
        "    # Prediction\n",
        "    axs[col].imshow(sample_imgs[\"y_pred\"][0, 0], cmap=\"gray\")\n",
        "    axs[col].set_title(\"Predicted\")\n",
        "    axs[col].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_metrics(history, save_path):\n",
        "    \"\"\"Plot training and validation curves for loss and metrics.\"\"\"\n",
        "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
        "\n",
        "    plt.figure(figsize=(16, 12))\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\")\n",
        "    plt.plot(epochs, history[\"val_loss\"], label=\"Val Loss\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Loss\")\n",
        "\n",
        "    # Dice\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(epochs, history[\"train_dice\"], label=\"Train Dice\")\n",
        "    plt.plot(epochs, history[\"val_dice\"], label=\"Val Dice\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Dice\")\n",
        "\n",
        "    # IoU\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(epochs, history[\"train_iou\"], label=\"Train IoU\")\n",
        "    plt.plot(epochs, history[\"val_iou\"], label=\"Val IoU\")\n",
        "    plt.legend()\n",
        "    plt.title(\"IoU\")\n",
        "\n",
        "    # Accuracy\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(epochs, history[\"train_acc\"], label=\"Train Acc\")\n",
        "    plt.plot(epochs, history[\"val_acc\"], label=\"Val Acc\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Accuracy\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion, scaler):\n",
        "    \"\"\"One training epoch over the dataloader.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = total_dice = total_iou = total_acc = 0.0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        x = batch[\"x\"].to(device)\n",
        "        y = batch[\"y\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Mixed precision forward pass\n",
        "        with torch.amp.autocast(\"cuda\", enabled=(device.type == \"cuda\")):\n",
        "            preds = model(x)\n",
        "            loss = criterion(preds, y)\n",
        "\n",
        "        # Backward with gradient scaling\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "        # Convert predictions to binary and compute metrics\n",
        "        preds_prob = (torch.sigmoid(preds).detach().cpu().numpy() > 0.5).astype(\n",
        "            np.uint8\n",
        "        )\n",
        "        y_np = (y.detach().cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        d, i, a = calc_metrics(y_np, preds_prob)\n",
        "\n",
        "        total_dice += d\n",
        "        total_iou += i\n",
        "        total_acc += a\n",
        "\n",
        "    n = len(dataloader)\n",
        "    return total_loss / n, total_dice / n, total_iou / n, total_acc / n\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_one_epoch(model, dataloader, criterion, keep_last_batch=True):\n",
        "    \"\"\"Evaluation loop over the validation dataloader.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = total_dice = total_iou = total_acc = 0.0\n",
        "    last_batch_imgs = None\n",
        "\n",
        "    for batch in dataloader:\n",
        "        x = batch[\"x\"].to(device)\n",
        "        y = batch[\"y\"].to(device)\n",
        "\n",
        "        preds = model(x)\n",
        "        loss = criterion(preds, y)\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "        preds_prob = (torch.sigmoid(preds).cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        y_np = (y.cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        d, i, a = calc_metrics(y_np, preds_prob)\n",
        "\n",
        "        total_dice += d\n",
        "        total_iou += i\n",
        "        total_acc += a\n",
        "\n",
        "        # Optionally keep last batch for visualization\n",
        "        if keep_last_batch:\n",
        "            ch0 = x[:, 0:1].cpu().numpy()\n",
        "            imgs = {\"mri\": ch0, \"y_true\": y.cpu().numpy(), \"y_pred\": preds_prob}\n",
        "            if x.shape[1] == 2:\n",
        "                ch1 = x[:, 1:2].cpu().numpy()\n",
        "                imgs[\"regions\"] = ch1\n",
        "            last_batch_imgs = imgs\n",
        "\n",
        "    n = len(dataloader)\n",
        "    return (\n",
        "        total_loss / n,\n",
        "        total_dice / n,\n",
        "        total_iou / n,\n",
        "        total_acc / n,\n",
        "        last_batch_imgs,\n",
        "    )\n",
        "\n",
        "\n",
        "def infer_and_visualize_best(\n",
        "    model,\n",
        "    val_dataset,\n",
        "    use_atlas: bool,\n",
        "    out_dir: str,\n",
        "    client_id: int,\n",
        "    best_ckpt_path: str,\n",
        "    k_samples: int = 3,\n",
        "    threshold: float = 0.5,\n",
        "):\n",
        "    \"\"\"Load best checkpoint, run inference on a few validation samples, and save visualizations.\"\"\"\n",
        "    import random\n",
        "\n",
        "    # Make sampling deterministic\n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    if os.path.isfile(best_ckpt_path):\n",
        "        model.load_state_dict(torch.load(best_ckpt_path, map_location=device))\n",
        "        model.eval()\n",
        "        print(f\"[Best Model] Loaded: {best_ckpt_path}\\n\")\n",
        "    else:\n",
        "        print(f\"[Best Model] Missing checkpoint: {best_ckpt_path}\\n\")\n",
        "        return\n",
        "\n",
        "    k = min(k_samples, len(val_dataset))\n",
        "    if k == 0:\n",
        "        print(\"[Best Model] Empty val dataset.\\n\")\n",
        "        return\n",
        "\n",
        "    # Deterministic random subset of validation indices\n",
        "    idxs = random.sample(range(len(val_dataset)), k)\n",
        "\n",
        "    def _predict_one(sample):\n",
        "        \"\"\"Run model on one sample and return inputs and binarized prediction.\"\"\"\n",
        "        mri = (\n",
        "            torch.tensor(sample[\"mri\"])\n",
        "            .unsqueeze(0)\n",
        "            .unsqueeze(0)\n",
        "            .float()\n",
        "            .to(device)\n",
        "        )\n",
        "\n",
        "        if use_atlas and (\"regions\" in sample):\n",
        "            regs = (\n",
        "                torch.tensor(sample[\"regions\"])\n",
        "                .unsqueeze(0)\n",
        "                .unsqueeze(0)\n",
        "                .float()\n",
        "                .to(device)\n",
        "            )\n",
        "            x = torch.cat([mri, regs], dim=1)\n",
        "        else:\n",
        "            x = mri\n",
        "\n",
        "        with torch.no_grad():\n",
        "            prob = torch.sigmoid(model(x)).cpu().numpy()\n",
        "            pred_bin = (prob > threshold).astype(np.uint8)\n",
        "\n",
        "        return x.cpu().numpy(), pred_bin\n",
        "\n",
        "    saved_paths = []\n",
        "\n",
        "    # Save individual sample figures\n",
        "    for i, idx in enumerate(idxs, 1):\n",
        "        sample = val_dataset[idx]\n",
        "        pid = sample.get(\"patient_id\", f\"val_{idx}\")\n",
        "        x_np, pred_bin = _predict_one(sample)\n",
        "\n",
        "        imgs = {\n",
        "            \"mri\": x_np[:, 0:1],\n",
        "            \"y_true\": np.expand_dims(\n",
        "                np.expand_dims(sample[\"tumor\"], 0), 0\n",
        "            ).astype(np.float32),\n",
        "            \"y_pred\": pred_bin.astype(np.float32),\n",
        "        }\n",
        "\n",
        "        if use_atlas and (\"regions\" in sample):\n",
        "            imgs[\"regions\"] = x_np[:, 1:2]\n",
        "\n",
        "        out_path = os.path.join(\n",
        "            out_dir, f\"best_val_sample_{i}_client{client_id}_{pid}.png\"\n",
        "        )\n",
        "        plot_prediction(imgs, out_path)\n",
        "        saved_paths.append(out_path)\n",
        "\n",
        "    print(\"[Best Model] Saved individual figures:\")\n",
        "    for p in saved_paths:\n",
        "        print(\" -\", p)\n",
        "    print()\n",
        "\n",
        "    # Save grid figure\n",
        "    cols = 4 if use_atlas else 3\n",
        "    fig, axs = plt.subplots(k, cols, figsize=(5 * cols, 4 * k))\n",
        "\n",
        "    if k == 1:\n",
        "        axs = np.expand_dims(axs, 0)\n",
        "\n",
        "    for row, idx in enumerate(idxs):\n",
        "        sample = val_dataset[idx]\n",
        "        x_np, pred_bin = _predict_one(sample)\n",
        "\n",
        "        mri = x_np[0, 0]\n",
        "        gt = sample[\"tumor\"]\n",
        "        col = 0\n",
        "\n",
        "        axs[row, col].imshow(mri, cmap=\"gray\")\n",
        "        axs[row, col].set_title(\"MRI\")\n",
        "        axs[row, col].axis(\"off\")\n",
        "        col += 1\n",
        "\n",
        "        if use_atlas and (\"regions\" in sample):\n",
        "            regs = x_np[0, 1]\n",
        "            axs[row, col].imshow(regs, cmap=\"gray\")\n",
        "            axs[row, col].set_title(\"Regions\")\n",
        "            axs[row, col].axis(\"off\")\n",
        "            col += 1\n",
        "\n",
        "        axs[row, col].imshow(gt, cmap=\"gray\")\n",
        "        axs[row, col].set_title(\"Ground Truth\")\n",
        "        axs[row, col].axis(\"off\")\n",
        "        col += 1\n",
        "\n",
        "        axs[row, col].imshow(pred_bin[0, 0], cmap=\"gray\")\n",
        "        axs[row, col].set_title(\"Predicted (τ=0.5)\")\n",
        "        axs[row, col].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    grid_path = os.path.join(\n",
        "        out_dir, f\"best_model_val_grid_{run_tag}_client{client_id}.png\"\n",
        "    )\n",
        "    plt.savefig(grid_path, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    print(f\"[Best Model] Saved grid -> {grid_path}\\n\")\n",
        "\n",
        "\n",
        "# History containers for training curves\n",
        "history = {\n",
        "    \"train_loss\": [],\n",
        "    \"train_dice\": [],\n",
        "    \"train_iou\": [],\n",
        "    \"train_acc\": [],\n",
        "    \"val_loss\": [],\n",
        "    \"val_dice\": [],\n",
        "    \"val_iou\": [],\n",
        "    \"val_acc\": [],\n",
        "}\n",
        "\n",
        "# Tracking best validation metrics\n",
        "best_val_iou = 0.0\n",
        "best_val_dice = -float(\"inf\")\n",
        "best_val_loss = float(\"inf\")\n",
        "best_path = os.path.join(\n",
        "    OUT_MODELS_DIR, f\"best_{run_tag}_client{CLIENT_ID}.pth\"\n",
        ")\n",
        "\n",
        "log_rows = []\n",
        "\n",
        "# Main training loop\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    # Train one epoch\n",
        "    trL, trD, trI, trA = train_one_epoch(\n",
        "        model, train_loader, optimizer, criterion, scaler\n",
        "    )\n",
        "    # Validate\n",
        "    vaL, vaD, vaI, vaA, _ = eval_one_epoch(\n",
        "        model, val_loader, criterion, keep_last_batch=True\n",
        "    )\n",
        "\n",
        "    # Update scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    # Save metrics for plots\n",
        "    history[\"train_loss\"].append(trL)\n",
        "    history[\"train_dice\"].append(trD)\n",
        "    history[\"train_iou\"].append(trI)\n",
        "    history[\"train_acc\"].append(trA)\n",
        "\n",
        "    history[\"val_loss\"].append(vaL)\n",
        "    history[\"val_dice\"].append(vaD)\n",
        "    history[\"val_iou\"].append(vaI)\n",
        "    history[\"val_acc\"].append(vaA)\n",
        "\n",
        "    print(\n",
        "        f\"[Epoch {epoch:03d}/{EPOCHS}] \"\n",
        "        f\"Train — Loss {trL:.4f} | Dice {trD:.4f} | IoU {trI:.4f} | Accuracy {trA:.4f} || \"\n",
        "        f\"Val — Loss {vaL:.4f} | Dice {vaD:.4f} | IoU {vaI:.4f} | Accuracy {vaA:.4f}\\n\"\n",
        "    )\n",
        "\n",
        "    # Save checkpoint if Dice improves and loss decreases\n",
        "    saved_ckpt = False\n",
        "    if (vaD > best_val_dice) and (vaL < best_val_loss):\n",
        "        best_val_dice = vaD\n",
        "        best_val_loss = vaL\n",
        "        torch.save(model.state_dict(), best_path)\n",
        "        saved_ckpt = True\n",
        "        print(\n",
        "            f\"Saved best model (Val Dice↑ {best_val_dice:.4f} & Val Loss↓ {best_val_loss:.4f}) -> {best_path}\\n\"\n",
        "        )\n",
        "\n",
        "    # Log row for CSV\n",
        "    log_rows.append(\n",
        "        {\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": trL,\n",
        "            \"train_dice\": trD,\n",
        "            \"train_iou\": trI,\n",
        "            \"train_acc\": trA,\n",
        "            \"val_loss\": vaL,\n",
        "            \"val_dice\": vaD,\n",
        "            \"val_iou\": vaI,\n",
        "            \"val_acc\": vaA,\n",
        "            \"saved_ckpt\": saved_ckpt,\n",
        "            \"marker\": \"X\" if saved_ckpt else \"\",\n",
        "        }\n",
        "    )\n",
        "\n",
        "# Save metrics as CSV\n",
        "metrics_csv = os.path.join(\n",
        "    OUT_GRAPHS_DIR, f\"metrics_{run_tag}_client{CLIENT_ID}.csv\"\n",
        ")\n",
        "pd.DataFrame(log_rows).to_csv(metrics_csv, index=False)\n",
        "print(f\"[Log] Wrote per-epoch metrics CSV -> {metrics_csv}\\n\")\n",
        "\n",
        "# Plot training curves (tagged)\n",
        "curves_path = os.path.join(\n",
        "    OUT_GRAPHS_DIR, f\"training_curves_{run_tag}_client{CLIENT_ID}.png\"\n",
        ")\n",
        "plot_metrics(history, curves_path)\n",
        "\n",
        "# Run inference with best model and visualize a few validation samples\n",
        "infer_and_visualize_best(\n",
        "    model=model,\n",
        "    val_dataset=val_dataset,\n",
        "    use_atlas=USE_ATLAS,\n",
        "    out_dir=OUT_GRAPHS_DIR,\n",
        "    client_id=CLIENT_ID,\n",
        "    best_ckpt_path=best_path,\n",
        "    k_samples=3,\n",
        "    threshold=0.5,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMKUalT3VBME"
      },
      "source": [
        "**<h2>UNet - \"mit_b3\"<h2>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LQFigtpPHpoI",
        "outputId": "683eabac-3424-4a2f-9c6b-0865d9d666de"
      },
      "outputs": [],
      "source": [
        "import os, json, pickle, random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "from sklearn.metrics import jaccard_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "# Paths and I/O config\n",
        "DATA_ROOT = \"/content/Preprocessed-Data\"\n",
        "METADATA_DF_PATH = \"cleaned_df.pkl\"\n",
        "OUT_BASE = \"AITDM\"\n",
        "CLIENT_DIR = os.path.join(OUT_BASE, \"/content/client\")\n",
        "\n",
        "# Experiment config\n",
        "USE_ATLAS = True\n",
        "CLIENT_ID = 2\n",
        "BATCH_SIZE = 8\n",
        "LR = 1e-3\n",
        "EPOCHS = 50\n",
        "NUM_WORKERS = 2\n",
        "SEED = 42\n",
        "\n",
        "# Model / encoder config\n",
        "ENCODER_NAME = \"mit_b3\"\n",
        "ENCODER_WEIGHTS = \"imagenet\"\n",
        "\n",
        "# Output directories\n",
        "OUT_MODELS_DIR = os.path.join(OUT_BASE, \"Models\", \"UNet_ImageOnly\")\n",
        "OUT_GRAPHS_DIR = os.path.join(OUT_BASE, \"Graphs\")\n",
        "os.makedirs(OUT_MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(OUT_GRAPHS_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "def sanitize(s: str) -> str:\n",
        "    return str(s).replace(\"/\", \"-\").replace(\" \", \"_\")\n",
        "\n",
        "\n",
        "encoder_tag = sanitize(ENCODER_NAME)\n",
        "weights_tag = (\n",
        "    sanitize(ENCODER_WEIGHTS) if ENCODER_WEIGHTS is not None else \"none\"\n",
        ")\n",
        "atlas_tag = \"atlas\" if USE_ATLAS else \"img\"\n",
        "\n",
        "run_tag = f\"unet_{encoder_tag}_{weights_tag}_{atlas_tag}\"\n",
        "\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.use_deterministic_algorithms(False)\n",
        "\n",
        "\n",
        "def worker_init_fn(worker_id):\n",
        "    worker_seed = SEED + worker_id\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "\n",
        "set_seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class ImageOnlyGliomaDataset(Dataset):\n",
        "    \"\"\"Dataset that loads MRI, tumor mask, and optional atlas regions per patient.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        metadata_df_path,\n",
        "        data_root,\n",
        "        use_atlas=True,\n",
        "        exclude_ids=None,\n",
        "        transform=None,\n",
        "    ):\n",
        "        import pickle, os, numpy as np, pandas as pd\n",
        "\n",
        "        # Load metadata dataframe\n",
        "        with open(metadata_df_path, \"rb\") as f:\n",
        "            df = pickle.load(f)\n",
        "\n",
        "        # Optionally exclude some patient IDs\n",
        "        if exclude_ids is None:\n",
        "            exclude_ids = [\"PatientID_0191\"]\n",
        "        self.df = df[~df[\"Patient_ID\"].isin(exclude_ids)].reset_index(drop=True)\n",
        "\n",
        "        self.data_root = data_root\n",
        "        self.use_atlas = use_atlas\n",
        "        self.transform = transform\n",
        "\n",
        "        # Collect patient IDs that have all required files\n",
        "        self.patient_ids = []\n",
        "        for pid in sorted(self.df[\"Patient_ID\"].tolist()):\n",
        "            mri_p = os.path.join(self.data_root, pid, f\"{pid}_mri.npy\")\n",
        "            tumor_p = os.path.join(self.data_root, pid, f\"{pid}_tumor.npy\")\n",
        "\n",
        "            if self.use_atlas:\n",
        "                reg_p = os.path.join(self.data_root, pid, f\"{pid}_regions.npy\")\n",
        "                if (\n",
        "                    os.path.isfile(mri_p)\n",
        "                    and os.path.isfile(tumor_p)\n",
        "                    and os.path.isfile(reg_p)\n",
        "                ):\n",
        "                    self.patient_ids.append(pid)\n",
        "            else:\n",
        "                if os.path.isfile(mri_p) and os.path.isfile(tumor_p):\n",
        "                    self.patient_ids.append(pid)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patient_ids)\n",
        "\n",
        "    @staticmethod\n",
        "    def _minmax(x):\n",
        "        \"\"\"Simple min-max normalization to [0, 1].\"\"\"\n",
        "        x = x.astype(np.float32)\n",
        "        mn, mx = np.min(x), np.max(x)\n",
        "        return (\n",
        "            (x - mn) / (mx - mn)\n",
        "            if mx > mn\n",
        "            else np.zeros_like(x, dtype=np.float32)\n",
        "        )\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Load and return one sample (dict) for a patient.\"\"\"\n",
        "        import os, numpy as np\n",
        "\n",
        "        pid = self.patient_ids[idx]\n",
        "        base = os.path.join(self.data_root, pid)\n",
        "\n",
        "        # Load MRI and tumor mask\n",
        "        mri = np.load(os.path.join(base, f\"{pid}_mri.npy\")).astype(np.float32)\n",
        "        tumor = np.load(os.path.join(base, f\"{pid}_tumor.npy\")).astype(\n",
        "            np.float32\n",
        "        )\n",
        "\n",
        "        # Normalize MRI and binarize tumor mask\n",
        "        mri = self._minmax(mri)\n",
        "        tumor = (tumor > 0.5).astype(np.float32)\n",
        "\n",
        "        sample = {\"patient_id\": pid, \"mri\": mri, \"tumor\": tumor}\n",
        "\n",
        "        # Optionally load regions/atlas\n",
        "        if self.use_atlas:\n",
        "            regions = np.load(os.path.join(base, f\"{pid}_regions.npy\")).astype(\n",
        "                np.float32\n",
        "            )\n",
        "            regions = self._minmax(regions)\n",
        "            sample[\"regions\"] = regions\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "def image_only_collate_fn(batch, use_atlas=True):\n",
        "    \"\"\"Custom collate: stack MRI (+ optional regions) and tumor into tensors.\"\"\"\n",
        "    mri = torch.stack([torch.tensor(it[\"mri\"]) for it in batch]).unsqueeze(1)\n",
        "    y = torch.stack([torch.tensor(it[\"tumor\"]) for it in batch]).unsqueeze(1)\n",
        "\n",
        "    if use_atlas:\n",
        "        regs = torch.stack(\n",
        "            [torch.tensor(it[\"regions\"]) for it in batch]\n",
        "        ).unsqueeze(1)\n",
        "        # Concatenate MRI and regions as channels\n",
        "        x = torch.cat([mri.float(), regs.float()], dim=1)\n",
        "    else:\n",
        "        x = mri.float()\n",
        "\n",
        "    return {\"x\": x, \"y\": y.float(), \"pid\": [it[\"patient_id\"] for it in batch]}\n",
        "\n",
        "\n",
        "class SubsetByPIDs(Dataset):\n",
        "    \"\"\"Wrap a dataset but keep only a subset of patient IDs.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, full_dataset: ImageOnlyGliomaDataset, pid_list: List[str]\n",
        "    ):\n",
        "        self.ds = full_dataset\n",
        "        # Map patient IDs to indices\n",
        "        pid_to_idx = {pid: i for i, pid in enumerate(self.ds.patient_ids)}\n",
        "        self.indices = [pid_to_idx[p] for p in pid_list if p in pid_to_idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.ds[self.indices[i]]\n",
        "\n",
        "\n",
        "# Load client-specific train/val patient IDs\n",
        "cdir = os.path.join(CLIENT_DIR, f\"client_{CLIENT_ID}\")\n",
        "with open(os.path.join(cdir, \"train_pids.json\"), \"r\") as f:\n",
        "    train_pids = json.load(f)\n",
        "with open(os.path.join(cdir, \"val_pids.json\"), \"r\") as f:\n",
        "    val_pids = json.load(f)\n",
        "\n",
        "# Build full dataset and then client subsets\n",
        "full_ds = ImageOnlyGliomaDataset(\n",
        "    METADATA_DF_PATH,\n",
        "    DATA_ROOT,\n",
        "    use_atlas=USE_ATLAS,\n",
        "    exclude_ids=[\"PatientID_0191\"],\n",
        ")\n",
        "train_dataset = SubsetByPIDs(full_ds, train_pids)\n",
        "val_dataset = SubsetByPIDs(full_ds, val_pids)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "    generator=torch.Generator().manual_seed(SEED),\n",
        "    worker_init_fn=worker_init_fn,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "    generator=torch.Generator().manual_seed(SEED),\n",
        "    worker_init_fn=worker_init_fn,\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"Loaded client_{CLIENT_ID}: train patients={len(train_dataset)}, val patients={len(val_dataset)}\"\n",
        ")\n",
        "\n",
        "# UNet model (from segmentation_models_pytorch)\n",
        "in_channels = 2 if USE_ATLAS else 1\n",
        "model = smp.Unet(\n",
        "    encoder_name=ENCODER_NAME,\n",
        "    encoder_weights=ENCODER_WEIGHTS,\n",
        "    in_channels=in_channels,\n",
        "    classes=1,\n",
        ").to(device)\n",
        "\n",
        "# Loss, optimizer, scheduler, and AMP scaler\n",
        "criterion = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
        "scheduler = optim.lr_scheduler.LinearLR(\n",
        "    optimizer, start_factor=1.0, end_factor=0.0, total_iters=EPOCHS\n",
        ")\n",
        "scaler = torch.amp.GradScaler(\"cuda\", enabled=(device.type == \"cuda\"))\n",
        "\n",
        "\n",
        "def calc_metrics(y_true, y_pred):\n",
        "    \"\"\"Compute Dice, IoU, and pixel accuracy.\"\"\"\n",
        "    y_true = y_true.astype(np.uint8).reshape(-1)\n",
        "    y_pred = y_pred.astype(np.uint8).reshape(-1)\n",
        "\n",
        "    inter = (y_true * y_pred).sum()\n",
        "    dice = (2.0 * inter) / (y_true.sum() + y_pred.sum() + 1e-8)\n",
        "\n",
        "    # IoU with sklearn, fallback if it fails\n",
        "    try:\n",
        "        iou = jaccard_score(y_true, y_pred, average=\"binary\")\n",
        "    except Exception:\n",
        "        union = y_true.sum() + y_pred.sum() - inter + 1e-8\n",
        "        iou = inter / union\n",
        "\n",
        "    acc = (y_true == y_pred).mean()\n",
        "    return float(dice), float(iou), float(acc)\n",
        "\n",
        "\n",
        "def plot_prediction(sample_imgs, save_path):\n",
        "    \"\"\"Plot MRI (+ optional regions), GT, and prediction for one batch.\"\"\"\n",
        "    if \"regions\" in sample_imgs:\n",
        "        fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
        "    else:\n",
        "        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # MRI\n",
        "    axs[0].imshow(sample_imgs[\"mri\"][0, 0], cmap=\"gray\")\n",
        "    axs[0].set_title(\"MRI\")\n",
        "    axs[0].axis(\"off\")\n",
        "\n",
        "    col = 1\n",
        "\n",
        "    # Regions (if available)\n",
        "    if \"regions\" in sample_imgs:\n",
        "        axs[col].imshow(sample_imgs[\"regions\"][0, 0], cmap=\"gray\")\n",
        "        axs[col].set_title(\"Regions\")\n",
        "        axs[col].axis(\"off\")\n",
        "        col += 1\n",
        "\n",
        "    # Ground truth\n",
        "    axs[col].imshow(sample_imgs[\"y_true\"][0, 0], cmap=\"gray\")\n",
        "    axs[col].set_title(\"Ground Truth\")\n",
        "    axs[col].axis(\"off\")\n",
        "    col += 1\n",
        "\n",
        "    # Prediction\n",
        "    axs[col].imshow(sample_imgs[\"y_pred\"][0, 0], cmap=\"gray\")\n",
        "    axs[col].set_title(\"Predicted\")\n",
        "    axs[col].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_metrics(history, save_path):\n",
        "    \"\"\"Plot training and validation curves for loss and metrics.\"\"\"\n",
        "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
        "\n",
        "    plt.figure(figsize=(16, 12))\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\")\n",
        "    plt.plot(epochs, history[\"val_loss\"], label=\"Val Loss\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Loss\")\n",
        "\n",
        "    # Dice\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(epochs, history[\"train_dice\"], label=\"Train Dice\")\n",
        "    plt.plot(epochs, history[\"val_dice\"], label=\"Val Dice\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Dice\")\n",
        "\n",
        "    # IoU\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(epochs, history[\"train_iou\"], label=\"Train IoU\")\n",
        "    plt.plot(epochs, history[\"val_iou\"], label=\"Val IoU\")\n",
        "    plt.legend()\n",
        "    plt.title(\"IoU\")\n",
        "\n",
        "    # Accuracy\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(epochs, history[\"train_acc\"], label=\"Train Acc\")\n",
        "    plt.plot(epochs, history[\"val_acc\"], label=\"Val Acc\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Accuracy\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion, scaler):\n",
        "    \"\"\"One training epoch over the dataloader.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = total_dice = total_iou = total_acc = 0.0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        x = batch[\"x\"].to(device)\n",
        "        y = batch[\"y\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Mixed precision forward pass\n",
        "        with torch.amp.autocast(\"cuda\", enabled=(device.type == \"cuda\")):\n",
        "            preds = model(x)\n",
        "            loss = criterion(preds, y)\n",
        "\n",
        "        # Backward with gradient scaling\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "        # Convert predictions to binary and compute metrics\n",
        "        preds_prob = (torch.sigmoid(preds).detach().cpu().numpy() > 0.5).astype(\n",
        "            np.uint8\n",
        "        )\n",
        "        y_np = (y.detach().cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        d, i, a = calc_metrics(y_np, preds_prob)\n",
        "\n",
        "        total_dice += d\n",
        "        total_iou += i\n",
        "        total_acc += a\n",
        "\n",
        "    n = len(dataloader)\n",
        "    return total_loss / n, total_dice / n, total_iou / n, total_acc / n\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_one_epoch(model, dataloader, criterion, keep_last_batch=True):\n",
        "    \"\"\"Evaluation loop over the validation dataloader.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = total_dice = total_iou = total_acc = 0.0\n",
        "    last_batch_imgs = None\n",
        "\n",
        "    for batch in dataloader:\n",
        "        x = batch[\"x\"].to(device)\n",
        "        y = batch[\"y\"].to(device)\n",
        "\n",
        "        preds = model(x)\n",
        "        loss = criterion(preds, y)\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "        preds_prob = (torch.sigmoid(preds).cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        y_np = (y.cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        d, i, a = calc_metrics(y_np, preds_prob)\n",
        "\n",
        "        total_dice += d\n",
        "        total_iou += i\n",
        "        total_acc += a\n",
        "\n",
        "        # Optionally keep last batch for visualization\n",
        "        if keep_last_batch:\n",
        "            ch0 = x[:, 0:1].cpu().numpy()\n",
        "            imgs = {\"mri\": ch0, \"y_true\": y.cpu().numpy(), \"y_pred\": preds_prob}\n",
        "            if x.shape[1] == 2:\n",
        "                ch1 = x[:, 1:2].cpu().numpy()\n",
        "                imgs[\"regions\"] = ch1\n",
        "            last_batch_imgs = imgs\n",
        "\n",
        "    n = len(dataloader)\n",
        "    return (\n",
        "        total_loss / n,\n",
        "        total_dice / n,\n",
        "        total_iou / n,\n",
        "        total_acc / n,\n",
        "        last_batch_imgs,\n",
        "    )\n",
        "\n",
        "\n",
        "def infer_and_visualize_best(\n",
        "    model,\n",
        "    val_dataset,\n",
        "    use_atlas: bool,\n",
        "    out_dir: str,\n",
        "    client_id: int,\n",
        "    best_ckpt_path: str,\n",
        "    k_samples: int = 3,\n",
        "    threshold: float = 0.5,\n",
        "):\n",
        "    \"\"\"Load best checkpoint, run inference on a few validation samples, and save visualizations.\"\"\"\n",
        "    import random\n",
        "\n",
        "    # Make sampling deterministic\n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    if os.path.isfile(best_ckpt_path):\n",
        "        model.load_state_dict(torch.load(best_ckpt_path, map_location=device))\n",
        "        model.eval()\n",
        "        print(f\"[Best Model] Loaded: {best_ckpt_path}\\n\")\n",
        "    else:\n",
        "        print(f\"[Best Model] Missing checkpoint: {best_ckpt_path}\\n\")\n",
        "        return\n",
        "\n",
        "    k = min(k_samples, len(val_dataset))\n",
        "    if k == 0:\n",
        "        print(\"[Best Model] Empty val dataset.\\n\")\n",
        "        return\n",
        "\n",
        "    # Deterministic random subset of validation indices\n",
        "    idxs = random.sample(range(len(val_dataset)), k)\n",
        "\n",
        "    def _predict_one(sample):\n",
        "        \"\"\"Run model on one sample and return inputs and binarized prediction.\"\"\"\n",
        "        mri = (\n",
        "            torch.tensor(sample[\"mri\"])\n",
        "            .unsqueeze(0)\n",
        "            .unsqueeze(0)\n",
        "            .float()\n",
        "            .to(device)\n",
        "        )\n",
        "\n",
        "        if use_atlas and (\"regions\" in sample):\n",
        "            regs = (\n",
        "                torch.tensor(sample[\"regions\"])\n",
        "                .unsqueeze(0)\n",
        "                .unsqueeze(0)\n",
        "                .float()\n",
        "                .to(device)\n",
        "            )\n",
        "            x = torch.cat([mri, regs], dim=1)\n",
        "        else:\n",
        "            x = mri\n",
        "\n",
        "        with torch.no_grad():\n",
        "            prob = torch.sigmoid(model(x)).cpu().numpy()\n",
        "            pred_bin = (prob > threshold).astype(np.uint8)\n",
        "\n",
        "        return x.cpu().numpy(), pred_bin\n",
        "\n",
        "    saved_paths = []\n",
        "\n",
        "    # Save individual sample figures\n",
        "    for i, idx in enumerate(idxs, 1):\n",
        "        sample = val_dataset[idx]\n",
        "        pid = sample.get(\"patient_id\", f\"val_{idx}\")\n",
        "        x_np, pred_bin = _predict_one(sample)\n",
        "\n",
        "        imgs = {\n",
        "            \"mri\": x_np[:, 0:1],\n",
        "            \"y_true\": np.expand_dims(\n",
        "                np.expand_dims(sample[\"tumor\"], 0), 0\n",
        "            ).astype(np.float32),\n",
        "            \"y_pred\": pred_bin.astype(np.float32),\n",
        "        }\n",
        "\n",
        "        if use_atlas and (\"regions\" in sample):\n",
        "            imgs[\"regions\"] = x_np[:, 1:2]\n",
        "\n",
        "        out_path = os.path.join(\n",
        "            out_dir, f\"best_val_sample_{i}_client{client_id}_{pid}.png\"\n",
        "        )\n",
        "        plot_prediction(imgs, out_path)\n",
        "        saved_paths.append(out_path)\n",
        "\n",
        "    print(\"[Best Model] Saved individual figures:\")\n",
        "    for p in saved_paths:\n",
        "        print(\" -\", p)\n",
        "    print()\n",
        "\n",
        "    # Save grid figure\n",
        "    cols = 4 if use_atlas else 3\n",
        "    fig, axs = plt.subplots(k, cols, figsize=(5 * cols, 4 * k))\n",
        "\n",
        "    if k == 1:\n",
        "        axs = np.expand_dims(axs, 0)\n",
        "\n",
        "    for row, idx in enumerate(idxs):\n",
        "        sample = val_dataset[idx]\n",
        "        x_np, pred_bin = _predict_one(sample)\n",
        "\n",
        "        mri = x_np[0, 0]\n",
        "        gt = sample[\"tumor\"]\n",
        "        col = 0\n",
        "\n",
        "        axs[row, col].imshow(mri, cmap=\"gray\")\n",
        "        axs[row, col].set_title(\"MRI\")\n",
        "        axs[row, col].axis(\"off\")\n",
        "        col += 1\n",
        "\n",
        "        if use_atlas and (\"regions\" in sample):\n",
        "            regs = x_np[0, 1]\n",
        "            axs[row, col].imshow(regs, cmap=\"gray\")\n",
        "            axs[row, col].set_title(\"Regions\")\n",
        "            axs[row, col].axis(\"off\")\n",
        "            col += 1\n",
        "\n",
        "        axs[row, col].imshow(gt, cmap=\"gray\")\n",
        "        axs[row, col].set_title(\"Ground Truth\")\n",
        "        axs[row, col].axis(\"off\")\n",
        "        col += 1\n",
        "\n",
        "        axs[row, col].imshow(pred_bin[0, 0], cmap=\"gray\")\n",
        "        axs[row, col].set_title(\"Predicted (τ=0.5)\")\n",
        "        axs[row, col].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    grid_path = os.path.join(\n",
        "        out_dir, f\"best_model_val_grid_{run_tag}_client{client_id}.png\"\n",
        "    )\n",
        "    plt.savefig(grid_path, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    print(f\"[Best Model] Saved grid -> {grid_path}\\n\")\n",
        "\n",
        "\n",
        "# History containers for training curves\n",
        "history = {\n",
        "    \"train_loss\": [],\n",
        "    \"train_dice\": [],\n",
        "    \"train_iou\": [],\n",
        "    \"train_acc\": [],\n",
        "    \"val_loss\": [],\n",
        "    \"val_dice\": [],\n",
        "    \"val_iou\": [],\n",
        "    \"val_acc\": [],\n",
        "}\n",
        "\n",
        "# Tracking best validation metrics\n",
        "best_val_iou = 0.0\n",
        "best_val_dice = -float(\"inf\")\n",
        "best_val_loss = float(\"inf\")\n",
        "best_path = os.path.join(\n",
        "    OUT_MODELS_DIR, f\"best_{run_tag}_client{CLIENT_ID}.pth\"\n",
        ")\n",
        "\n",
        "log_rows = []\n",
        "\n",
        "# Main training loop\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    # Train one epoch\n",
        "    trL, trD, trI, trA = train_one_epoch(\n",
        "        model, train_loader, optimizer, criterion, scaler\n",
        "    )\n",
        "    # Validate\n",
        "    vaL, vaD, vaI, vaA, _ = eval_one_epoch(\n",
        "        model, val_loader, criterion, keep_last_batch=True\n",
        "    )\n",
        "\n",
        "    # Update scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    # Save metrics for plots\n",
        "    history[\"train_loss\"].append(trL)\n",
        "    history[\"train_dice\"].append(trD)\n",
        "    history[\"train_iou\"].append(trI)\n",
        "    history[\"train_acc\"].append(trA)\n",
        "\n",
        "    history[\"val_loss\"].append(vaL)\n",
        "    history[\"val_dice\"].append(vaD)\n",
        "    history[\"val_iou\"].append(vaI)\n",
        "    history[\"val_acc\"].append(vaA)\n",
        "\n",
        "    print(\n",
        "        f\"[Epoch {epoch:03d}/{EPOCHS}] \"\n",
        "        f\"Train — Loss {trL:.4f} | Dice {trD:.4f} | IoU {trI:.4f} | Accuracy {trA:.4f} || \"\n",
        "        f\"Val — Loss {vaL:.4f} | Dice {vaD:.4f} | IoU {vaI:.4f} | Accuracy {vaA:.4f}\\n\"\n",
        "    )\n",
        "\n",
        "    # Save checkpoint if Dice improves and loss decreases\n",
        "    saved_ckpt = False\n",
        "    if (vaD > best_val_dice) and (vaL < best_val_loss):\n",
        "        best_val_dice = vaD\n",
        "        best_val_loss = vaL\n",
        "        torch.save(model.state_dict(), best_path)\n",
        "        saved_ckpt = True\n",
        "        print(\n",
        "            f\"Saved best model (Val Dice↑ {best_val_dice:.4f} & Val Loss↓ {best_val_loss:.4f}) -> {best_path}\\n\"\n",
        "        )\n",
        "\n",
        "    # Log row for CSV\n",
        "    log_rows.append(\n",
        "        {\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": trL,\n",
        "            \"train_dice\": trD,\n",
        "            \"train_iou\": trI,\n",
        "            \"train_acc\": trA,\n",
        "            \"val_loss\": vaL,\n",
        "            \"val_dice\": vaD,\n",
        "            \"val_iou\": vaI,\n",
        "            \"val_acc\": vaA,\n",
        "            \"saved_ckpt\": saved_ckpt,\n",
        "            \"marker\": \"X\" if saved_ckpt else \"\",\n",
        "        }\n",
        "    )\n",
        "\n",
        "# Save metrics as CSV\n",
        "metrics_csv = os.path.join(\n",
        "    OUT_GRAPHS_DIR, f\"metrics_{run_tag}_client{CLIENT_ID}.csv\"\n",
        ")\n",
        "pd.DataFrame(log_rows).to_csv(metrics_csv, index=False)\n",
        "print(f\"[Log] Wrote per-epoch metrics CSV -> {metrics_csv}\\n\")\n",
        "\n",
        "# Plot training curves (tagged)\n",
        "curves_path = os.path.join(\n",
        "    OUT_GRAPHS_DIR, f\"training_curves_{run_tag}_client{CLIENT_ID}.png\"\n",
        ")\n",
        "plot_metrics(history, curves_path)\n",
        "\n",
        "# Run inference with best model and visualize a few validation samples\n",
        "infer_and_visualize_best(\n",
        "    model=model,\n",
        "    val_dataset=val_dataset,\n",
        "    use_atlas=USE_ATLAS,\n",
        "    out_dir=OUT_GRAPHS_DIR,\n",
        "    client_id=CLIENT_ID,\n",
        "    best_ckpt_path=best_path,\n",
        "    k_samples=3,\n",
        "    threshold=0.5,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZjGJX8yVG0C"
      },
      "source": [
        "**<h2>DeepLabV3Plus - \"timm-mobilenetv3_small_100\"<h2>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VvaGPwgQPnIQ",
        "outputId": "a5c5120a-35bb-4e43-cd44-f5e885154b39"
      },
      "outputs": [],
      "source": [
        "import os, json, pickle, random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "from sklearn.metrics import jaccard_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "# Paths and I/O config\n",
        "DATA_ROOT = \"/content/Preprocessed-Data\"\n",
        "METADATA_DF_PATH = \"cleaned_df.pkl\"\n",
        "OUT_BASE = \"AITDM\"\n",
        "CLIENT_DIR = os.path.join(OUT_BASE, \"/content/client\")\n",
        "\n",
        "# Experiment config\n",
        "USE_ATLAS = True\n",
        "CLIENT_ID = 2\n",
        "BATCH_SIZE = 8\n",
        "LR = 1e-3\n",
        "EPOCHS = 50\n",
        "NUM_WORKERS = 2\n",
        "SEED = 42\n",
        "\n",
        "# Model / encoder config\n",
        "ENCODER_NAME = \"timm-mobilenetv3_small_100\"\n",
        "ENCODER_WEIGHTS = \"imagenet\"\n",
        "\n",
        "# Output directories\n",
        "OUT_MODELS_DIR = os.path.join(OUT_BASE, \"Models\", \"DeepLabV3Plus_ImageOnly\")\n",
        "OUT_GRAPHS_DIR = os.path.join(OUT_BASE, \"Graphs\")\n",
        "os.makedirs(OUT_MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(OUT_GRAPHS_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "def sanitize(s: str) -> str:\n",
        "    return str(s).replace(\"/\", \"-\").replace(\" \", \"_\")\n",
        "\n",
        "\n",
        "encoder_tag = sanitize(ENCODER_NAME)\n",
        "weights_tag = (\n",
        "    sanitize(ENCODER_WEIGHTS) if ENCODER_WEIGHTS is not None else \"none\"\n",
        ")\n",
        "atlas_tag = \"atlas\" if USE_ATLAS else \"img\"\n",
        "\n",
        "run_tag = f\"deeplabv3plus_{encoder_tag}_{weights_tag}_{atlas_tag}\"\n",
        "\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.use_deterministic_algorithms(False)\n",
        "\n",
        "\n",
        "def worker_init_fn(worker_id):\n",
        "    worker_seed = SEED + worker_id\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "\n",
        "set_seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class ImageOnlyGliomaDataset(Dataset):\n",
        "    \"\"\"Dataset that loads MRI, tumor mask, and optional atlas regions per patient.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        metadata_df_path,\n",
        "        data_root,\n",
        "        use_atlas=True,\n",
        "        exclude_ids=None,\n",
        "        transform=None,\n",
        "    ):\n",
        "        import pickle, os, numpy as np, pandas as pd\n",
        "\n",
        "        # Load metadata dataframe\n",
        "        with open(metadata_df_path, \"rb\") as f:\n",
        "            df = pickle.load(f)\n",
        "\n",
        "        # Optionally exclude some patient IDs\n",
        "        if exclude_ids is None:\n",
        "            exclude_ids = [\"PatientID_0191\"]\n",
        "        self.df = df[~df[\"Patient_ID\"].isin(exclude_ids)].reset_index(drop=True)\n",
        "\n",
        "        self.data_root = data_root\n",
        "        self.use_atlas = use_atlas\n",
        "        self.transform = transform\n",
        "\n",
        "        # Collect patient IDs that have all required files\n",
        "        self.patient_ids = []\n",
        "        for pid in sorted(self.df[\"Patient_ID\"].tolist()):\n",
        "            mri_p = os.path.join(self.data_root, pid, f\"{pid}_mri.npy\")\n",
        "            tumor_p = os.path.join(self.data_root, pid, f\"{pid}_tumor.npy\")\n",
        "\n",
        "            if self.use_atlas:\n",
        "                reg_p = os.path.join(self.data_root, pid, f\"{pid}_regions.npy\")\n",
        "                if (\n",
        "                    os.path.isfile(mri_p)\n",
        "                    and os.path.isfile(tumor_p)\n",
        "                    and os.path.isfile(reg_p)\n",
        "                ):\n",
        "                    self.patient_ids.append(pid)\n",
        "            else:\n",
        "                if os.path.isfile(mri_p) and os.path.isfile(tumor_p):\n",
        "                    self.patient_ids.append(pid)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patient_ids)\n",
        "\n",
        "    @staticmethod\n",
        "    def _minmax(x):\n",
        "        \"\"\"Simple min-max normalization to [0, 1].\"\"\"\n",
        "        x = x.astype(np.float32)\n",
        "        mn, mx = np.min(x), np.max(x)\n",
        "        return (\n",
        "            (x - mn) / (mx - mn)\n",
        "            if mx > mn\n",
        "            else np.zeros_like(x, dtype=np.float32)\n",
        "        )\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Load and return one sample (dict) for a patient.\"\"\"\n",
        "        import os, numpy as np\n",
        "\n",
        "        pid = self.patient_ids[idx]\n",
        "        base = os.path.join(self.data_root, pid)\n",
        "\n",
        "        # Load MRI and tumor mask\n",
        "        mri = np.load(os.path.join(base, f\"{pid}_mri.npy\")).astype(np.float32)\n",
        "        tumor = np.load(os.path.join(base, f\"{pid}_tumor.npy\")).astype(\n",
        "            np.float32\n",
        "        )\n",
        "\n",
        "        # Normalize MRI and binarize tumor mask\n",
        "        mri = self._minmax(mri)\n",
        "        tumor = (tumor > 0.5).astype(np.float32)\n",
        "\n",
        "        sample = {\"patient_id\": pid, \"mri\": mri, \"tumor\": tumor}\n",
        "\n",
        "        # Optionally load regions/atlas\n",
        "        if self.use_atlas:\n",
        "            regions = np.load(os.path.join(base, f\"{pid}_regions.npy\")).astype(\n",
        "                np.float32\n",
        "            )\n",
        "            regions = self._minmax(regions)\n",
        "            sample[\"regions\"] = regions\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "def image_only_collate_fn(batch, use_atlas=True):\n",
        "    \"\"\"Custom collate: stack MRI (+ optional regions) and tumor into tensors.\"\"\"\n",
        "    mri = torch.stack([torch.tensor(it[\"mri\"]) for it in batch]).unsqueeze(1)\n",
        "    y = torch.stack([torch.tensor(it[\"tumor\"]) for it in batch]).unsqueeze(1)\n",
        "\n",
        "    if use_atlas:\n",
        "        regs = torch.stack(\n",
        "            [torch.tensor(it[\"regions\"]) for it in batch]\n",
        "        ).unsqueeze(1)\n",
        "        # Concatenate MRI and regions as channels\n",
        "        x = torch.cat([mri.float(), regs.float()], dim=1)\n",
        "    else:\n",
        "        x = mri.float()\n",
        "\n",
        "    return {\"x\": x, \"y\": y.float(), \"pid\": [it[\"patient_id\"] for it in batch]}\n",
        "\n",
        "\n",
        "class SubsetByPIDs(Dataset):\n",
        "    \"\"\"Wrap a dataset but keep only a subset of patient IDs.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, full_dataset: ImageOnlyGliomaDataset, pid_list: List[str]\n",
        "    ):\n",
        "        self.ds = full_dataset\n",
        "        # Map patient IDs to indices\n",
        "        pid_to_idx = {pid: i for i, pid in enumerate(self.ds.patient_ids)}\n",
        "        self.indices = [pid_to_idx[p] for p in pid_list if p in pid_to_idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.ds[self.indices[i]]\n",
        "\n",
        "\n",
        "# Load client-specific train/val patient IDs\n",
        "cdir = os.path.join(CLIENT_DIR, f\"client_{CLIENT_ID}\")\n",
        "with open(os.path.join(cdir, \"train_pids.json\"), \"r\") as f:\n",
        "    train_pids = json.load(f)\n",
        "with open(os.path.join(cdir, \"val_pids.json\"), \"r\") as f:\n",
        "    val_pids = json.load(f)\n",
        "\n",
        "# Build full dataset and then client subsets\n",
        "full_ds = ImageOnlyGliomaDataset(\n",
        "    METADATA_DF_PATH,\n",
        "    DATA_ROOT,\n",
        "    use_atlas=USE_ATLAS,\n",
        "    exclude_ids=[\"PatientID_0191\"],\n",
        ")\n",
        "train_dataset = SubsetByPIDs(full_ds, train_pids)\n",
        "val_dataset = SubsetByPIDs(full_ds, val_pids)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "    generator=torch.Generator().manual_seed(SEED),\n",
        "    worker_init_fn=worker_init_fn,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "    generator=torch.Generator().manual_seed(SEED),\n",
        "    worker_init_fn=worker_init_fn,\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"Loaded client_{CLIENT_ID}: train patients={len(train_dataset)}, val patients={len(val_dataset)}\"\n",
        ")\n",
        "\n",
        "# UNet model (from segmentation_models_pytorch)\n",
        "in_channels = 2 if USE_ATLAS else 1\n",
        "model = smp.DeepLabV3Plus(\n",
        "    encoder_name=ENCODER_NAME,\n",
        "    encoder_weights=ENCODER_WEIGHTS,\n",
        "    in_channels=in_channels,\n",
        "    classes=1,\n",
        ").to(device)\n",
        "\n",
        "# Loss, optimizer, scheduler, and AMP scaler\n",
        "criterion = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
        "scheduler = optim.lr_scheduler.LinearLR(\n",
        "    optimizer, start_factor=1.0, end_factor=0.0, total_iters=EPOCHS\n",
        ")\n",
        "scaler = torch.amp.GradScaler(\"cuda\", enabled=(device.type == \"cuda\"))\n",
        "\n",
        "\n",
        "def calc_metrics(y_true, y_pred):\n",
        "    \"\"\"Compute Dice, IoU, and pixel accuracy.\"\"\"\n",
        "    y_true = y_true.astype(np.uint8).reshape(-1)\n",
        "    y_pred = y_pred.astype(np.uint8).reshape(-1)\n",
        "\n",
        "    inter = (y_true * y_pred).sum()\n",
        "    dice = (2.0 * inter) / (y_true.sum() + y_pred.sum() + 1e-8)\n",
        "\n",
        "    # IoU with sklearn, fallback if it fails\n",
        "    try:\n",
        "        iou = jaccard_score(y_true, y_pred, average=\"binary\")\n",
        "    except Exception:\n",
        "        union = y_true.sum() + y_pred.sum() - inter + 1e-8\n",
        "        iou = inter / union\n",
        "\n",
        "    acc = (y_true == y_pred).mean()\n",
        "    return float(dice), float(iou), float(acc)\n",
        "\n",
        "\n",
        "def plot_prediction(sample_imgs, save_path):\n",
        "    \"\"\"Plot MRI (+ optional regions), GT, and prediction for one batch.\"\"\"\n",
        "    if \"regions\" in sample_imgs:\n",
        "        fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
        "    else:\n",
        "        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # MRI\n",
        "    axs[0].imshow(sample_imgs[\"mri\"][0, 0], cmap=\"gray\")\n",
        "    axs[0].set_title(\"MRI\")\n",
        "    axs[0].axis(\"off\")\n",
        "\n",
        "    col = 1\n",
        "\n",
        "    # Regions (if available)\n",
        "    if \"regions\" in sample_imgs:\n",
        "        axs[col].imshow(sample_imgs[\"regions\"][0, 0], cmap=\"gray\")\n",
        "        axs[col].set_title(\"Regions\")\n",
        "        axs[col].axis(\"off\")\n",
        "        col += 1\n",
        "\n",
        "    # Ground truth\n",
        "    axs[col].imshow(sample_imgs[\"y_true\"][0, 0], cmap=\"gray\")\n",
        "    axs[col].set_title(\"Ground Truth\")\n",
        "    axs[col].axis(\"off\")\n",
        "    col += 1\n",
        "\n",
        "    # Prediction\n",
        "    axs[col].imshow(sample_imgs[\"y_pred\"][0, 0], cmap=\"gray\")\n",
        "    axs[col].set_title(\"Predicted\")\n",
        "    axs[col].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_metrics(history, save_path):\n",
        "    \"\"\"Plot training and validation curves for loss and metrics.\"\"\"\n",
        "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
        "\n",
        "    plt.figure(figsize=(16, 12))\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\")\n",
        "    plt.plot(epochs, history[\"val_loss\"], label=\"Val Loss\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Loss\")\n",
        "\n",
        "    # Dice\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(epochs, history[\"train_dice\"], label=\"Train Dice\")\n",
        "    plt.plot(epochs, history[\"val_dice\"], label=\"Val Dice\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Dice\")\n",
        "\n",
        "    # IoU\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(epochs, history[\"train_iou\"], label=\"Train IoU\")\n",
        "    plt.plot(epochs, history[\"val_iou\"], label=\"Val IoU\")\n",
        "    plt.legend()\n",
        "    plt.title(\"IoU\")\n",
        "\n",
        "    # Accuracy\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(epochs, history[\"train_acc\"], label=\"Train Acc\")\n",
        "    plt.plot(epochs, history[\"val_acc\"], label=\"Val Acc\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Accuracy\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion, scaler):\n",
        "    \"\"\"One training epoch over the dataloader.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = total_dice = total_iou = total_acc = 0.0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        x = batch[\"x\"].to(device)\n",
        "        y = batch[\"y\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Mixed precision forward pass\n",
        "        with torch.amp.autocast(\"cuda\", enabled=(device.type == \"cuda\")):\n",
        "            preds = model(x)\n",
        "            loss = criterion(preds, y)\n",
        "\n",
        "        # Backward with gradient scaling\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "        # Convert predictions to binary and compute metrics\n",
        "        preds_prob = (torch.sigmoid(preds).detach().cpu().numpy() > 0.5).astype(\n",
        "            np.uint8\n",
        "        )\n",
        "        y_np = (y.detach().cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        d, i, a = calc_metrics(y_np, preds_prob)\n",
        "\n",
        "        total_dice += d\n",
        "        total_iou += i\n",
        "        total_acc += a\n",
        "\n",
        "    n = len(dataloader)\n",
        "    return total_loss / n, total_dice / n, total_iou / n, total_acc / n\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_one_epoch(model, dataloader, criterion, keep_last_batch=True):\n",
        "    \"\"\"Evaluation loop over the validation dataloader.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = total_dice = total_iou = total_acc = 0.0\n",
        "    last_batch_imgs = None\n",
        "\n",
        "    for batch in dataloader:\n",
        "        x = batch[\"x\"].to(device)\n",
        "        y = batch[\"y\"].to(device)\n",
        "\n",
        "        preds = model(x)\n",
        "        loss = criterion(preds, y)\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "        preds_prob = (torch.sigmoid(preds).cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        y_np = (y.cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        d, i, a = calc_metrics(y_np, preds_prob)\n",
        "\n",
        "        total_dice += d\n",
        "        total_iou += i\n",
        "        total_acc += a\n",
        "\n",
        "        # Optionally keep last batch for visualization\n",
        "        if keep_last_batch:\n",
        "            ch0 = x[:, 0:1].cpu().numpy()\n",
        "            imgs = {\"mri\": ch0, \"y_true\": y.cpu().numpy(), \"y_pred\": preds_prob}\n",
        "            if x.shape[1] == 2:\n",
        "                ch1 = x[:, 1:2].cpu().numpy()\n",
        "                imgs[\"regions\"] = ch1\n",
        "            last_batch_imgs = imgs\n",
        "\n",
        "    n = len(dataloader)\n",
        "    return (\n",
        "        total_loss / n,\n",
        "        total_dice / n,\n",
        "        total_iou / n,\n",
        "        total_acc / n,\n",
        "        last_batch_imgs,\n",
        "    )\n",
        "\n",
        "\n",
        "def infer_and_visualize_best(\n",
        "    model,\n",
        "    val_dataset,\n",
        "    use_atlas: bool,\n",
        "    out_dir: str,\n",
        "    client_id: int,\n",
        "    best_ckpt_path: str,\n",
        "    k_samples: int = 3,\n",
        "    threshold: float = 0.5,\n",
        "):\n",
        "    \"\"\"Load best checkpoint, run inference on a few validation samples, and save visualizations.\"\"\"\n",
        "    import random\n",
        "\n",
        "    # Make sampling deterministic\n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    if os.path.isfile(best_ckpt_path):\n",
        "        model.load_state_dict(torch.load(best_ckpt_path, map_location=device))\n",
        "        model.eval()\n",
        "        print(f\"[Best Model] Loaded: {best_ckpt_path}\\n\")\n",
        "    else:\n",
        "        print(f\"[Best Model] Missing checkpoint: {best_ckpt_path}\\n\")\n",
        "        return\n",
        "\n",
        "    k = min(k_samples, len(val_dataset))\n",
        "    if k == 0:\n",
        "        print(\"[Best Model] Empty val dataset.\\n\")\n",
        "        return\n",
        "\n",
        "    # Deterministic random subset of validation indices\n",
        "    idxs = random.sample(range(len(val_dataset)), k)\n",
        "\n",
        "    def _predict_one(sample):\n",
        "        \"\"\"Run model on one sample and return inputs and binarized prediction.\"\"\"\n",
        "        mri = (\n",
        "            torch.tensor(sample[\"mri\"])\n",
        "            .unsqueeze(0)\n",
        "            .unsqueeze(0)\n",
        "            .float()\n",
        "            .to(device)\n",
        "        )\n",
        "\n",
        "        if use_atlas and (\"regions\" in sample):\n",
        "            regs = (\n",
        "                torch.tensor(sample[\"regions\"])\n",
        "                .unsqueeze(0)\n",
        "                .unsqueeze(0)\n",
        "                .float()\n",
        "                .to(device)\n",
        "            )\n",
        "            x = torch.cat([mri, regs], dim=1)\n",
        "        else:\n",
        "            x = mri\n",
        "\n",
        "        with torch.no_grad():\n",
        "            prob = torch.sigmoid(model(x)).cpu().numpy()\n",
        "            pred_bin = (prob > threshold).astype(np.uint8)\n",
        "\n",
        "        return x.cpu().numpy(), pred_bin\n",
        "\n",
        "    saved_paths = []\n",
        "\n",
        "    # Save individual sample figures\n",
        "    for i, idx in enumerate(idxs, 1):\n",
        "        sample = val_dataset[idx]\n",
        "        pid = sample.get(\"patient_id\", f\"val_{idx}\")\n",
        "        x_np, pred_bin = _predict_one(sample)\n",
        "\n",
        "        imgs = {\n",
        "            \"mri\": x_np[:, 0:1],\n",
        "            \"y_true\": np.expand_dims(\n",
        "                np.expand_dims(sample[\"tumor\"], 0), 0\n",
        "            ).astype(np.float32),\n",
        "            \"y_pred\": pred_bin.astype(np.float32),\n",
        "        }\n",
        "\n",
        "        if use_atlas and (\"regions\" in sample):\n",
        "            imgs[\"regions\"] = x_np[:, 1:2]\n",
        "\n",
        "        out_path = os.path.join(\n",
        "            out_dir, f\"best_val_sample_{i}_client{client_id}_{pid}.png\"\n",
        "        )\n",
        "        plot_prediction(imgs, out_path)\n",
        "        saved_paths.append(out_path)\n",
        "\n",
        "    print(\"[Best Model] Saved individual figures:\")\n",
        "    for p in saved_paths:\n",
        "        print(\" -\", p)\n",
        "    print()\n",
        "\n",
        "    # Save grid figure\n",
        "    cols = 4 if use_atlas else 3\n",
        "    fig, axs = plt.subplots(k, cols, figsize=(5 * cols, 4 * k))\n",
        "\n",
        "    if k == 1:\n",
        "        axs = np.expand_dims(axs, 0)\n",
        "\n",
        "    for row, idx in enumerate(idxs):\n",
        "        sample = val_dataset[idx]\n",
        "        x_np, pred_bin = _predict_one(sample)\n",
        "\n",
        "        mri = x_np[0, 0]\n",
        "        gt = sample[\"tumor\"]\n",
        "        col = 0\n",
        "\n",
        "        axs[row, col].imshow(mri, cmap=\"gray\")\n",
        "        axs[row, col].set_title(\"MRI\")\n",
        "        axs[row, col].axis(\"off\")\n",
        "        col += 1\n",
        "\n",
        "        if use_atlas and (\"regions\" in sample):\n",
        "            regs = x_np[0, 1]\n",
        "            axs[row, col].imshow(regs, cmap=\"gray\")\n",
        "            axs[row, col].set_title(\"Regions\")\n",
        "            axs[row, col].axis(\"off\")\n",
        "            col += 1\n",
        "\n",
        "        axs[row, col].imshow(gt, cmap=\"gray\")\n",
        "        axs[row, col].set_title(\"Ground Truth\")\n",
        "        axs[row, col].axis(\"off\")\n",
        "        col += 1\n",
        "\n",
        "        axs[row, col].imshow(pred_bin[0, 0], cmap=\"gray\")\n",
        "        axs[row, col].set_title(\"Predicted (τ=0.5)\")\n",
        "        axs[row, col].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    grid_path = os.path.join(\n",
        "        out_dir, f\"best_model_val_grid_{run_tag}_client{client_id}.png\"\n",
        "    )\n",
        "    plt.savefig(grid_path, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    print(f\"[Best Model] Saved grid -> {grid_path}\\n\")\n",
        "\n",
        "\n",
        "# History containers for training curves\n",
        "history = {\n",
        "    \"train_loss\": [],\n",
        "    \"train_dice\": [],\n",
        "    \"train_iou\": [],\n",
        "    \"train_acc\": [],\n",
        "    \"val_loss\": [],\n",
        "    \"val_dice\": [],\n",
        "    \"val_iou\": [],\n",
        "    \"val_acc\": [],\n",
        "}\n",
        "\n",
        "# Tracking best validation metrics\n",
        "best_val_iou = 0.0\n",
        "best_val_dice = -float(\"inf\")\n",
        "best_val_loss = float(\"inf\")\n",
        "best_path = os.path.join(\n",
        "    OUT_MODELS_DIR, f\"best_{run_tag}_client{CLIENT_ID}.pth\"\n",
        ")\n",
        "\n",
        "log_rows = []\n",
        "\n",
        "# Main training loop\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    # Train one epoch\n",
        "    trL, trD, trI, trA = train_one_epoch(\n",
        "        model, train_loader, optimizer, criterion, scaler\n",
        "    )\n",
        "    # Validate\n",
        "    vaL, vaD, vaI, vaA, _ = eval_one_epoch(\n",
        "        model, val_loader, criterion, keep_last_batch=True\n",
        "    )\n",
        "\n",
        "    # Update scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    # Save metrics for plots\n",
        "    history[\"train_loss\"].append(trL)\n",
        "    history[\"train_dice\"].append(trD)\n",
        "    history[\"train_iou\"].append(trI)\n",
        "    history[\"train_acc\"].append(trA)\n",
        "\n",
        "    history[\"val_loss\"].append(vaL)\n",
        "    history[\"val_dice\"].append(vaD)\n",
        "    history[\"val_iou\"].append(vaI)\n",
        "    history[\"val_acc\"].append(vaA)\n",
        "\n",
        "    print(\n",
        "        f\"[Epoch {epoch:03d}/{EPOCHS}] \"\n",
        "        f\"Train — Loss {trL:.4f} | Dice {trD:.4f} | IoU {trI:.4f} | Accuracy {trA:.4f} || \"\n",
        "        f\"Val — Loss {vaL:.4f} | Dice {vaD:.4f} | IoU {vaI:.4f} | Accuracy {vaA:.4f}\\n\"\n",
        "    )\n",
        "\n",
        "    # Save checkpoint if Dice improves and loss decreases\n",
        "    saved_ckpt = False\n",
        "    if (vaD > best_val_dice) and (vaL < best_val_loss):\n",
        "        best_val_dice = vaD\n",
        "        best_val_loss = vaL\n",
        "        torch.save(model.state_dict(), best_path)\n",
        "        saved_ckpt = True\n",
        "        print(\n",
        "            f\"Saved best model (Val Dice↑ {best_val_dice:.4f} & Val Loss↓ {best_val_loss:.4f}) -> {best_path}\\n\"\n",
        "        )\n",
        "\n",
        "    # Log row for CSV\n",
        "    log_rows.append(\n",
        "        {\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": trL,\n",
        "            \"train_dice\": trD,\n",
        "            \"train_iou\": trI,\n",
        "            \"train_acc\": trA,\n",
        "            \"val_loss\": vaL,\n",
        "            \"val_dice\": vaD,\n",
        "            \"val_iou\": vaI,\n",
        "            \"val_acc\": vaA,\n",
        "            \"saved_ckpt\": saved_ckpt,\n",
        "            \"marker\": \"X\" if saved_ckpt else \"\",\n",
        "        }\n",
        "    )\n",
        "\n",
        "# Save metrics as CSV\n",
        "metrics_csv = os.path.join(\n",
        "    OUT_GRAPHS_DIR, f\"metrics_{run_tag}_client{CLIENT_ID}.csv\"\n",
        ")\n",
        "pd.DataFrame(log_rows).to_csv(metrics_csv, index=False)\n",
        "print(f\"[Log] Wrote per-epoch metrics CSV -> {metrics_csv}\\n\")\n",
        "\n",
        "# Plot training curves (tagged)\n",
        "curves_path = os.path.join(\n",
        "    OUT_GRAPHS_DIR, f\"training_curves_{run_tag}_client{CLIENT_ID}.png\"\n",
        ")\n",
        "plot_metrics(history, curves_path)\n",
        "\n",
        "# Run inference with best model and visualize a few validation samples\n",
        "infer_and_visualize_best(\n",
        "    model=model,\n",
        "    val_dataset=val_dataset,\n",
        "    use_atlas=USE_ATLAS,\n",
        "    out_dir=OUT_GRAPHS_DIR,\n",
        "    client_id=CLIENT_ID,\n",
        "    best_ckpt_path=best_path,\n",
        "    k_samples=3,\n",
        "    threshold=0.5,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdquMfwqMDg4"
      },
      "source": [
        "**<h2>Ensemble<h2>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GmoBrDCRS57T",
        "outputId": "b1d700d0-92b8-4306-ae0c-1924bab539b8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import segmentation_models_pytorch as smp\n",
        "from sklearn.metrics import jaccard_score\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SEED = 42\n",
        "USE_ATLAS = True\n",
        "THRESHOLD = 0.5\n",
        "K_SAMPLES = 3\n",
        "\n",
        "OUT_BASE = \"AITDM\"\n",
        "OUT_GRAPHS_DIR = os.path.join(OUT_BASE, \"Graphs\")\n",
        "os.makedirs(OUT_GRAPHS_DIR, exist_ok=True)\n",
        "\n",
        "OUT_MODELS_UNET_DIR = os.path.join(OUT_BASE, \"Models\", \"UNet_ImageOnly\")\n",
        "CKPT_RESNET50 = os.path.join(\n",
        "    OUT_MODELS_UNET_DIR, \"best_unet_resnet50_imagenet_atlas_client0.pth\"\n",
        ")\n",
        "CKPT_MITB3 = os.path.join(\n",
        "    OUT_MODELS_UNET_DIR, \"best_unet_mit_b3_imagenet_atlas_client0.pth\"\n",
        ")\n",
        "\n",
        "OUT_MODELS_DLV3P_DIR = os.path.join(\n",
        "    OUT_BASE, \"Models\", \"DeepLabV3Plus_ImageOnly\"\n",
        ")\n",
        "CKPT_DLV3P = os.path.join(\n",
        "    OUT_MODELS_DLV3P_DIR,\n",
        "    \"best_deeplabv3plus_timm-mobilenetv3_small_100_imagenet_atlas_client0.pth\",\n",
        ")\n",
        "\n",
        "ENS_WEIGHTS = [2.5 / 10, 2.5 / 10, 5 / 10]\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "set_seed(SEED)\n",
        "\n",
        "\n",
        "def calc_metrics(y_true, y_pred):\n",
        "    y_true = y_true.astype(np.uint8).reshape(-1)\n",
        "    y_pred = y_pred.astype(np.uint8).reshape(-1)\n",
        "\n",
        "    inter = (y_true * y_pred).sum()\n",
        "    dice = (2.0 * inter) / (y_true.sum() + y_pred.sum() + 1e-8)\n",
        "\n",
        "    try:\n",
        "        iou = jaccard_score(y_true, y_pred, average=\"binary\")\n",
        "    except Exception:\n",
        "        union = y_true.sum() + y_pred.sum() - inter + 1e-8\n",
        "        iou = inter / union\n",
        "\n",
        "    acc = (y_true == y_pred).mean()\n",
        "    return float(dice), float(iou), float(acc)\n",
        "\n",
        "\n",
        "def plot_prediction(imgs, save_path, title=None):\n",
        "    has_regions = \"regions\" in imgs\n",
        "    cols = 4 if has_regions else 3\n",
        "    fig, axs = plt.subplots(1, cols, figsize=(5 * cols, 5))\n",
        "\n",
        "    axs[0].imshow(imgs[\"mri\"][0, 0], cmap=\"gray\")\n",
        "    axs[0].set_title(\"MRI\")\n",
        "    axs[0].axis(\"off\")\n",
        "\n",
        "    c = 1\n",
        "    if has_regions:\n",
        "        axs[c].imshow(imgs[\"regions\"][0, 0], cmap=\"gray\")\n",
        "        axs[c].set_title(\"Regions\")\n",
        "        axs[c].axis(\"off\")\n",
        "        c += 1\n",
        "\n",
        "    axs[c].imshow(imgs[\"y_true\"][0, 0], cmap=\"gray\")\n",
        "    axs[c].set_title(\"Ground Truth\")\n",
        "    axs[c].axis(\"off\")\n",
        "    c += 1\n",
        "\n",
        "    axs[c].imshow(imgs[\"y_pred\"][0, 0], cmap=\"gray\")\n",
        "    axs[c].set_title(f\"Ensemble (τ={THRESHOLD})\")\n",
        "    axs[c].axis(\"off\")\n",
        "\n",
        "    if title:\n",
        "        fig.suptitle(title, y=1.02)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def build_unet(\n",
        "    encoder_name, encoder_weights=\"imagenet\", in_channels=2, classes=1\n",
        "):\n",
        "    return smp.Unet(\n",
        "        encoder_name=encoder_name,\n",
        "        encoder_weights=encoder_weights,\n",
        "        in_channels=in_channels,\n",
        "        classes=classes,\n",
        "    )\n",
        "\n",
        "\n",
        "def build_deeplabv3p(\n",
        "    encoder_name, encoder_weights=\"imagenet\", in_channels=2, classes=1\n",
        "):\n",
        "    return smp.DeepLabV3Plus(\n",
        "        encoder_name=encoder_name,\n",
        "        encoder_weights=encoder_weights,\n",
        "        in_channels=in_channels,\n",
        "        classes=classes,\n",
        "    )\n",
        "\n",
        "\n",
        "def load_model_unet(encoder_name, ckpt_path, in_channels):\n",
        "    assert os.path.isfile(ckpt_path), f\"Missing checkpoint: {ckpt_path}\"\n",
        "    m = build_unet(\n",
        "        encoder_name, \"imagenet\", in_channels=in_channels, classes=1\n",
        "    ).to(DEVICE)\n",
        "    m.load_state_dict(torch.load(ckpt_path, map_location=DEVICE))\n",
        "    m.eval()\n",
        "    return m\n",
        "\n",
        "\n",
        "def load_model_dlv3p(encoder_name, ckpt_path, in_channels):\n",
        "    assert os.path.isfile(ckpt_path), f\"Missing checkpoint: {ckpt_path}\"\n",
        "    m = build_deeplabv3p(\n",
        "        encoder_name, \"imagenet\", in_channels=in_channels, classes=1\n",
        "    ).to(DEVICE)\n",
        "    m.load_state_dict(torch.load(ckpt_path, map_location=DEVICE))\n",
        "    m.eval()\n",
        "    return m\n",
        "\n",
        "\n",
        "in_channels = 2 if USE_ATLAS else 1\n",
        "model_r50 = load_model_unet(\"resnet50\", CKPT_RESNET50, in_channels=in_channels)\n",
        "model_mit3 = load_model_unet(\"mit_b3\", CKPT_MITB3, in_channels=in_channels)\n",
        "model_dlv3p = load_model_dlv3p(\n",
        "    \"timm-mobilenetv3_small_100\", CKPT_DLV3P, in_channels=in_channels\n",
        ")\n",
        "\n",
        "MODELS = [model_r50, model_mit3, model_dlv3p]\n",
        "criterion = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def ensemble_forward_logits_multi(x, models, weights, target_hw=None):\n",
        "    # Weighted sum of logits; resize to target_hw if needed\n",
        "    w = np.array(weights, dtype=np.float32)\n",
        "    w = w / (w.sum() + 1e-8)\n",
        "\n",
        "    logits_sum = None\n",
        "    for mi, wi in zip(models, w):\n",
        "        li = mi(x)\n",
        "        if target_hw is not None and li.shape[-2:] != target_hw:\n",
        "            li = F.interpolate(\n",
        "                li, size=target_hw, mode=\"bilinear\", align_corners=False\n",
        "            )\n",
        "        logits_sum = (\n",
        "            li * float(wi)\n",
        "            if logits_sum is None\n",
        "            else logits_sum + li * float(wi)\n",
        "        )\n",
        "\n",
        "    return logits_sum\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_one_epoch_ensemble(\n",
        "    dataloader,\n",
        "    models,\n",
        "    weights,\n",
        "    threshold=0.5,\n",
        "    criterion=None,\n",
        "    keep_last_batch=True,\n",
        "):\n",
        "    # Metrics are computed per-batch, then averaged across batches\n",
        "    total_loss = total_dice = total_iou = total_acc = 0.0\n",
        "    last_batch_imgs = None\n",
        "    n = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        x = batch[\"x\"].to(DEVICE)\n",
        "        y = batch[\"y\"].to(DEVICE)\n",
        "\n",
        "        logits = ensemble_forward_logits_multi(\n",
        "            x, models=models, weights=weights, target_hw=y.shape[-2:]\n",
        "        )\n",
        "\n",
        "        if criterion is not None:\n",
        "            loss = criterion(logits, y)\n",
        "            total_loss += float(loss.item())\n",
        "\n",
        "        preds_bin = (\n",
        "            torch.sigmoid(logits).detach().cpu().numpy() > threshold\n",
        "        ).astype(np.uint8)\n",
        "        y_np = (y.detach().cpu().numpy() > 0.5).astype(np.uint8)\n",
        "\n",
        "        d, i, a = calc_metrics(y_np, preds_bin)\n",
        "        total_dice += d\n",
        "        total_iou += i\n",
        "        total_acc += a\n",
        "        n += 1\n",
        "\n",
        "        if keep_last_batch:\n",
        "            ch0 = x[:, 0:1].detach().cpu().numpy()\n",
        "            imgs = {\n",
        "                \"mri\": ch0,\n",
        "                \"y_true\": y.detach().cpu().numpy(),\n",
        "                \"y_pred\": preds_bin,\n",
        "            }\n",
        "            if x.shape[1] == 2:\n",
        "                imgs[\"regions\"] = x[:, 1:2].detach().cpu().numpy()\n",
        "            last_batch_imgs = imgs\n",
        "\n",
        "    mean_loss = (total_loss / n) if (criterion is not None and n > 0) else None\n",
        "    mean_dice = total_dice / max(n, 1)\n",
        "    mean_iou = total_iou / max(n, 1)\n",
        "    mean_acc = total_acc / max(n, 1)\n",
        "\n",
        "    return mean_loss, mean_dice, mean_iou, mean_acc, last_batch_imgs\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def ensemble_infer_and_visualize(\n",
        "    val_dataset, out_dir, models, weights, k_samples=3, threshold=0.5\n",
        "):\n",
        "    # Visualization only (random K patients)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    k = min(k_samples, len(val_dataset))\n",
        "    if k == 0:\n",
        "        print(\"Empty val_dataset.\")\n",
        "        return\n",
        "\n",
        "    set_seed(SEED)\n",
        "    idxs = random.sample(range(len(val_dataset)), k)\n",
        "    per_sample = []\n",
        "\n",
        "    for i, idx in enumerate(idxs, 1):\n",
        "        sample = val_dataset[idx]\n",
        "        pid = sample.get(\"patient_id\", f\"val_{idx}\")\n",
        "\n",
        "        mri = torch.tensor(sample[\"mri\"]).unsqueeze(0).unsqueeze(0).float()\n",
        "        y = torch.tensor(sample[\"tumor\"]).unsqueeze(0).unsqueeze(0).float()\n",
        "\n",
        "        if USE_ATLAS and (\"regions\" in sample):\n",
        "            reg = (\n",
        "                torch.tensor(sample[\"regions\"])\n",
        "                .unsqueeze(0)\n",
        "                .unsqueeze(0)\n",
        "                .float()\n",
        "            )\n",
        "            x = torch.cat([mri, reg], dim=1)\n",
        "        else:\n",
        "            x = mri\n",
        "\n",
        "        x = x.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "        logits = ensemble_forward_logits_multi(\n",
        "            x, models=models, weights=weights, target_hw=y.shape[-2:]\n",
        "        )\n",
        "        pred_bin = (torch.sigmoid(logits).cpu().numpy() > threshold).astype(\n",
        "            np.uint8\n",
        "        )\n",
        "        y_np = (y.cpu().numpy() > 0.5).astype(np.uint8)\n",
        "\n",
        "        d, iou, acc = calc_metrics(y_np, pred_bin)\n",
        "\n",
        "        x_np = x.detach().cpu().numpy()\n",
        "        imgs = {\n",
        "            \"mri\": x_np[:, 0:1],\n",
        "            \"y_true\": y.detach().cpu().numpy(),\n",
        "            \"y_pred\": pred_bin.astype(np.float32),\n",
        "        }\n",
        "        if USE_ATLAS and x_np.shape[1] == 2:\n",
        "            imgs[\"regions\"] = x_np[:, 1:2]\n",
        "\n",
        "        out_path = os.path.join(out_dir, f\"ensemble3_val_sample_{i}_{pid}.png\")\n",
        "        plot_prediction(\n",
        "            imgs,\n",
        "            out_path,\n",
        "            title=f\"{pid} | Dice={d:.4f} IoU={iou:.4f} Acc={acc:.4f}\",\n",
        "        )\n",
        "        per_sample.append(\n",
        "            {\"pid\": pid, \"dice\": d, \"iou\": iou, \"acc\": acc, \"path\": out_path}\n",
        "        )\n",
        "\n",
        "    print(\"[Ensemble-3] Saved individual figures:\")\n",
        "    for r in per_sample:\n",
        "        print(\n",
        "            f\" - {r['pid']}: Dice={r['dice']:.4f} IoU={r['iou']:.4f} Acc={r['acc']:.4f} -> {r['path']}\"\n",
        "        )\n",
        "\n",
        "    cols = 4 if USE_ATLAS else 3\n",
        "    fig, axs = plt.subplots(k, cols, figsize=(5 * cols, 4 * k))\n",
        "    if k == 1:\n",
        "        axs = np.expand_dims(axs, 0)\n",
        "\n",
        "    for row, idx in enumerate(idxs):\n",
        "        sample = val_dataset[idx]\n",
        "\n",
        "        mri = torch.tensor(sample[\"mri\"]).unsqueeze(0).unsqueeze(0).float()\n",
        "        y = torch.tensor(sample[\"tumor\"]).unsqueeze(0).unsqueeze(0).float()\n",
        "\n",
        "        if USE_ATLAS and (\"regions\" in sample):\n",
        "            reg = (\n",
        "                torch.tensor(sample[\"regions\"])\n",
        "                .unsqueeze(0)\n",
        "                .unsqueeze(0)\n",
        "                .float()\n",
        "            )\n",
        "            x = torch.cat([mri, reg], dim=1)\n",
        "        else:\n",
        "            x = mri\n",
        "\n",
        "        x = x.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "        logits = ensemble_forward_logits_multi(\n",
        "            x, models=models, weights=weights, target_hw=y.shape[-2:]\n",
        "        )\n",
        "        pred_bin = (torch.sigmoid(logits).cpu().numpy() > threshold).astype(\n",
        "            np.uint8\n",
        "        )\n",
        "        y_np = (y.cpu().numpy() > 0.5).astype(np.uint8)\n",
        "\n",
        "        d, _, _ = calc_metrics(y_np, pred_bin)\n",
        "        x_np = x.detach().cpu().numpy()\n",
        "\n",
        "        c = 0\n",
        "        axs[row, c].imshow(x_np[0, 0], cmap=\"gray\")\n",
        "        axs[row, c].set_title(\"MRI\")\n",
        "        axs[row, c].axis(\"off\")\n",
        "        c += 1\n",
        "\n",
        "        if USE_ATLAS and x_np.shape[1] == 2:\n",
        "            axs[row, c].imshow(x_np[0, 1], cmap=\"gray\")\n",
        "            axs[row, c].set_title(\"Regions\")\n",
        "            axs[row, c].axis(\"off\")\n",
        "            c += 1\n",
        "\n",
        "        axs[row, c].imshow(y_np[0, 0], cmap=\"gray\")\n",
        "        axs[row, c].set_title(\"GT\")\n",
        "        axs[row, c].axis(\"off\")\n",
        "        c += 1\n",
        "\n",
        "        axs[row, c].imshow(pred_bin[0, 0], cmap=\"gray\")\n",
        "        axs[row, c].set_title(f\"Ens τ={threshold}\\nDice={d:.3f}\")\n",
        "        axs[row, c].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    wtag = \"-\".join(\n",
        "        [f\"{w:.2f}\" for w in (np.array(weights) / (np.sum(weights) + 1e-8))]\n",
        "    )\n",
        "    grid_path = os.path.join(\n",
        "        out_dir, f\"ensemble3_val_grid_tau{threshold}_w{wtag}.png\"\n",
        "    )\n",
        "    plt.savefig(grid_path, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    print(f\"[Ensemble-3] Saved grid -> {grid_path}\")\n",
        "\n",
        "\n",
        "# Assumes val_loader and val_dataset already exist in your notebook\n",
        "ens_loss, ens_d, ens_i, ens_a, last_imgs = eval_one_epoch_ensemble(\n",
        "    dataloader=val_loader,\n",
        "    models=MODELS,\n",
        "    weights=ENS_WEIGHTS,\n",
        "    threshold=THRESHOLD,\n",
        "    criterion=criterion,\n",
        "    keep_last_batch=True,\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"[Ensemble-3 Val] Loss {ens_loss:.4f} | Dice {ens_d:.4f} | IoU {ens_i:.4f} | Acc {ens_a:.4f}\"\n",
        ")\n",
        "\n",
        "if last_imgs is not None:\n",
        "    out_path = os.path.join(\n",
        "        OUT_GRAPHS_DIR, f\"ensemble3_last_batch_tau{THRESHOLD}.png\"\n",
        "    )\n",
        "    plot_prediction(last_imgs, out_path, title=\"Ensemble-3 - last val batch\")\n",
        "\n",
        "ensemble_infer_and_visualize(\n",
        "    val_dataset=val_dataset,\n",
        "    out_dir=OUT_GRAPHS_DIR,\n",
        "    models=MODELS,\n",
        "    weights=ENS_WEIGHTS,\n",
        "    k_samples=K_SAMPLES,\n",
        "    threshold=THRESHOLD,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Pf52xcpQyUg"
      },
      "source": [
        "# **M2 - FL with ensemble**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPgflPCuQ5vh",
        "outputId": "1b89eefd-7c83-4686-b365-ac48a0dee043"
      },
      "outputs": [],
      "source": [
        "%%writefile seg_data.py\n",
        "import os, pickle, numpy as np, torch\n",
        "from typing import List\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Global paths and configuration\n",
        "# DATA_ROOT = \"/content/Preprocessed-Data\"\n",
        "DATA_ROOT = \"Preprocessed-Data\"\n",
        "METADATA_DF_PATH = \"cleaned_df.pkl\"\n",
        "USE_ATLAS = True\n",
        "EXCLUDE_IDS = [\"PatientID_0191\"]\n",
        "\n",
        "\n",
        "# Dataset that loads MRI, tumor mask and optional atlas for each patient\n",
        "class ImageOnlyGliomaDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        metadata_df_path=METADATA_DF_PATH,\n",
        "        data_root=DATA_ROOT,\n",
        "        use_atlas=USE_ATLAS,\n",
        "        exclude_ids=None,\n",
        "        transform=None,\n",
        "    ):\n",
        "        with open(metadata_df_path, \"rb\") as f:\n",
        "            df = pickle.load(f)\n",
        "\n",
        "        # Optionally exclude specific patients\n",
        "        if exclude_ids is None:\n",
        "            exclude_ids = EXCLUDE_IDS\n",
        "\n",
        "        # Keep only non-excluded patient rows\n",
        "        self.df = df[~df[\"Patient_ID\"].isin(exclude_ids)].reset_index(drop=True)\n",
        "        self.data_root = data_root\n",
        "        self.use_atlas = use_atlas\n",
        "        self.transform = transform\n",
        "\n",
        "        # Collect patient IDs that have all required .npy files\n",
        "        self.patient_ids = []\n",
        "        for pid in sorted(self.df[\"Patient_ID\"].tolist()):\n",
        "            base = os.path.join(self.data_root, pid)\n",
        "            mri_p = os.path.join(base, f\"{pid}_mri.npy\")\n",
        "            tumor_p = os.path.join(base, f\"{pid}_tumor.npy\")\n",
        "            if self.use_atlas:\n",
        "                reg_p = os.path.join(base, f\"{pid}_regions.npy\")\n",
        "                ok = (\n",
        "                    os.path.isfile(mri_p)\n",
        "                    and os.path.isfile(tumor_p)\n",
        "                    and os.path.isfile(reg_p)\n",
        "                )\n",
        "            else:\n",
        "                ok = os.path.isfile(mri_p) and os.path.isfile(tumor_p)\n",
        "            if ok:\n",
        "                self.patient_ids.append(pid)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patient_ids)\n",
        "\n",
        "    # Simple min–max normalization\n",
        "    @staticmethod\n",
        "    def _minmax(x):\n",
        "        x = x.astype(np.float32)\n",
        "        mn, mx = np.min(x), np.max(x)\n",
        "        return (\n",
        "            (x - mn) / (mx - mn)\n",
        "            if mx > mn\n",
        "            else np.zeros_like(x, dtype=np.float32)\n",
        "        )\n",
        "\n",
        "    # Load a single sample: MRI, tumor mask, and optional atlas\n",
        "    def __getitem__(self, idx):\n",
        "        pid = self.patient_ids[idx]\n",
        "        base = os.path.join(self.data_root, pid)\n",
        "\n",
        "        mri = np.load(os.path.join(base, f\"{pid}_mri.npy\")).astype(np.float32)\n",
        "        tumor = np.load(os.path.join(base, f\"{pid}_tumor.npy\")).astype(\n",
        "            np.float32\n",
        "        )\n",
        "\n",
        "        mri = self._minmax(mri)\n",
        "        tumor = (tumor > 0.5).astype(np.float32)\n",
        "\n",
        "        sample = {\"patient_id\": pid, \"mri\": mri, \"tumor\": tumor}\n",
        "\n",
        "        if self.use_atlas:\n",
        "            regions = np.load(os.path.join(base, f\"{pid}_regions.npy\")).astype(\n",
        "                np.float32\n",
        "            )\n",
        "            regions = self._minmax(regions)\n",
        "            sample[\"regions\"] = regions\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "# Collate function to build batched tensors and patient ID list\n",
        "def image_only_collate_fn(batch, use_atlas=USE_ATLAS):\n",
        "    mri = torch.stack([torch.tensor(it[\"mri\"]) for it in batch]).unsqueeze(1)\n",
        "    y = torch.stack([torch.tensor(it[\"tumor\"]) for it in batch]).unsqueeze(1)\n",
        "\n",
        "    if use_atlas:\n",
        "        regs = torch.stack(\n",
        "            [torch.tensor(it[\"regions\"]) for it in batch]\n",
        "        ).unsqueeze(1)\n",
        "        x = torch.cat([mri.float(), regs.float()], dim=1)\n",
        "    else:\n",
        "        x = mri.float()\n",
        "\n",
        "    return {\"x\": x, \"y\": y.float(), \"pid\": [it[\"patient_id\"] for it in batch]}\n",
        "\n",
        "\n",
        "# Dataset wrapper that restricts to a subset of patient IDs\n",
        "class SubsetByPIDs(Dataset):\n",
        "    def __init__(\n",
        "        self, full_dataset: ImageOnlyGliomaDataset, pid_list: List[str]\n",
        "    ):\n",
        "        self.ds = full_dataset\n",
        "        pid_to_idx = {pid: i for i, pid in enumerate(self.ds.patient_ids)}\n",
        "        self.indices = [pid_to_idx[p] for p in pid_list if p in pid_to_idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.ds[self.indices[i]]\n",
        "\n",
        "\n",
        "# Compute Dice, IoU and accuracy for binary masks\n",
        "def calc_metrics(y_true, y_pred):\n",
        "    y_true = y_true.astype(np.uint8).reshape(-1)\n",
        "    y_pred = y_pred.astype(np.uint8).reshape(-1)\n",
        "\n",
        "    inter = (y_true & y_pred).sum()\n",
        "    dice = (2.0 * inter) / (y_true.sum() + y_pred.sum() + 1e-8)\n",
        "    union = y_true.sum() + y_pred.sum() - inter + 1e-8\n",
        "    iou = inter / union\n",
        "    acc = (y_true == y_pred).mean()\n",
        "\n",
        "    return float(dice), float(iou), float(acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3xGGSblAMp7",
        "outputId": "872458b0-d7f0-417b-c5d3-8f9b1d7c6086"
      },
      "outputs": [],
      "source": [
        "%%writefile fl_client.py\n",
        "import os\n",
        "\n",
        "os.environ.setdefault(\"CUBLAS_WORKSPACE_CONFIG\", \":4096:8\")\n",
        "\n",
        "import time\n",
        "import argparse\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import flwr as fl\n",
        "import copy\n",
        "import segmentation_models_pytorch as smp\n",
        "import random\n",
        "\n",
        "try:\n",
        "    import psutil\n",
        "except Exception:\n",
        "    psutil = None\n",
        "\n",
        "from seg_data import (\n",
        "    ImageOnlyGliomaDataset,\n",
        "    SubsetByPIDs,\n",
        "    image_only_collate_fn,\n",
        "    calc_metrics,\n",
        "    DATA_ROOT,\n",
        "    METADATA_DF_PATH,\n",
        "    USE_ATLAS,\n",
        ")\n",
        "\n",
        "CLIENT_DIR = \"client\"\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "NUM_WORKERS = 2\n",
        "SEED = 42\n",
        "\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "DEFAULT_MODEL_NAME = \"unet\"\n",
        "DEFAULT_ENCODER_NAME = \"timm-mobilenetv3_small_100\"\n",
        "DEFAULT_ENCODER_WEIGHTS = \"imagenet\"\n",
        "\n",
        "\n",
        "def model_num_params(model: torch.nn.Module) -> int:\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "\n",
        "def model_size_bytes(model: torch.nn.Module) -> int:\n",
        "    total = 0\n",
        "    for p in model.parameters():\n",
        "        total += p.numel() * p.element_size()\n",
        "    for b in model.buffers():\n",
        "        total += b.numel() * b.element_size()\n",
        "    return int(total)\n",
        "\n",
        "\n",
        "def get_rss_bytes() -> int:\n",
        "    if psutil is None:\n",
        "        return -1\n",
        "    return int(psutil.Process(os.getpid()).memory_info().rss)\n",
        "\n",
        "\n",
        "def _run_name(model_name: str, encoder_name: str) -> str:\n",
        "    return f\"{model_name}__{encoder_name}\".replace(\"/\", \"-\")\n",
        "\n",
        "\n",
        "def seed_everything(seed: int, deterministic: bool = True) -> None:\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    try:\n",
        "        torch.set_float32_matmul_precision(\"high\")\n",
        "    except Exception:\n",
        "        pass\n",
        "    if deterministic:\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.use_deterministic_algorithms(True)\n",
        "\n",
        "\n",
        "def seed_worker(worker_id: int) -> None:\n",
        "    worker_seed = (torch.initial_seed() + worker_id) % (2**32)\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "\n",
        "def get_model(\n",
        "    model_name=DEFAULT_MODEL_NAME,\n",
        "    encoder_name=DEFAULT_ENCODER_NAME,\n",
        "    encoder_weights=DEFAULT_ENCODER_WEIGHTS,\n",
        "):\n",
        "    in_ch = 2 if USE_ATLAS else 1\n",
        "    mn = model_name.lower()\n",
        "\n",
        "    if mn == \"unet\":\n",
        "        model = smp.Unet(\n",
        "            encoder_name=encoder_name,\n",
        "            encoder_weights=encoder_weights,\n",
        "            in_channels=in_ch,\n",
        "            classes=1,\n",
        "        )\n",
        "    elif mn in [\"deeplabv3plus\", \"deeplabv3+\", \"dlv3p\"]:\n",
        "        model = smp.DeepLabV3Plus(\n",
        "            encoder_name=encoder_name,\n",
        "            encoder_weights=encoder_weights,\n",
        "            in_channels=in_ch,\n",
        "            classes=1,\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            f\"Unknown model_name={model_name}. Use 'unet' or 'deeplabv3plus'.\"\n",
        "        )\n",
        "\n",
        "    return model.to(DEVICE)\n",
        "\n",
        "\n",
        "def get_loaders(cid: int, base_seed: int):\n",
        "    full = ImageOnlyGliomaDataset(\n",
        "        METADATA_DF_PATH,\n",
        "        DATA_ROOT,\n",
        "        use_atlas=USE_ATLAS,\n",
        "        exclude_ids=[\"PatientID_0191\"],\n",
        "    )\n",
        "\n",
        "    with open(\n",
        "        os.path.join(CLIENT_DIR, f\"client_{cid}\", \"train_pids.json\")\n",
        "    ) as f:\n",
        "        tr_p = json.load(f)\n",
        "    with open(os.path.join(CLIENT_DIR, f\"client_{cid}\", \"val_pids.json\")) as f:\n",
        "        va_p = json.load(f)\n",
        "\n",
        "    ds_tr = SubsetByPIDs(full, tr_p)\n",
        "    ds_va = SubsetByPIDs(full, va_p)\n",
        "\n",
        "    g_tr = torch.Generator().manual_seed(base_seed + 12345)\n",
        "    g_va = torch.Generator().manual_seed(base_seed + 67890)\n",
        "\n",
        "    ld_tr = DataLoader(\n",
        "        ds_tr,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "        collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=g_tr,\n",
        "        persistent_workers=(NUM_WORKERS > 0),\n",
        "    )\n",
        "\n",
        "    ld_va = DataLoader(\n",
        "        ds_va,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "        collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=g_va,\n",
        "        persistent_workers=(NUM_WORKERS > 0),\n",
        "    )\n",
        "\n",
        "    return ld_tr, ld_va, len(ds_tr), len(ds_va)\n",
        "\n",
        "\n",
        "def get_parameters(model):\n",
        "    return [p.detach().cpu().numpy() for _, p in model.state_dict().items()]\n",
        "\n",
        "\n",
        "def set_parameters(model, params):\n",
        "    sd = model.state_dict()\n",
        "    for k, v in zip(sd.keys(), params):\n",
        "        sd[k] = torch.tensor(v)\n",
        "    model.load_state_dict(sd, strict=True)\n",
        "\n",
        "\n",
        "bce = nn.BCEWithLogitsLoss()\n",
        "dice_loss = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
        "\n",
        "\n",
        "def criterion(pred, y):\n",
        "    return 0.5 * bce(pred, y) + 0.5 * dice_loss(pred, y)\n",
        "\n",
        "\n",
        "def maybe_save_best(run_dir, cid, val_loss, val_dice, best_epoch, rnd, model):\n",
        "    ckpt_dir = os.path.join(run_dir, \"checkpoints\")\n",
        "    os.makedirs(ckpt_dir, exist_ok=True)\n",
        "\n",
        "    best_json = os.path.join(ckpt_dir, f\"client_{cid}_best.json\")\n",
        "    best_pt = os.path.join(ckpt_dir, f\"client_{cid}_best.pt\")\n",
        "\n",
        "    prev = {\"val_loss\": float(\"inf\"), \"val_dice\": -1.0}\n",
        "    if os.path.isfile(best_json):\n",
        "        try:\n",
        "            with open(best_json, \"r\") as f:\n",
        "                prev = json.load(f)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    improved = (val_loss < prev.get(\"val_loss\", float(\"inf\"))) and (\n",
        "        val_dice > prev.get(\"val_dice\", -1.0)\n",
        "    )\n",
        "    if improved:\n",
        "        torch.save(model.state_dict(), best_pt)\n",
        "        with open(best_json, \"w\") as f:\n",
        "            json.dump(\n",
        "                {\n",
        "                    \"round\": int(rnd),\n",
        "                    \"epoch\": int(best_epoch),\n",
        "                    \"val_loss\": float(val_loss),\n",
        "                    \"val_dice\": float(val_dice),\n",
        "                },\n",
        "                f,\n",
        "            )\n",
        "\n",
        "\n",
        "class SegClient(fl.client.NumPyClient):\n",
        "    def __init__(\n",
        "        self,\n",
        "        cid: int,\n",
        "        model_name=DEFAULT_MODEL_NAME,\n",
        "        encoder_name=DEFAULT_ENCODER_NAME,\n",
        "        encoder_weights=DEFAULT_ENCODER_WEIGHTS,\n",
        "    ):\n",
        "        self.cid = int(cid)\n",
        "        self.model_name = model_name\n",
        "        self.encoder_name = encoder_name\n",
        "        self.encoder_weights = encoder_weights\n",
        "\n",
        "        self.base_seed = SEED + self.cid\n",
        "        seed_everything(self.base_seed, deterministic=True)\n",
        "\n",
        "        self.run_name = _run_name(model_name, encoder_name)\n",
        "        self.run_dir = os.path.join(\"AITDM\", self.run_name)\n",
        "\n",
        "        self.model = get_model(model_name, encoder_name, encoder_weights)\n",
        "        self.n_params = model_num_params(self.model)\n",
        "        self.model_bytes = model_size_bytes(self.model)\n",
        "\n",
        "        self.train_loader, self.val_loader, self.ntr, self.nva = get_loaders(\n",
        "            self.cid, self.base_seed\n",
        "        )\n",
        "\n",
        "    def get_parameters(self, config):\n",
        "        return get_parameters(self.model)\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        set_parameters(self.model, parameters)\n",
        "\n",
        "        epochs = int(config.get(\"local_epochs\", 1))\n",
        "        lr = float(config.get(\"lr\", 1e-3))\n",
        "        rnd = int(config.get(\"round\", 0))\n",
        "\n",
        "        t_fit0 = time.perf_counter()\n",
        "        rss0 = get_rss_bytes()\n",
        "\n",
        "        if DEVICE.type == \"cuda\":\n",
        "            torch.cuda.reset_peak_memory_stats()\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        opt = optim.AdamW(self.model.parameters(), lr=lr)\n",
        "        scaler = torch.amp.GradScaler(\"cuda\", enabled=(DEVICE.type == \"cuda\"))\n",
        "\n",
        "        best_state = None\n",
        "        best_val_loss = float(\"inf\")\n",
        "        best_val_dice = -1.0\n",
        "        best_epoch_idx = -1\n",
        "        epoch_logs = []\n",
        "\n",
        "        for epoch_idx in range(1, epochs + 1):\n",
        "            self.model.train()\n",
        "            tot_tr_loss = tot_tr_d = tot_tr_i = tot_tr_a = 0.0\n",
        "            nb_tr = 0\n",
        "\n",
        "            for batch in self.train_loader:\n",
        "                x = batch[\"x\"].to(DEVICE)\n",
        "                y = batch[\"y\"].to(DEVICE)\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "\n",
        "                with torch.amp.autocast(\n",
        "                    \"cuda\", enabled=(DEVICE.type == \"cuda\")\n",
        "                ):\n",
        "                    pred = self.model(x)\n",
        "                    loss = criterion(pred, y)\n",
        "\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(opt)\n",
        "                scaler.update()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    y_hat = (\n",
        "                        torch.sigmoid(pred).detach().cpu().numpy() > 0.5\n",
        "                    ).astype(np.uint8)\n",
        "                    y_np = (y.detach().cpu().numpy() > 0.5).astype(np.uint8)\n",
        "                    d, i, a = calc_metrics(y_np, y_hat)\n",
        "\n",
        "                tot_tr_loss += float(loss.item())\n",
        "                tot_tr_d += d\n",
        "                tot_tr_i += i\n",
        "                tot_tr_a += a\n",
        "                nb_tr += 1\n",
        "\n",
        "            nb_tr = max(nb_tr, 1)\n",
        "            epoch_tr_loss = tot_tr_loss / nb_tr\n",
        "            epoch_tr_dice = tot_tr_d / nb_tr\n",
        "            epoch_tr_iou = tot_tr_i / nb_tr\n",
        "            epoch_tr_acc = tot_tr_a / nb_tr\n",
        "\n",
        "            self.model.eval()\n",
        "            tot_val_loss = tot_val_d = tot_val_i = tot_val_a = 0.0\n",
        "            nb_val = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in self.val_loader:\n",
        "                    x = batch[\"x\"].to(DEVICE)\n",
        "                    y = batch[\"y\"].to(DEVICE)\n",
        "                    pred = self.model(x)\n",
        "\n",
        "                    v_loss = float(criterion(pred, y).item())\n",
        "                    y_hat = (torch.sigmoid(pred).cpu().numpy() > 0.5).astype(\n",
        "                        np.uint8\n",
        "                    )\n",
        "                    y_np = (y.cpu().numpy() > 0.5).astype(np.uint8)\n",
        "                    d, i, a = calc_metrics(y_np, y_hat)\n",
        "\n",
        "                    tot_val_loss += v_loss\n",
        "                    tot_val_d += d\n",
        "                    tot_val_i += i\n",
        "                    tot_val_a += a\n",
        "                    nb_val += 1\n",
        "\n",
        "            nb_val = max(nb_val, 1)\n",
        "            epoch_val_loss = tot_val_loss / nb_val\n",
        "            epoch_val_dice = tot_val_d / nb_val\n",
        "            epoch_val_iou = tot_val_i / nb_val\n",
        "            epoch_val_acc = tot_val_a / nb_val\n",
        "\n",
        "            epoch_logs.append(\n",
        "                {\n",
        "                    \"epoch\": int(epoch_idx),\n",
        "                    \"train_loss\": float(epoch_tr_loss),\n",
        "                    \"train_dice\": float(epoch_tr_dice),\n",
        "                    \"train_iou\": float(epoch_tr_iou),\n",
        "                    \"train_acc\": float(epoch_tr_acc),\n",
        "                    \"val_loss\": float(epoch_val_loss),\n",
        "                    \"val_dice\": float(epoch_val_dice),\n",
        "                    \"val_iou\": float(epoch_val_iou),\n",
        "                    \"val_acc\": float(epoch_val_acc),\n",
        "                }\n",
        "            )\n",
        "\n",
        "            if (epoch_val_loss < best_val_loss) and (\n",
        "                epoch_val_dice > best_val_dice\n",
        "            ):\n",
        "                best_val_loss = epoch_val_loss\n",
        "                best_val_dice = epoch_val_dice\n",
        "                best_state = copy.deepcopy(self.model.state_dict())\n",
        "                best_epoch_idx = epoch_idx\n",
        "\n",
        "        if best_state is not None:\n",
        "            self.model.load_state_dict(best_state)\n",
        "\n",
        "        for ep in epoch_logs:\n",
        "            ep[\"best_epoch\"] = ep[\"epoch\"] == best_epoch_idx\n",
        "\n",
        "        if DEVICE.type == \"cuda\":\n",
        "            torch.cuda.synchronize()\n",
        "            gpu_max_alloc = int(torch.cuda.max_memory_allocated())\n",
        "            gpu_max_reserved = int(torch.cuda.max_memory_reserved())\n",
        "        else:\n",
        "            gpu_max_alloc = -1\n",
        "            gpu_max_reserved = -1\n",
        "\n",
        "        t_fit = time.perf_counter() - t_fit0\n",
        "        rss1 = get_rss_bytes()\n",
        "        rss_delta = (rss1 - rss0) if (rss0 >= 0 and rss1 >= 0) else -1\n",
        "\n",
        "        train_metrics = {\n",
        "            \"cid\": int(self.cid),\n",
        "            \"run_name\": self.run_name,\n",
        "            \"model_name\": self.model_name,\n",
        "            \"encoder_name\": self.encoder_name,\n",
        "            \"round\": int(rnd),\n",
        "            \"local_epochs\": int(epochs),\n",
        "            \"lr\": float(lr),\n",
        "            \"best_epoch\": int(best_epoch_idx),\n",
        "            \"best_val_loss\": float(best_val_loss),\n",
        "            \"best_val_dice\": float(best_val_dice),\n",
        "            \"per_epoch\": json.dumps(epoch_logs),\n",
        "            \"fit_time_sec\": float(t_fit),\n",
        "            \"gpu_max_alloc_bytes\": int(gpu_max_alloc),\n",
        "            \"gpu_max_reserved_bytes\": int(gpu_max_reserved),\n",
        "            \"rss_start_bytes\": int(rss0),\n",
        "            \"rss_end_bytes\": int(rss1),\n",
        "            \"rss_delta_bytes\": int(rss_delta),\n",
        "            \"n_params\": int(self.n_params),\n",
        "            \"model_bytes\": int(self.model_bytes),\n",
        "        }\n",
        "\n",
        "        maybe_save_best(\n",
        "            self.run_dir,\n",
        "            self.cid,\n",
        "            best_val_loss,\n",
        "            best_val_dice,\n",
        "            best_epoch_idx,\n",
        "            rnd,\n",
        "            self.model,\n",
        "        )\n",
        "\n",
        "        return get_parameters(self.model), self.ntr, train_metrics\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "        set_parameters(self.model, parameters)\n",
        "        self.model.eval()\n",
        "\n",
        "        t_ev0 = time.perf_counter()\n",
        "        rss0 = get_rss_bytes()\n",
        "\n",
        "        if DEVICE.type == \"cuda\":\n",
        "            torch.cuda.reset_peak_memory_stats()\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        tot_loss = tot_d = tot_i = tot_a = 0.0\n",
        "        nb = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in self.val_loader:\n",
        "                x = batch[\"x\"].to(DEVICE)\n",
        "                y = batch[\"y\"].to(DEVICE)\n",
        "                pred = self.model(x)\n",
        "\n",
        "                loss = float(criterion(pred, y).item())\n",
        "                y_hat = (torch.sigmoid(pred).cpu().numpy() > 0.5).astype(\n",
        "                    np.uint8\n",
        "                )\n",
        "                y_np = (y.cpu().numpy() > 0.5).astype(np.uint8)\n",
        "                d, i, a = calc_metrics(y_np, y_hat)\n",
        "\n",
        "                tot_loss += loss\n",
        "                tot_d += d\n",
        "                tot_i += i\n",
        "                tot_a += a\n",
        "                nb += 1\n",
        "\n",
        "        nb = max(nb, 1)\n",
        "\n",
        "        if DEVICE.type == \"cuda\":\n",
        "            torch.cuda.synchronize()\n",
        "            gpu_max_alloc = int(torch.cuda.max_memory_allocated())\n",
        "            gpu_max_reserved = int(torch.cuda.max_memory_reserved())\n",
        "        else:\n",
        "            gpu_max_alloc = -1\n",
        "            gpu_max_reserved = -1\n",
        "\n",
        "        t_ev = time.perf_counter() - t_ev0\n",
        "        rss1 = get_rss_bytes()\n",
        "        rss_delta = (rss1 - rss0) if (rss0 >= 0 and rss1 >= 0) else -1\n",
        "\n",
        "        metrics = {\n",
        "            \"loss\": tot_loss / nb,\n",
        "            \"dice\": tot_d / nb,\n",
        "            \"iou\": tot_i / nb,\n",
        "            \"acc\": tot_a / nb,\n",
        "            \"cid\": int(self.cid),\n",
        "            \"run_name\": self.run_name,\n",
        "            \"eval_time_sec\": float(t_ev),\n",
        "            \"gpu_max_alloc_bytes\": int(gpu_max_alloc),\n",
        "            \"gpu_max_reserved_bytes\": int(gpu_max_reserved),\n",
        "            \"rss_start_bytes\": int(rss0),\n",
        "            \"rss_end_bytes\": int(rss1),\n",
        "            \"rss_delta_bytes\": int(rss_delta),\n",
        "            \"n_params\": int(self.n_params),\n",
        "            \"model_bytes\": int(self.model_bytes),\n",
        "        }\n",
        "\n",
        "        return metrics[\"loss\"], self.nva, metrics\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--cid\", type=int, required=True)\n",
        "    parser.add_argument(\"--server\", default=\"0.0.0.0:8080\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    fl.client.start_numpy_client(\n",
        "        server_address=args.server,\n",
        "        client=SegClient(args.cid),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38AkBD0Kb16I",
        "outputId": "b3016465-a74b-4e69-8287-f47b27543b9a"
      },
      "outputs": [],
      "source": [
        "%%writefile fl_sim_colab.py\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import flwr as fl\n",
        "from flwr.common import FitIns\n",
        "\n",
        "from fl_client import SegClient\n",
        "\n",
        "\n",
        "def ensure_csv(path: str, header: list[str]):\n",
        "    if not os.path.isfile(path):\n",
        "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "        with open(path, \"w\", newline=\"\") as f:\n",
        "            csv.writer(f).writerow(header)\n",
        "\n",
        "\n",
        "def append_row(path: str, row: list):\n",
        "    with open(path, \"a\", newline=\"\") as f:\n",
        "        csv.writer(f).writerow(row)\n",
        "\n",
        "\n",
        "def fmt_seconds(sec: float) -> str:\n",
        "    sec = float(sec)\n",
        "    h = int(sec // 3600)\n",
        "    m = int((sec % 3600) // 60)\n",
        "    s = sec % 60\n",
        "    return f\"{h:02d}:{m:02d}:{s:05.2f}\"\n",
        "\n",
        "\n",
        "def save_summary_txt(\n",
        "    base_dir: str, num_rounds: int, local_epochs: int, elapsed_seconds: float\n",
        "):\n",
        "    os.makedirs(base_dir, exist_ok=True)\n",
        "    out_path = os.path.join(base_dir, \"experiment_summary.txt\")\n",
        "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    out_path_ts = os.path.join(base_dir, f\"experiment_summary_{ts}.txt\")\n",
        "\n",
        "    txt = (\n",
        "        \"=== Experiment summary ===\\n\"\n",
        "        f\"Timestamp: {datetime.now().isoformat(timespec='seconds')}\\n\\n\"\n",
        "        f\"Rounds: {int(num_rounds)}\\n\"\n",
        "        f\"Local epochs per round: {int(local_epochs)}\\n\\n\"\n",
        "        f\"Total duration (seconds): {float(elapsed_seconds):.4f}\\n\"\n",
        "        f\"Total duration (hh:mm:ss): {fmt_seconds(elapsed_seconds)}\\n\"\n",
        "    )\n",
        "\n",
        "    for p in (out_path, out_path_ts):\n",
        "        with open(p, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(txt)\n",
        "\n",
        "\n",
        "class PerClientLoggingFedAvg(fl.server.strategy.FedAvg):\n",
        "    def __init__(self, metrics_dir: str, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.metrics_dir = metrics_dir\n",
        "        self.header = [\n",
        "            \"round\",\n",
        "            \"epoch\",\n",
        "            \"train_loss\",\n",
        "            \"train_dice\",\n",
        "            \"train_iou\",\n",
        "            \"train_acc\",\n",
        "            \"val_loss\",\n",
        "            \"val_dice\",\n",
        "            \"val_iou\",\n",
        "            \"val_acc\",\n",
        "            \"best_epoch\",\n",
        "        ]\n",
        "        self.round_fit_times = []\n",
        "        self.round_eval_times = []\n",
        "        self.round_gpu_alloc = []\n",
        "        self.round_rss_delta = []\n",
        "        self.last_model_bytes = None\n",
        "        self.last_n_params = None\n",
        "\n",
        "    def configure_fit(self, server_round, parameters, client_manager):\n",
        "        items = super().configure_fit(server_round, parameters, client_manager)\n",
        "        out = []\n",
        "        for it in items:\n",
        "            if isinstance(it, tuple):\n",
        "                client, fitins = it\n",
        "            else:\n",
        "                client, fitins = None, it\n",
        "            cfg = dict(fitins.config)\n",
        "            cfg[\"round\"] = server_round\n",
        "            new_fitins = FitIns(fitins.parameters, cfg)\n",
        "            out.append(\n",
        "                (client, new_fitins) if client is not None else new_fitins\n",
        "            )\n",
        "        return out\n",
        "\n",
        "    def aggregate_fit(self, rnd, results, failures):\n",
        "        agg = super().aggregate_fit(rnd, results, failures)\n",
        "\n",
        "        for client_proxy, fit_res in results:\n",
        "            m = fit_res.metrics or {}\n",
        "            cid = str(m.get(\"cid\", client_proxy.cid))\n",
        "\n",
        "            ft = m.get(\"fit_time_sec\", None)\n",
        "            if ft is not None:\n",
        "                self.round_fit_times.append(float(ft))\n",
        "\n",
        "            ga = m.get(\"gpu_max_alloc_bytes\", None)\n",
        "            if ga is not None and int(ga) >= 0:\n",
        "                self.round_gpu_alloc.append(int(ga))\n",
        "\n",
        "            rd = m.get(\"rss_delta_bytes\", None)\n",
        "            if rd is not None and int(rd) >= 0:\n",
        "                self.round_rss_delta.append(int(rd))\n",
        "\n",
        "            mb = m.get(\"model_bytes\", None)\n",
        "            np_ = m.get(\"n_params\", None)\n",
        "            if mb is not None:\n",
        "                self.last_model_bytes = int(mb)\n",
        "            if np_ is not None:\n",
        "                self.last_n_params = int(np_)\n",
        "\n",
        "            client_csv = os.path.join(\n",
        "                self.metrics_dir, f\"metrics_client_{cid}.csv\"\n",
        "            )\n",
        "            ensure_csv(client_csv, self.header)\n",
        "\n",
        "            best_epoch = int(m.get(\"best_epoch\", -1))\n",
        "            per_epoch_raw = m.get(\"per_epoch\", \"[]\")\n",
        "\n",
        "            try:\n",
        "                per_epoch = json.loads(per_epoch_raw)\n",
        "            except Exception:\n",
        "                per_epoch = []\n",
        "\n",
        "            for ep in per_epoch:\n",
        "                epoch = ep.get(\"epoch\", \"\")\n",
        "                row = [\n",
        "                    rnd,\n",
        "                    epoch,\n",
        "                    ep.get(\"train_loss\", \"\"),\n",
        "                    ep.get(\"train_dice\", \"\"),\n",
        "                    ep.get(\"train_iou\", \"\"),\n",
        "                    ep.get(\"train_acc\", \"\"),\n",
        "                    ep.get(\"val_loss\", \"\"),\n",
        "                    ep.get(\"val_dice\", \"\"),\n",
        "                    ep.get(\"val_iou\", \"\"),\n",
        "                    ep.get(\"val_acc\", \"\"),\n",
        "                    \"x\" if int(epoch) == best_epoch else \"\",\n",
        "                ]\n",
        "                append_row(client_csv, row)\n",
        "\n",
        "        return agg\n",
        "\n",
        "    def aggregate_evaluate(self, rnd, results, failures):\n",
        "        agg = super().aggregate_evaluate(rnd, results, failures)\n",
        "        for client_proxy, eval_res in results:\n",
        "            m = eval_res.metrics or {}\n",
        "            et = m.get(\"eval_time_sec\", None)\n",
        "            if et is not None:\n",
        "                self.round_eval_times.append(float(et))\n",
        "        return agg\n",
        "\n",
        "\n",
        "def run_one_experiment(\n",
        "    model_name: str, encoder_name: str, num_rounds=5, local_epochs=5, lr=1e-3\n",
        "):\n",
        "    run_name = f\"{model_name}__{encoder_name}\".replace(\"/\", \"-\")\n",
        "    base_dir = os.path.join(\"AITDM\", run_name)\n",
        "    metrics_dir = os.path.join(base_dir, \"metrics\")\n",
        "    os.makedirs(metrics_dir, exist_ok=True)\n",
        "\n",
        "    exp_csv = os.path.join(base_dir, \"experiment_metrics.csv\")\n",
        "    ensure_csv(\n",
        "        exp_csv,\n",
        "        [\n",
        "            \"run_name\",\n",
        "            \"model_name\",\n",
        "            \"encoder_name\",\n",
        "            \"num_rounds\",\n",
        "            \"local_epochs\",\n",
        "            \"lr\",\n",
        "            \"elapsed_sec\",\n",
        "            \"model_bytes\",\n",
        "            \"n_params\",\n",
        "            \"est_total_comm_mb\",\n",
        "            \"avg_fit_time_sec_per_round_per_client\",\n",
        "            \"avg_eval_time_sec_per_round_per_client\",\n",
        "            \"avg_gpu_max_alloc_mb\",\n",
        "            \"avg_rss_delta_mb\",\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "\n",
        "    def client_fn(cid: str):\n",
        "        return SegClient(\n",
        "            int(cid), model_name=model_name, encoder_name=encoder_name\n",
        "        ).to_client()\n",
        "\n",
        "    strategy = PerClientLoggingFedAvg(\n",
        "        metrics_dir=metrics_dir,\n",
        "        fraction_fit=1.0,\n",
        "        fraction_evaluate=1.0,\n",
        "        min_fit_clients=7,\n",
        "        min_evaluate_clients=7,\n",
        "        min_available_clients=7,\n",
        "        on_fit_config_fn=lambda rnd: {\"local_epochs\": local_epochs, \"lr\": lr},\n",
        "    )\n",
        "\n",
        "    use_gpu = torch.cuda.is_available()\n",
        "    client_resources = {\"num_cpus\": 1, \"num_gpus\": 1.0 if use_gpu else 0.0}\n",
        "\n",
        "    fl.simulation.start_simulation(\n",
        "        client_fn=client_fn,\n",
        "        num_clients=7,\n",
        "        config=fl.server.ServerConfig(num_rounds=num_rounds),\n",
        "        strategy=strategy,\n",
        "        client_resources=client_resources,\n",
        "        ray_init_args={\"include_dashboard\": False},\n",
        "    )\n",
        "\n",
        "    elapsed = time.perf_counter() - t0\n",
        "\n",
        "    model_bytes = (\n",
        "        strategy.last_model_bytes\n",
        "        if strategy.last_model_bytes is not None\n",
        "        else -1\n",
        "    )\n",
        "    n_params = (\n",
        "        strategy.last_n_params if strategy.last_n_params is not None else -1\n",
        "    )\n",
        "\n",
        "    num_clients = 7\n",
        "    if model_bytes > 0:\n",
        "        est_total_comm_mb = (num_rounds * num_clients * 2.0 * model_bytes) / (\n",
        "            1024**2\n",
        "        )\n",
        "    else:\n",
        "        est_total_comm_mb = -1.0\n",
        "\n",
        "    def _avg(xs):\n",
        "        return float(sum(xs) / max(len(xs), 1)) if xs else -1.0\n",
        "\n",
        "    avg_fit = _avg(strategy.round_fit_times)\n",
        "    avg_eval = _avg(strategy.round_eval_times)\n",
        "    avg_gpu_mb = (\n",
        "        _avg([b / (1024**2) for b in strategy.round_gpu_alloc])\n",
        "        if strategy.round_gpu_alloc\n",
        "        else -1.0\n",
        "    )\n",
        "    avg_rss_mb = (\n",
        "        _avg([b / (1024**2) for b in strategy.round_rss_delta])\n",
        "        if strategy.round_rss_delta\n",
        "        else -1.0\n",
        "    )\n",
        "\n",
        "    append_row(\n",
        "        exp_csv,\n",
        "        [\n",
        "            run_name,\n",
        "            model_name,\n",
        "            encoder_name,\n",
        "            int(num_rounds),\n",
        "            int(local_epochs),\n",
        "            float(lr),\n",
        "            float(elapsed),\n",
        "            int(model_bytes),\n",
        "            int(n_params),\n",
        "            float(est_total_comm_mb),\n",
        "            float(avg_fit),\n",
        "            float(avg_eval),\n",
        "            float(avg_gpu_mb),\n",
        "            float(avg_rss_mb),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    save_summary_txt(\n",
        "        base_dir=base_dir,\n",
        "        num_rounds=num_rounds,\n",
        "        local_epochs=local_epochs,\n",
        "        elapsed_seconds=elapsed,\n",
        "    )\n",
        "    print(\n",
        "        f\"[{run_name}] total duration: {fmt_seconds(elapsed)} ({elapsed:.2f}s)\"\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    t_all0 = time.perf_counter()\n",
        "\n",
        "    experiments = [\n",
        "        (\"unet\", \"resnet50\"),\n",
        "        (\"unet\", \"mit_b3\"),\n",
        "        (\"deeplabv3plus\", \"timm-mobilenetv3_small_100\"),\n",
        "    ]\n",
        "\n",
        "    for model_name, encoder_name in experiments:\n",
        "        print(f\"\\n=== Running: {model_name} + {encoder_name} ===\")\n",
        "        run_one_experiment(\n",
        "            model_name, encoder_name, num_rounds=7, local_epochs=7, lr=1e-3\n",
        "        )\n",
        "\n",
        "    total_all = time.perf_counter() - t_all0\n",
        "    print(\n",
        "        f\"\\n=== TOTAL (all experiments) === {fmt_seconds(total_all)} ({total_all:.2f}s)\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0ZJ8Ee2b45Y",
        "outputId": "bca65e38-72ce-47c8-c115-026b2da51908"
      },
      "outputs": [],
      "source": [
        "!python fl_sim_colab.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqeZ1twexkyB"
      },
      "source": [
        "**3 clients**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "J2mAaqi34g92",
        "outputId": "9cf81364-fdaa-47df-a061-1133d41e5d43"
      },
      "outputs": [],
      "source": [
        "import os, json, random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import segmentation_models_pytorch as smp\n",
        "import logging\n",
        "\n",
        "from seg_data import (\n",
        "    ImageOnlyGliomaDataset,\n",
        "    SubsetByPIDs,\n",
        "    image_only_collate_fn,\n",
        "    DATA_ROOT,\n",
        "    METADATA_DF_PATH,\n",
        "    USE_ATLAS,\n",
        "    calc_metrics,\n",
        ")\n",
        "\n",
        "logging.getLogger(\"timm.models._builder\").setLevel(logging.ERROR)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SEED = 42\n",
        "THRESHOLD = 0.5\n",
        "\n",
        "CLIENT_DIR = \"client\"\n",
        "BATCH_SIZE = 8\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "ENSEMBLE_CFGS = [\n",
        "    (\"unet\", \"resnet50\"),\n",
        "    (\"unet\", \"mit_b3\"),\n",
        "    (\"deeplabv3plus\", \"timm-mobilenetv3_small_100\"),\n",
        "]\n",
        "\n",
        "WEIGHT_MODE = \"power\"\n",
        "WEIGHT_POWER = 8.0\n",
        "WEIGHT_EPS = 1e-6\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "set_seed(SEED)\n",
        "\n",
        "bce = nn.BCEWithLogitsLoss()\n",
        "dice_loss = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
        "\n",
        "\n",
        "def criterion(logits, y):\n",
        "    return 0.5 * bce(logits, y) + 0.5 * dice_loss(logits, y)\n",
        "\n",
        "\n",
        "def get_val_loader(cid: int):\n",
        "    full = ImageOnlyGliomaDataset(\n",
        "        METADATA_DF_PATH,\n",
        "        DATA_ROOT,\n",
        "        use_atlas=USE_ATLAS,\n",
        "        exclude_ids=[\"PatientID_0191\"],\n",
        "    )\n",
        "    with open(os.path.join(CLIENT_DIR, f\"client_{cid}\", \"val_pids.json\")) as f:\n",
        "        va_p = json.load(f)\n",
        "\n",
        "    ds_va = SubsetByPIDs(full, va_p)\n",
        "    g = torch.Generator().manual_seed(SEED)\n",
        "\n",
        "    ld_va = torch.utils.data.DataLoader(\n",
        "        ds_va,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "        collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "        generator=g,\n",
        "    )\n",
        "    return ld_va, len(ds_va)\n",
        "\n",
        "\n",
        "def run_name(model_name: str, encoder_name: str) -> str:\n",
        "    return f\"{model_name}__{encoder_name}\".replace(\"/\", \"-\")\n",
        "\n",
        "\n",
        "def build_model(model_name: str, encoder_name: str, encoder_weights=\"imagenet\"):\n",
        "    in_ch = 2 if USE_ATLAS else 1\n",
        "    mn = model_name.lower()\n",
        "\n",
        "    if mn == \"unet\":\n",
        "        m = smp.Unet(\n",
        "            encoder_name=encoder_name,\n",
        "            encoder_weights=encoder_weights,\n",
        "            in_channels=in_ch,\n",
        "            classes=1,\n",
        "        )\n",
        "    elif mn in [\"deeplabv3plus\", \"deeplabv3+\", \"dlv3p\"]:\n",
        "        m = smp.DeepLabV3Plus(\n",
        "            encoder_name=encoder_name,\n",
        "            encoder_weights=encoder_weights,\n",
        "            in_channels=in_ch,\n",
        "            classes=1,\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model_name={model_name}\")\n",
        "\n",
        "    return m.to(DEVICE)\n",
        "\n",
        "\n",
        "def ckpt_path(model_name: str, encoder_name: str, cid: int) -> str:\n",
        "    rn = run_name(model_name, encoder_name)\n",
        "    return os.path.join(\"AITDM\", rn, \"checkpoints\", f\"client_{cid}_best.pt\")\n",
        "\n",
        "\n",
        "def best_json_path(model_name: str, encoder_name: str, cid: int) -> str:\n",
        "    rn = run_name(model_name, encoder_name)\n",
        "    return os.path.join(\"AITDM\", rn, \"checkpoints\", f\"client_{cid}_best.json\")\n",
        "\n",
        "\n",
        "def load_model(model_name: str, encoder_name: str, cid: int):\n",
        "    path = ckpt_path(model_name, encoder_name, cid)\n",
        "    if not os.path.isfile(path):\n",
        "        raise FileNotFoundError(f\"Missing checkpoint: {path}\")\n",
        "    m = build_model(model_name, encoder_name).to(DEVICE)\n",
        "    sd = torch.load(path, map_location=\"cpu\")\n",
        "    m.load_state_dict(sd, strict=True)\n",
        "    m.eval()\n",
        "    return m\n",
        "\n",
        "\n",
        "def load_best_val_dice(model_name: str, encoder_name: str, cid: int) -> float:\n",
        "    p = best_json_path(model_name, encoder_name, cid)\n",
        "    if not os.path.isfile(p):\n",
        "        raise FileNotFoundError(f\"Missing best json: {p}\")\n",
        "    with open(p, \"r\") as f:\n",
        "        j = json.load(f)\n",
        "    return float(j.get(\"val_dice\", 0.0))\n",
        "\n",
        "\n",
        "def get_client_weights(cid: int, cfgs, mode=\"power\", power=2.0, eps=1e-6):\n",
        "    dices = [load_best_val_dice(mn, enc, cid) for (mn, enc) in cfgs]\n",
        "    d = np.array(dices, dtype=np.float32)\n",
        "\n",
        "    if mode == \"linear\":\n",
        "        raw = np.clip(d, 0.0, None)\n",
        "    elif mode == \"power\":\n",
        "        raw = np.power(np.clip(d, 0.0, None), power)\n",
        "    else:\n",
        "        raise ValueError(\"mode must be 'linear' or 'power'\")\n",
        "\n",
        "    raw = raw + eps\n",
        "    w = raw / raw.sum()\n",
        "    return w.tolist(), dices\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def ensemble_forward_logits(x, models, weights, target_hw=None):\n",
        "    w = np.array(weights, dtype=np.float32)\n",
        "    w = w / (w.sum() + 1e-8)\n",
        "\n",
        "    logits_sum = None\n",
        "    for mi, wi in zip(models, w):\n",
        "        li = mi(x)\n",
        "        if target_hw is not None and li.shape[-2:] != target_hw:\n",
        "            li = F.interpolate(\n",
        "                li, size=target_hw, mode=\"bilinear\", align_corners=False\n",
        "            )\n",
        "        logits_sum = (\n",
        "            li * float(wi)\n",
        "            if logits_sum is None\n",
        "            else logits_sum + li * float(wi)\n",
        "        )\n",
        "    return logits_sum\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_ensemble_on_client(cid: int, threshold=0.5):\n",
        "    val_loader, nva = get_val_loader(cid)\n",
        "    models = [load_model(mn, enc, cid) for (mn, enc) in ENSEMBLE_CFGS]\n",
        "\n",
        "    weights, best_dices = get_client_weights(\n",
        "        cid, ENSEMBLE_CFGS, mode=WEIGHT_MODE, power=WEIGHT_POWER, eps=WEIGHT_EPS\n",
        "    )\n",
        "\n",
        "    tot_loss = tot_d = tot_i = tot_a = 0.0\n",
        "    nb = 0\n",
        "\n",
        "    for batch in val_loader:\n",
        "        x = batch[\"x\"].to(DEVICE)\n",
        "        y = batch[\"y\"].to(DEVICE)\n",
        "\n",
        "        logits = ensemble_forward_logits(\n",
        "            x, models=models, weights=weights, target_hw=y.shape[-2:]\n",
        "        )\n",
        "        loss = float(criterion(logits, y).item())\n",
        "\n",
        "        preds_bin = (torch.sigmoid(logits).cpu().numpy() > threshold).astype(\n",
        "            np.uint8\n",
        "        )\n",
        "        y_np = (y.cpu().numpy() > 0.5).astype(np.uint8)\n",
        "\n",
        "        d, i, a = calc_metrics(y_np, preds_bin)\n",
        "\n",
        "        tot_loss += loss\n",
        "        tot_d += d\n",
        "        tot_i += i\n",
        "        tot_a += a\n",
        "        nb += 1\n",
        "\n",
        "    nb = max(nb, 1)\n",
        "    return {\n",
        "        \"cid\": int(cid),\n",
        "        \"nva\": int(nva),\n",
        "        \"loss\": tot_loss / nb,\n",
        "        \"dice\": tot_d / nb,\n",
        "        \"iou\": tot_i / nb,\n",
        "        \"acc\": tot_a / nb,\n",
        "        \"weights\": weights,\n",
        "        \"best_dices\": best_dices,\n",
        "    }\n",
        "\n",
        "\n",
        "def format_report(results) -> str:\n",
        "    lines = []\n",
        "    lines.append(\n",
        "        f\"[Ensemble-3 | threshold={THRESHOLD} | weight_mode={WEIGHT_MODE} | power={WEIGHT_POWER}]\"\n",
        "    )\n",
        "    lines.append(\"\")\n",
        "    for r in results:\n",
        "        bd = r[\"best_dices\"]\n",
        "        ww = r[\"weights\"]\n",
        "        lines.append(\n",
        "            f\"Client {r['cid']} (n={r['nva']}): \"\n",
        "            f\"loss={r['loss']:.4f} dice={r['dice']:.4f} iou={r['iou']:.4f} acc={r['acc']:.4f}\"\n",
        "        )\n",
        "        lines.append(\n",
        "            f\"  best_dice={['%.4f' % x for x in bd]} -> weights={['%.3f' % x for x in ww]}\"\n",
        "        )\n",
        "        lines.append(\"\")\n",
        "    return \"\\n\".join(lines).rstrip() + \"\\n\"\n",
        "\n",
        "\n",
        "def main():\n",
        "    results = []\n",
        "    for cid in [0, 1, 2]:\n",
        "        results.append(eval_ensemble_on_client(cid, threshold=THRESHOLD))\n",
        "\n",
        "    report = format_report(results)\n",
        "    print(report, end=\"\")\n",
        "\n",
        "    out_path = \"ensemble.txt\"\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(report)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8gckmP9xmJS"
      },
      "source": [
        "**5 clients**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EDwvt_Nxnip",
        "outputId": "5952d1e8-acf1-45d8-af44-f86e67ac052b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import segmentation_models_pytorch as smp\n",
        "import logging\n",
        "\n",
        "from seg_data import (\n",
        "    ImageOnlyGliomaDataset,\n",
        "    SubsetByPIDs,\n",
        "    image_only_collate_fn,\n",
        "    DATA_ROOT,\n",
        "    METADATA_DF_PATH,\n",
        "    USE_ATLAS,\n",
        "    calc_metrics,\n",
        ")\n",
        "\n",
        "logging.getLogger(\"timm.models._builder\").setLevel(logging.ERROR)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SEED = 42\n",
        "THRESHOLD = 0.5\n",
        "\n",
        "CLIENT_DIR = \"client\"\n",
        "BATCH_SIZE = 8\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "CLIENT_IDS = list(range(7))\n",
        "\n",
        "ENSEMBLE_CFGS = [\n",
        "    (\"unet\", \"resnet50\"),\n",
        "    (\"unet\", \"mit_b3\"),\n",
        "    (\"deeplabv3plus\", \"timm-mobilenetv3_small_100\"),\n",
        "]\n",
        "\n",
        "WEIGHT_MODE = \"power\"\n",
        "WEIGHT_POWER = 8.0\n",
        "WEIGHT_EPS = 1e-6\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "set_seed(SEED)\n",
        "\n",
        "bce = nn.BCEWithLogitsLoss()\n",
        "dice_loss = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
        "\n",
        "\n",
        "def criterion(logits, y):\n",
        "    return 0.5 * bce(logits, y) + 0.5 * dice_loss(logits, y)\n",
        "\n",
        "\n",
        "def get_val_loader(cid: int):\n",
        "    full = ImageOnlyGliomaDataset(\n",
        "        METADATA_DF_PATH,\n",
        "        DATA_ROOT,\n",
        "        use_atlas=USE_ATLAS,\n",
        "        exclude_ids=[\"PatientID_0191\"],\n",
        "    )\n",
        "\n",
        "    with open(\n",
        "        os.path.join(CLIENT_DIR, f\"client_{cid}\", \"val_pids.json\"), \"r\"\n",
        "    ) as f:\n",
        "        val_pids = json.load(f)\n",
        "\n",
        "    ds_val = SubsetByPIDs(full, val_pids)\n",
        "\n",
        "    g = torch.Generator().manual_seed(SEED)\n",
        "    loader = torch.utils.data.DataLoader(\n",
        "        ds_val,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "        collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "        generator=g,\n",
        "    )\n",
        "    return loader, len(ds_val)\n",
        "\n",
        "\n",
        "def run_name(model_name: str, encoder_name: str) -> str:\n",
        "    return f\"{model_name}__{encoder_name}\".replace(\"/\", \"-\")\n",
        "\n",
        "\n",
        "def build_model(model_name: str, encoder_name: str, encoder_weights=\"imagenet\"):\n",
        "    in_channels = 2 if USE_ATLAS else 1\n",
        "    name = model_name.lower()\n",
        "\n",
        "    if name == \"unet\":\n",
        "        model = smp.Unet(\n",
        "            encoder_name=encoder_name,\n",
        "            encoder_weights=encoder_weights,\n",
        "            in_channels=in_channels,\n",
        "            classes=1,\n",
        "        )\n",
        "    elif name in [\"deeplabv3plus\", \"deeplabv3+\", \"dlv3p\"]:\n",
        "        model = smp.DeepLabV3Plus(\n",
        "            encoder_name=encoder_name,\n",
        "            encoder_weights=encoder_weights,\n",
        "            in_channels=in_channels,\n",
        "            classes=1,\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(model_name)\n",
        "\n",
        "    return model.to(DEVICE)\n",
        "\n",
        "\n",
        "def ckpt_path(model_name: str, encoder_name: str, cid: int) -> str:\n",
        "    rn = run_name(model_name, encoder_name)\n",
        "    return os.path.join(\"AITDM\", rn, \"checkpoints\", f\"client_{cid}_best.pt\")\n",
        "\n",
        "\n",
        "def best_json_path(model_name: str, encoder_name: str, cid: int) -> str:\n",
        "    rn = run_name(model_name, encoder_name)\n",
        "    return os.path.join(\"AITDM\", rn, \"checkpoints\", f\"client_{cid}_best.json\")\n",
        "\n",
        "\n",
        "def try_load_model(model_name: str, encoder_name: str, cid: int):\n",
        "    path = ckpt_path(model_name, encoder_name, cid)\n",
        "    if not os.path.isfile(path):\n",
        "        return None\n",
        "\n",
        "    model = build_model(model_name, encoder_name)\n",
        "    state = torch.load(path, map_location=\"cpu\")\n",
        "    model.load_state_dict(state, strict=True)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "def try_load_best_val_dice(\n",
        "    model_name: str, encoder_name: str, cid: int\n",
        ") -> float:\n",
        "    path = best_json_path(model_name, encoder_name, cid)\n",
        "    if not os.path.isfile(path):\n",
        "        return 0.0\n",
        "\n",
        "    with open(path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    return float(data.get(\"val_dice\", 0.0))\n",
        "\n",
        "\n",
        "def get_client_weights(cid: int, cfgs, mode=\"power\", power=2.0, eps=1e-6):\n",
        "    dices = [try_load_best_val_dice(mn, enc, cid) for mn, enc in cfgs]\n",
        "    d = np.asarray(dices, dtype=np.float32)\n",
        "\n",
        "    if mode == \"linear\":\n",
        "        raw = np.clip(d, 0.0, None)\n",
        "    elif mode == \"power\":\n",
        "        raw = np.power(np.clip(d, 0.0, None), power)\n",
        "    else:\n",
        "        raise ValueError(mode)\n",
        "\n",
        "    raw = raw + eps\n",
        "    weights = raw / raw.sum()\n",
        "    return weights.tolist(), dices\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def ensemble_forward_logits(x, models, weights, target_hw=None):\n",
        "    w = np.asarray(weights, dtype=np.float32)\n",
        "    w = w / (w.sum() + 1e-8)\n",
        "\n",
        "    out = None\n",
        "    for model, weight in zip(models, w):\n",
        "        logits = model(x)\n",
        "        if target_hw is not None and logits.shape[-2:] != target_hw:\n",
        "            logits = F.interpolate(\n",
        "                logits, size=target_hw, mode=\"bilinear\", align_corners=False\n",
        "            )\n",
        "        out = (\n",
        "            logits * float(weight)\n",
        "            if out is None\n",
        "            else out + logits * float(weight)\n",
        "        )\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_ensemble_on_client(cid: int, threshold=0.5):\n",
        "    loader, n_val = get_val_loader(cid)\n",
        "\n",
        "    models = []\n",
        "    cfgs_ok = []\n",
        "    for mn, enc in ENSEMBLE_CFGS:\n",
        "        m = try_load_model(mn, enc, cid)\n",
        "        if m is not None:\n",
        "            models.append(m)\n",
        "            cfgs_ok.append((mn, enc))\n",
        "\n",
        "    if len(models) == 0:\n",
        "        raise RuntimeError(f\"No available checkpoints for client {cid}\")\n",
        "\n",
        "    weights, best_dices = get_client_weights(\n",
        "        cid,\n",
        "        cfgs_ok,\n",
        "        mode=WEIGHT_MODE,\n",
        "        power=WEIGHT_POWER,\n",
        "        eps=WEIGHT_EPS,\n",
        "    )\n",
        "\n",
        "    total_loss = total_dice = total_iou = total_acc = 0.0\n",
        "    steps = 0\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"x\"].to(DEVICE)\n",
        "        y = batch[\"y\"].to(DEVICE)\n",
        "\n",
        "        logits = ensemble_forward_logits(\n",
        "            x, models=models, weights=weights, target_hw=y.shape[-2:]\n",
        "        )\n",
        "\n",
        "        loss = criterion(logits, y).item()\n",
        "\n",
        "        preds = (torch.sigmoid(logits).cpu().numpy() > threshold).astype(\n",
        "            np.uint8\n",
        "        )\n",
        "        targets = (y.cpu().numpy() > 0.5).astype(np.uint8)\n",
        "\n",
        "        d, i, a = calc_metrics(targets, preds)\n",
        "\n",
        "        total_loss += loss\n",
        "        total_dice += d\n",
        "        total_iou += i\n",
        "        total_acc += a\n",
        "        steps += 1\n",
        "\n",
        "    steps = max(steps, 1)\n",
        "    return {\n",
        "        \"cid\": cid,\n",
        "        \"nva\": n_val,\n",
        "        \"loss\": total_loss / steps,\n",
        "        \"dice\": total_dice / steps,\n",
        "        \"iou\": total_iou / steps,\n",
        "        \"acc\": total_acc / steps,\n",
        "        \"weights\": weights,\n",
        "        \"best_dices\": best_dices,\n",
        "        \"used_cfgs\": cfgs_ok,\n",
        "    }\n",
        "\n",
        "\n",
        "def format_report(results) -> str:\n",
        "    lines = []\n",
        "    lines.append(\n",
        "        f\"[Ensemble | threshold={THRESHOLD} | weight_mode={WEIGHT_MODE} | power={WEIGHT_POWER}]\"\n",
        "    )\n",
        "    lines.append(\"\")\n",
        "\n",
        "    for r in results:\n",
        "        lines.append(\n",
        "            f\"Client {r['cid']} (n={r['nva']}): \"\n",
        "            f\"loss={r['loss']:.4f} dice={r['dice']:.4f} \"\n",
        "            f\"iou={r['iou']:.4f} acc={r['acc']:.4f}\"\n",
        "        )\n",
        "        lines.append(\n",
        "            f\"  best_dice={['%.4f' % x for x in r['best_dices']]} \"\n",
        "            f\"-> weights={['%.3f' % x for x in r['weights']]}\"\n",
        "        )\n",
        "        lines.append(f\"  used_cfgs={r['used_cfgs']}\")\n",
        "        lines.append(\"\")\n",
        "\n",
        "    return \"\\n\".join(lines).rstrip() + \"\\n\"\n",
        "\n",
        "\n",
        "def main():\n",
        "    results = [\n",
        "        eval_ensemble_on_client(cid, threshold=THRESHOLD) for cid in CLIENT_IDS\n",
        "    ]\n",
        "    report = format_report(results)\n",
        "    print(report, end=\"\")\n",
        "\n",
        "    with open(\"ensemble.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(report)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwwElHbwLpjB",
        "outputId": "b5b2aac7-f41a-4a66-9c26-40f588571604"
      },
      "outputs": [],
      "source": [
        "! zip -r AITDM.zip AITDM -x \"*/checkpoints/*\" \"*/checkpoints\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "JPU64UV3E4v9",
        "zeATNHiwE86m",
        "WzRm-EHu8iCx",
        "tXoXxVZ4Lig_",
        "3Pf52xcpQyUg"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "machine_learning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.14.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "190d9c431dad4fe6aeb719c53aae00e3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1de8b89d5f84448988da5a57ed013b87": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2079106bec34430bac8b49b9c58c2b1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a7d5d99927547aba220065b78ce3baa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4f64d81650e494d90b558da50c3c940",
            "max": 87275112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d0ed26d8e9434b3495befb420c83050c",
            "value": 87275112
          }
        },
        "2f2fea9499654a3ab85ebba399a5eae5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7498d427ce53446887252636c149da43",
            "max": 156,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_47007b561c284a9ead6056d8d06f81b9",
            "value": 156
          }
        },
        "3117f186ea8f4403bba5499a1ffbd3d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_746baebad78a4870904a3e11ae57081a",
              "IPY_MODEL_2f2fea9499654a3ab85ebba399a5eae5",
              "IPY_MODEL_ae7a43193675492c8f5a9d7f0517f73d"
            ],
            "layout": "IPY_MODEL_81b79dac85eb44aaab0c89b3858fd70a"
          }
        },
        "32424859b7944119b90eed837ce124d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33211f55587948ef8c631c8f5eb5f42d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43835fe3d6d740868ffd71ae83bec2ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47007b561c284a9ead6056d8d06f81b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4955b82bb8ef4cd4a23cf7e213d04b40": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "746baebad78a4870904a3e11ae57081a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4955b82bb8ef4cd4a23cf7e213d04b40",
            "placeholder": "​",
            "style": "IPY_MODEL_d53d9a6f5e344ac08bc684c74dca1a54",
            "value": "config.json: 100%"
          }
        },
        "7498d427ce53446887252636c149da43": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76986cd4b96d49f48560a18268c337d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_190d9c431dad4fe6aeb719c53aae00e3",
            "placeholder": "​",
            "style": "IPY_MODEL_43835fe3d6d740868ffd71ae83bec2ef",
            "value": " 87.3M/87.3M [00:02&lt;00:00, 38.6MB/s]"
          }
        },
        "81b79dac85eb44aaab0c89b3858fd70a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae7a43193675492c8f5a9d7f0517f73d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33211f55587948ef8c631c8f5eb5f42d",
            "placeholder": "​",
            "style": "IPY_MODEL_32424859b7944119b90eed837ce124d7",
            "value": " 156/156 [00:00&lt;00:00, 6.84kB/s]"
          }
        },
        "b4f64d81650e494d90b558da50c3c940": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c51731484e434818ad01f3ce139af20f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f834de8b38b344bab710da19e9765023",
            "placeholder": "​",
            "style": "IPY_MODEL_2079106bec34430bac8b49b9c58c2b1c",
            "value": "model.safetensors: 100%"
          }
        },
        "d0ed26d8e9434b3495befb420c83050c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d156bff5a12b4fd2a402f76fd45ae1a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c51731484e434818ad01f3ce139af20f",
              "IPY_MODEL_2a7d5d99927547aba220065b78ce3baa",
              "IPY_MODEL_76986cd4b96d49f48560a18268c337d0"
            ],
            "layout": "IPY_MODEL_1de8b89d5f84448988da5a57ed013b87"
          }
        },
        "d53d9a6f5e344ac08bc684c74dca1a54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f834de8b38b344bab710da19e9765023": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
