{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Sj5tPAuEKUbY",
        "6Nw80ekO7R0i",
        "RYfjGgr9E0Su",
        "JPU64UV3E4v9",
        "zeATNHiwE86m",
        "WzRm-EHu8iCx",
        "tXoXxVZ4Lig_",
        "3Pf52xcpQyUg"
      ],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>Enhancing Brain Disease Diagnosis<h1>**\n",
        "\n",
        "**<h2>Federated Learning for Multi-Center Medical Imaging<h2>**\n",
        "\n",
        "**<h3>AI for Trustworthy Decision Making<h3>**\n",
        "\n",
        "- Poață Andrei-Cătălin\n",
        "\n",
        "- Vulpe Ștefan\n",
        "\n",
        "- Vișan Ionuț\n",
        "\n",
        "*github: https://github.com/stefanvulpe-dev/brain-disease-diagnosis*"
      ],
      "metadata": {
        "id": "mM4J2YDWDx4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup**"
      ],
      "metadata": {
        "id": "Sj5tPAuEKUbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp /content/drive/MyDrive/AITDM_Stuff/Preprocessed-Data.zip /content/Preprocessed-Data.zip\n",
        "!cp /content/drive/MyDrive/AITDM_Stuff/client.zip /content/client.zip\n",
        "!cp /content/drive/MyDrive/AITDM_Stuff/cleaned_df.pkl /content/cleaned_df.pkl\n",
        "!cp /content/drive/MyDrive/AITDM_Stuff/labels_list.pkl /content/labels_list.pkl"
      ],
      "metadata": {
        "id": "XdQ3QSV1oqyq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a7374c6-cf4c-4978-a8fb-9f14e33ab1e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segmentation_models_pytorch\n",
        "!pip install torchio"
      ],
      "metadata": {
        "id": "PXlb41oIJqYa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1a9f433-a51e-4842-df19-74dc43377ad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting segmentation_models_pytorch\n",
            "  Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.12/dist-packages (from segmentation_models_pytorch) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.12/dist-packages (from segmentation_models_pytorch) (2.0.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from segmentation_models_pytorch) (11.3.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from segmentation_models_pytorch) (0.7.0)\n",
            "Requirement already satisfied: timm>=0.9 in /usr/local/lib/python3.12/dist-packages (from segmentation_models_pytorch) (1.0.22)\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.12/dist-packages (from segmentation_models_pytorch) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.12/dist-packages (from segmentation_models_pytorch) (0.24.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from segmentation_models_pytorch) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (2.32.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8->segmentation_models_pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8->segmentation_models_pytorch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (2025.11.12)\n",
            "Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: segmentation_models_pytorch\n",
            "Successfully installed segmentation_models_pytorch-0.5.0\n",
            "Collecting torchio\n",
            "  Downloading torchio-0.21.1-py3-none-any.whl.metadata (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deprecated>=1.2 (from torchio)\n",
            "  Downloading deprecated-1.3.1-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: einops>=0.3 in /usr/local/lib/python3.12/dist-packages (from torchio) (0.8.1)\n",
            "Requirement already satisfied: humanize>=0.1 in /usr/local/lib/python3.12/dist-packages (from torchio) (4.14.0)\n",
            "Requirement already satisfied: nibabel>=3 in /usr/local/lib/python3.12/dist-packages (from torchio) (5.3.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.12/dist-packages (from torchio) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from torchio) (25.0)\n",
            "Requirement already satisfied: rich>=10 in /usr/local/lib/python3.12/dist-packages (from torchio) (13.9.4)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.12/dist-packages (from torchio) (1.16.3)\n",
            "Collecting simpleitk!=2.0.*,!=2.1.1.1,>=1.3 (from torchio)\n",
            "  Downloading simpleitk-2.5.3-cp311-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.12/dist-packages (from torchio) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.40 in /usr/local/lib/python3.12/dist-packages (from torchio) (4.67.1)\n",
            "Requirement already satisfied: typer>=0.1 in /usr/local/lib/python3.12/dist-packages (from torchio) (0.20.0)\n",
            "Requirement already satisfied: wrapt<3,>=1.10 in /usr/local/lib/python3.12/dist-packages (from deprecated>=1.2->torchio) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.12/dist-packages (from nibabel>=3->torchio) (4.15.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10->torchio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10->torchio) (2.19.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->torchio) (3.5.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.1->torchio) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.1->torchio) (1.5.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10->torchio) (0.1.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.9->torchio) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.9->torchio) (3.0.3)\n",
            "Downloading torchio-0.21.1-py3-none-any.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.3/194.3 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deprecated-1.3.1-py2.py3-none-any.whl (11 kB)\n",
            "Downloading simpleitk-2.5.3-cp311-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (52.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: simpleitk, deprecated, torchio\n",
            "Successfully installed deprecated-1.3.1 simpleitk-2.5.3 torchio-0.21.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qo /content/client.zip -d /content/client"
      ],
      "metadata": {
        "id": "qYFrmuebJtzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qo /content/Preprocessed-Data.zip -d /content/Preprocessed-Data"
      ],
      "metadata": {
        "id": "5a1acYv0Ju3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \"flwr[simulation]\""
      ],
      "metadata": {
        "id": "Z_K1UZo4RoWi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2959527-57ca-4d3e-dc14-93c055ca0077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m128.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.3/323.3 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.1/250.1 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m727.1/727.1 kB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.2 which is incompatible.\n",
            "pyopenssl 24.2.1 requires cryptography<44,>=41.0.5, but you have cryptography 44.0.3 which is incompatible.\n",
            "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 44.0.3 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.2 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.33.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Main Code**"
      ],
      "metadata": {
        "id": "6Nw80ekO7R0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "class GliomaDataset(Dataset):\n",
        "    def __init__(self, metadata_df_path, labels_path, transform=None):\n",
        "        with open(metadata_df_path, 'rb') as f:\n",
        "            self.metadata_df = pickle.load(f)\n",
        "\n",
        "        self.metadata_df = self.metadata_df[self.metadata_df['Patient_ID'] != 'PatientID_0191']\n",
        "        with open(labels_path, 'rb') as f:\n",
        "            self.labels = pickle.load(f)\n",
        "\n",
        "        self.data_root = \"/content/drive/MyDrive/PKG - MU-Glioma-Post/Preprocessed-Data\"\n",
        "        self.transform = transform\n",
        "\n",
        "\n",
        "        self.useful_columns = [\n",
        "            'Patient_ID', 'Sex at Birth', 'Race', 'Age at diagnosis',\n",
        "            'Primary Diagnosis', 'H3-3A mutation', 'PTEN mutation',\n",
        "            'CDKN2A/B deletion', 'TP53 alteration', 'Other mutations/alterations',\n",
        "            'Previous Brain Tumor', 'Type of previous brain tumor',\n",
        "            'Report', 'Age Range', 'Top 5 Regions'\n",
        "        ]\n",
        "\n",
        "        self.categorical_cols = [\n",
        "            'Sex at Birth', 'Race', 'Primary Diagnosis',\n",
        "            'Previous Brain Tumor', 'Type of previous brain tumor', 'Age Range'\n",
        "        ]\n",
        "\n",
        "        self.code_maps = {}\n",
        "\n",
        "        for col in self.categorical_cols:\n",
        "            cat = pd.Categorical(self.metadata_df[col])\n",
        "            self.metadata_df[col + '_code'] = cat.codes\n",
        "            self.code_maps[col] = dict(enumerate(cat.categories))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.metadata_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.metadata_df.iloc[idx]\n",
        "\n",
        "        patient_id = row['Patient_ID']\n",
        "\n",
        "        sex = row['Sex at Birth_code']\n",
        "        race = row['Race_code']\n",
        "        primary_diagnosis = row['Primary Diagnosis_code']\n",
        "        previous_brain_tumor = row['Previous Brain Tumor_code']\n",
        "        type_of_previous_brain_tumor = row['Type of previous brain tumor_code']\n",
        "        age_range = row['Age Range_code']\n",
        "\n",
        "        age = row['Age at diagnosis']\n",
        "        h3_3a_mutation = row['H3-3A mutation']\n",
        "        pten_mutation = row['PTEN mutation']\n",
        "        CDKN2A_B_deletion = row['CDKN2A/B deletion']\n",
        "        TP53_alteration = row['TP53 alteration']\n",
        "        other_mutations_alterations = row['Other mutations/alterations']\n",
        "        report = row['Report']\n",
        "        top_5 = [self.labels.index(elem) for elem in row['Top 5 Regions']]\n",
        "\n",
        "        target_regions = np.zeros(len(self.labels))\n",
        "        for i in top_5:\n",
        "          target_regions[i] = 1\n",
        "\n",
        "        mri = np.load(self.data_root + f'/{patient_id}/{patient_id}_mri.npy')\n",
        "        regions = np.load(self.data_root + f'/{patient_id}/{patient_id}_regions.npy')\n",
        "        tumor = np.load(self.data_root + f'/{patient_id}/{patient_id}_tumor.npy')\n",
        "\n",
        "        if self.transform:\n",
        "            mri = self.transform(mri)\n",
        "            regions = self.transform(regions)\n",
        "            tumor = self.transform(tumor)\n",
        "\n",
        "        dict_output = {\n",
        "            'mri': mri,\n",
        "            'regions': regions,\n",
        "            'tumor': tumor,\n",
        "            'sex': sex,\n",
        "            'race': race,\n",
        "            'age': age,\n",
        "            'primary_diagnosis': primary_diagnosis,\n",
        "            'h3_3a_mutation': h3_3a_mutation,\n",
        "            'pten_mutation': pten_mutation,\n",
        "            'CDKN2A_B_deletion': CDKN2A_B_deletion,\n",
        "            'TP53_alteration': TP53_alteration,\n",
        "            'other_mutations_alterations': other_mutations_alterations,\n",
        "            'previous_brain_tumor': previous_brain_tumor,\n",
        "            'type_of_previous_brain_tumor': type_of_previous_brain_tumor,\n",
        "            'report': report,\n",
        "            'age_range': age_range,\n",
        "            'target_regions': target_regions,\n",
        "        }\n",
        "\n",
        "        return dict_output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def glioma_collate_fn(batch):\n",
        "    mri_batch = torch.stack([torch.tensor(item['mri']) for item in batch])\n",
        "    regions_batch = torch.stack([torch.tensor(item['regions']) for item in batch])\n",
        "    tumor_batch = torch.stack([torch.tensor(item['tumor']) for item in batch])\n",
        "\n",
        "    sex_batch = torch.tensor([item['sex'] for item in batch], dtype=torch.long)\n",
        "    race_batch = torch.tensor([item['race'] for item in batch], dtype=torch.long)\n",
        "    age_batch = torch.tensor([item['age'] for item in batch], dtype=torch.float)\n",
        "    primary_diagnosis_batch = torch.tensor([item['primary_diagnosis'] for item in batch], dtype=torch.long)\n",
        "    h3_3a_mutation_batch = torch.tensor([item['h3_3a_mutation'] for item in batch], dtype=torch.float)\n",
        "    pten_mutation_batch = torch.tensor([item['pten_mutation'] for item in batch], dtype=torch.float)\n",
        "    CDKN2A_B_deletion_batch = torch.tensor([item['CDKN2A_B_deletion'] for item in batch], dtype=torch.float)\n",
        "    TP53_alteration_batch = torch.tensor([item['TP53_alteration'] for item in batch], dtype=torch.float)\n",
        "    previous_brain_tumor_batch = torch.tensor([item['previous_brain_tumor'] for item in batch], dtype=torch.long)\n",
        "    type_of_previous_brain_tumor_batch = torch.tensor([item['type_of_previous_brain_tumor'] for item in batch], dtype=torch.long)\n",
        "    age_range_batch = torch.tensor([item['age_range'] for item in batch], dtype=torch.long)\n",
        "    target_regions_batch = torch.from_numpy(np.array([item['target_regions'] for item in batch])).float()\n",
        "\n",
        "    other_mutations_batch = [item['other_mutations_alterations'] for item in batch]\n",
        "    report_batch = [item['report'] for item in batch]\n",
        "\n",
        "    return {\n",
        "        'mri': mri_batch,\n",
        "        'regions': regions_batch,\n",
        "        'tumor': tumor_batch,\n",
        "        'sex': sex_batch,\n",
        "        'race': race_batch,\n",
        "        'age': age_batch,\n",
        "        'primary_diagnosis': primary_diagnosis_batch,\n",
        "        'h3_3a_mutation': h3_3a_mutation_batch,\n",
        "        'pten_mutation': pten_mutation_batch,\n",
        "        'CDKN2A_B_deletion': CDKN2A_B_deletion_batch,\n",
        "        'TP53_alteration': TP53_alteration_batch,\n",
        "        'other_mutations_alterations': other_mutations_batch,\n",
        "        'previous_brain_tumor': previous_brain_tumor_batch,\n",
        "        'type_of_previous_brain_tumor': type_of_previous_brain_tumor_batch,\n",
        "        'report': report_batch,\n",
        "        'age_range': age_range_batch,\n",
        "        'target_regions': target_regions_batch,\n",
        "    }"
      ],
      "metadata": {
        "id": "FfrU4Wyh7bv6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "5da861ad-2923-46b3-cca3-c40882ce5c5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2572841789.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnibabel\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_lock_unlock_module\u001b[0;34m(name)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "dataset = GliomaDataset(\"/content/drive/MyDrive/PKG - MU-Glioma-Post/cleaned_df.pkl\", \"/content/drive/MyDrive/PKG - MU-Glioma-Post/labels_list.pkl\")\n",
        "\n",
        "test_size = int(0.2 * len(dataset))\n",
        "train_size = len(dataset) - test_size\n",
        "\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=glioma_collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=glioma_collate_fn)\n",
        "\n",
        "print(f\"Train loader length = {len(train_loader)}\")\n",
        "print(f\"Test loader length = {len(test_loader)}\")\n",
        "\n",
        "for batch in train_loader:\n",
        "    print(\"Train batch:\", batch)\n",
        "    break\n",
        "\n",
        "for batch in test_loader:\n",
        "    print(\"Test batch:\", batch)\n",
        "    break"
      ],
      "metadata": {
        "id": "WpymGCI37dOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "class BioBERTMultiLabelClassifier(nn.Module):\n",
        "    def __init__(self, model_name, num_labels):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        x = self.dropout(pooled_output)\n",
        "        return self.classifier(x)\n",
        "\n",
        "# Tokenizer (Bio_ClinicalBERT e foarte bun)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Functie de procesare rapoarte\n",
        "def tokenize_reports(reports, tokenizer, max_len=512):\n",
        "    return tokenizer(\n",
        "        reports,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_len,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "def plot_and_save_history(history, save_dir=\"/content/drive/MyDrive/PKG - MU-Glioma-Post/Graphs\"):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    epochs = len(history['train_loss'])\n",
        "    epochs_range = range(1, epochs + 1)\n",
        "\n",
        "    # Loss\n",
        "    plt.figure()\n",
        "    plt.plot(epochs_range, history['train_loss'], label='Train Loss')\n",
        "    plt.plot(epochs_range, history['test_loss'], label='Test Loss')\n",
        "    plt.title(\"Bioclinical BERT Loss Evolution\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(save_dir, \"loss_evolution_biobert.png\"))\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    # F1 Score\n",
        "    plt.figure()\n",
        "    plt.plot(epochs_range, history['train_f1'], label='Train F1')\n",
        "    plt.plot(epochs_range, history['test_f1'], label='Test F1')\n",
        "    plt.title(\"Bioclinical BERT F1 Evolution\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"F1 Score\")\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(save_dir, \"f1_evolution_biobert.png\"))\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    # Accuracy (doar dacă există în history)\n",
        "    if 'train_acc' in history and 'test_acc' in history:\n",
        "        plt.figure()\n",
        "        plt.plot(epochs_range, history['train_acc'], label='Train Accuracy')\n",
        "        plt.plot(epochs_range, history['test_acc'], label='Test Accuracy')\n",
        "        plt.title(\"Bioclinical BERT Accuracy Evolution\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Accuracy\")\n",
        "        plt.legend()\n",
        "        plt.savefig(os.path.join(save_dir, \"accuracy_evolution_biobert.png\"))\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "    else:\n",
        "        print(\"Accuracy metrics not found in history, skipping accuracy plot.\")\n"
      ],
      "metadata": {
        "id": "tTXIvQSS7esg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, roc_auc_score, classification_report\n",
        "import numpy as np\n",
        "import os\n",
        "import warnings\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "\n",
        "\n",
        "warnings.simplefilter(\"ignore\", category=UndefinedMetricWarning)\n",
        "\n",
        "def train_eval_biobert(model, train_loader, test_loader, label_names=None, epochs=5, lr=2e-5, save_path=\"best_model.pt\"):\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    total_steps = len(train_loader) * epochs\n",
        "    warmup_steps = int(0.1 * total_steps)  # 10% pași warmup\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
        "\n",
        "    model.to(device)\n",
        "    best_f1 = 0\n",
        "    history = {\n",
        "        'train_loss': [], 'train_f1': [], 'train_acc': [],\n",
        "        'test_loss': [], 'test_f1': [], 'test_acc': [],\n",
        "        'test_f1_per_class': [], 'test_auc_per_class': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        y_true_train = []\n",
        "        y_pred_train = []\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}\"):\n",
        "            reports = batch['report']\n",
        "            labels = batch['target_regions'].to(device)\n",
        "\n",
        "            encodings = tokenize_reports(reports, tokenizer)\n",
        "            input_ids = encodings['input_ids'].to(device)\n",
        "            attention_mask = encodings['attention_mask'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            preds = (torch.sigmoid(outputs) > 0.5).int().cpu()\n",
        "            y_true_train.append(labels.cpu())\n",
        "            y_pred_train.append(preds)\n",
        "\n",
        "        y_true_train = torch.cat(y_true_train).numpy()\n",
        "        y_pred_train = torch.cat(y_pred_train).numpy()\n",
        "\n",
        "        f1_train = f1_score(y_true_train, y_pred_train, average='macro')\n",
        "        acc_train = (y_true_train == y_pred_train).mean()  # calcul acuratețe pe toate clasele și exemplele\n",
        "\n",
        "        history['train_loss'].append(train_loss / len(train_loader))\n",
        "        history['train_f1'].append(f1_train)\n",
        "        history['train_acc'].append(acc_train)\n",
        "\n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        y_true_test = []\n",
        "        y_pred_test = []\n",
        "        y_proba_test = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(test_loader, desc=\"Test\"):\n",
        "                reports = batch['report']\n",
        "                labels = batch['target_regions'].to(device)\n",
        "\n",
        "                encodings = tokenize_reports(reports, tokenizer)\n",
        "                input_ids = encodings['input_ids'].to(device)\n",
        "                attention_mask = encodings['attention_mask'].to(device)\n",
        "\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                test_loss += loss.item()\n",
        "\n",
        "                probs = torch.sigmoid(outputs).cpu()\n",
        "                preds = (probs > 0.5).int()\n",
        "\n",
        "                y_true_test.append(labels.cpu())\n",
        "                y_pred_test.append(preds)\n",
        "                y_proba_test.append(probs)\n",
        "\n",
        "        y_true_test = torch.cat(y_true_test).numpy()\n",
        "        y_pred_test = torch.cat(y_pred_test).numpy()\n",
        "        y_proba_test = torch.cat(y_proba_test).numpy()\n",
        "\n",
        "        f1_test = f1_score(y_true_test, y_pred_test, average='macro')\n",
        "        acc_test = (y_true_test == y_pred_test).mean()\n",
        "\n",
        "        f1_per_class = f1_score(y_true_test, y_pred_test, average=None).tolist()\n",
        "\n",
        "        try:\n",
        "            auc_per_class = roc_auc_score(y_true_test, y_proba_test, average=None).tolist()\n",
        "        except ValueError:\n",
        "            auc_per_class = [float('nan')] * y_true_test.shape[1]\n",
        "\n",
        "        history['test_loss'].append(test_loss / len(test_loader))\n",
        "        history['test_f1'].append(f1_test)\n",
        "        history['test_acc'].append(acc_test)\n",
        "        history['test_f1_per_class'].append(f1_per_class)\n",
        "        history['test_auc_per_class'].append(auc_per_class)\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}:\")\n",
        "        print(f\"Train Loss: {history['train_loss'][-1]:.4f}\")\n",
        "        print(f\"Test Loss:  {history['test_loss'][-1]:.4f}\")\n",
        "        print(f\"Train F1 (macro): {f1_train:.4f}\")\n",
        "        print(f\"Train Accuracy:   {acc_train:.4f}\")\n",
        "        print(f\"Test F1 (macro):  {f1_test:.4f}\")\n",
        "        print(f\"Test Accuracy:    {acc_test:.4f}\")\n",
        "        print(\"F1 per class:\")\n",
        "        if label_names:\n",
        "            for i, name in enumerate(label_names):\n",
        "                print(f\"  {name}: {f1_per_class[i]:.4f} | AUC: {auc_per_class[i]:.4f}\")\n",
        "        else:\n",
        "            for i, f1c in enumerate(f1_per_class):\n",
        "                print(f\"  Class {i}: F1={f1c:.4f}, AUC={auc_per_class[i]:.4f}\")\n",
        "\n",
        "        if f1_test > best_f1:\n",
        "            best_f1 = f1_test\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"✅ Saved new best model with F1={f1_test:.4f} to {save_path}\")\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = BioBERTMultiLabelClassifier(\"emilyalsentzer/Bio_ClinicalBERT\", num_labels=len(dataset.labels))\n",
        "\n",
        "label_names = dataset.labels  # de ex: ['frontal', 'temporal', 'parietal', ...]\n",
        "history = train_eval_biobert(\n",
        "    model,\n",
        "    train_loader,\n",
        "    test_loader,\n",
        "    label_names=label_names,\n",
        "    epochs=20,\n",
        "    save_path=\"/content/drive/MyDrive/PKG - MU-Glioma-Post/Models/BioClinicalBert/best.pt\"\n",
        ")\n",
        "\n",
        "plot_and_save_history(history)"
      ],
      "metadata": {
        "id": "pDUlKum57f74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import segmentation_models_pytorch as smp\n",
        "import torch.nn as nn\n",
        "\n",
        "class UNetTumorSegmentation(nn.Module):\n",
        "    def __init__(self, encoder_name='resnet34', pretrained=True):\n",
        "        super().__init__()\n",
        "        self.model = smp.Unet(\n",
        "            encoder_name=encoder_name,\n",
        "            encoder_weights='imagenet' if pretrained else None,\n",
        "            in_channels=2,  # mri + regions\n",
        "            classes=1,\n",
        "            activation=None  # BCEWithLogitsLoss expects raw logits\n",
        "        )\n",
        "\n",
        "    def forward(self, mri, regions):\n",
        "        x = torch.cat([mri, regions], dim=1)  # concatenate along channel dim\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "oPsYFRhn7g6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import segmentation_models_pytorch as smp\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import jaccard_score\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Model U-Net cu 2 input channels (MRI + Regions), 1 output channel\n",
        "model = smp.Unet(\n",
        "    encoder_name=\"resnet34\",\n",
        "    encoder_weights=\"imagenet\",\n",
        "    in_channels=2,\n",
        "    classes=1,\n",
        ").to(device)\n",
        "\n",
        "# Loss + optimizer + scheduler\n",
        "criterion = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.0, total_iters=50)\n",
        "\n",
        "def calc_metrics(y_true, y_pred):\n",
        "    \"\"\" Calculează Dice, IoU, Accuracy pe tensori numpy binarizați \"\"\"\n",
        "    y_true = y_true.flatten()\n",
        "    y_pred = y_pred.flatten()\n",
        "    dice = 2 * (y_true * y_pred).sum() / (y_true.sum() + y_pred.sum() + 1e-8)\n",
        "    iou = jaccard_score(y_true, y_pred, average='binary')\n",
        "    acc = (y_true == y_pred).mean()\n",
        "    return dice, iou, acc\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss, total_dice, total_iou, total_acc = 0, 0, 0, 0\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Input: concatenăm MRI și Regions pe canal\n",
        "        x1 = batch['mri'].float().unsqueeze(1).to(device)  # [B,1,H,W]\n",
        "        x2 = batch['regions'].float().unsqueeze(1).to(device)  # [B,1,H,W]\n",
        "        x = torch.cat([x1, x2], dim=1)  # [B,2,H,W]\n",
        "\n",
        "        y = batch['tumor'].float().unsqueeze(1).to(device)  # [B,1,H,W]\n",
        "\n",
        "        preds = model(x)\n",
        "\n",
        "        loss = criterion(preds, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # preds: logits -> sigmoid -> binarizare prag 0.5\n",
        "        preds_prob = torch.sigmoid(preds).detach().cpu().numpy() > 0.5\n",
        "        y_np = y.detach().cpu().numpy() > 0.5\n",
        "\n",
        "        dice, iou, acc = calc_metrics(y_np, preds_prob)\n",
        "\n",
        "        total_dice += dice\n",
        "        total_iou += iou\n",
        "        total_acc += acc\n",
        "\n",
        "    n = len(dataloader)\n",
        "    return total_loss / n, total_dice / n, total_iou / n, total_acc / n\n",
        "\n",
        "def eval_one_epoch(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    total_loss, total_dice, total_iou, total_acc = 0, 0, 0, 0\n",
        "    last_batch_imgs = None  # pentru a salva imagini de afișat\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            x1 = batch['mri'].float().unsqueeze(1).to(device)\n",
        "            x2 = batch['regions'].float().unsqueeze(1).to(device)\n",
        "            x = torch.cat([x1, x2], dim=1)\n",
        "\n",
        "            y = batch['tumor'].float().unsqueeze(1).to(device)\n",
        "\n",
        "            preds = model(x)\n",
        "            loss = criterion(preds, y)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds_prob = torch.sigmoid(preds).cpu().numpy() > 0.5\n",
        "            y_np = y.cpu().numpy() > 0.5\n",
        "\n",
        "            dice, iou, acc = calc_metrics(y_np, preds_prob)\n",
        "            total_dice += dice\n",
        "            total_iou += iou\n",
        "            total_acc += acc\n",
        "\n",
        "            # Salvăm batch-ul curent pentru afișare (ultimul batch)\n",
        "            last_batch_imgs = {\n",
        "                'mri': x1.cpu().numpy(),\n",
        "                'regions': x2.cpu().numpy(),\n",
        "                'y_true': y.cpu().numpy(),\n",
        "                'y_pred': preds_prob\n",
        "            }\n",
        "\n",
        "    n = len(dataloader)\n",
        "    return total_loss / n, total_dice / n, total_iou / n, total_acc / n, last_batch_imgs\n",
        "\n",
        "def plot_prediction(imgs):\n",
        "    # imgs conține: mri, regions, y_true, y_pred; toate în format numpy\n",
        "    mri = imgs['mri'][0,0]        # [B,1,H,W], selectăm prima imagine și canalul 0\n",
        "    regions = imgs['regions'][0,0]\n",
        "    y_true = imgs['y_true'][0,0]\n",
        "    y_pred = imgs['y_pred'][0,0]\n",
        "\n",
        "    fig, axs = plt.subplots(1,4, figsize=(20,5))\n",
        "    axs[0].imshow(mri, cmap='gray')\n",
        "    axs[0].set_title(\"MRI\")\n",
        "    axs[1].imshow(regions, cmap='gray')\n",
        "    axs[1].set_title(\"Regions\")\n",
        "    axs[2].imshow(y_true, cmap='gray')\n",
        "    axs[2].set_title(\"Ground Truth Mask\")\n",
        "    axs[3].imshow(y_pred, cmap='gray')\n",
        "    axs[3].set_title(\"Predicted Mask\")\n",
        "    for ax in axs:\n",
        "        ax.axis('off')\n",
        "    plt.savefig(\"/content/drive/MyDrive/PKG - MU-Glioma-Post/Graphs/sample_prediction_UNet.png\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def plot_metrics(train_metrics, val_metrics, save_path):\n",
        "    epochs = range(1, len(train_metrics['loss']) + 1)\n",
        "    plt.figure(figsize=(16, 12))\n",
        "\n",
        "    plt.subplot(2,2,1)\n",
        "    plt.plot(epochs, train_metrics['loss'], label='Train Loss')\n",
        "    plt.plot(epochs, val_metrics['loss'], label='Val Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Loss')\n",
        "\n",
        "    plt.subplot(2,2,2)\n",
        "    plt.plot(epochs, train_metrics['dice'], label='Train Dice')\n",
        "    plt.plot(epochs, val_metrics['dice'], label='Val Dice')\n",
        "    plt.legend()\n",
        "    plt.title('Dice Score')\n",
        "\n",
        "    plt.subplot(2,2,3)\n",
        "    plt.plot(epochs, train_metrics['iou'], label='Train IoU')\n",
        "    plt.plot(epochs, val_metrics['iou'], label='Val IoU')\n",
        "    plt.legend()\n",
        "    plt.title('IoU')\n",
        "\n",
        "    plt.subplot(2,2,4)\n",
        "    plt.plot(epochs, train_metrics['acc'], label='Train Accuracy')\n",
        "    plt.plot(epochs, val_metrics['acc'], label='Val Accuracy')\n",
        "    plt.legend()\n",
        "    plt.title('Accuracy')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    plt.show()\n",
        "\n",
        "num_epochs = 50\n",
        "best_val_iou = 0\n",
        "train_metrics = {'loss': [], 'dice': [], 'iou': [], 'acc': []}\n",
        "val_metrics = {'loss': [], 'dice': [], 'iou': [], 'acc': []}\n",
        "last_val_batch_imgs = None  # aici salvăm imaginile pentru ultima evaluare\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train_loss, train_dice, train_iou, train_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
        "    val_loss, val_dice, val_iou, val_acc, val_imgs = eval_one_epoch(model, test_loader, criterion)\n",
        "    scheduler.step()\n",
        "\n",
        "    train_metrics['loss'].append(train_loss)\n",
        "    train_metrics['dice'].append(train_dice)\n",
        "    train_metrics['iou'].append(train_iou)\n",
        "    train_metrics['acc'].append(train_acc)\n",
        "\n",
        "    val_metrics['loss'].append(val_loss)\n",
        "    val_metrics['dice'].append(val_dice)\n",
        "    val_metrics['iou'].append(val_iou)\n",
        "    val_metrics['acc'].append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch}/{num_epochs}\")\n",
        "    print(f\" Train   - Loss: {train_loss:.4f}, Dice: {train_dice:.4f}, IoU: {train_iou:.4f}, Acc: {train_acc:.4f}\")\n",
        "    print(f\" Valid   - Loss: {val_loss:.4f}, Dice: {val_dice:.4f}, IoU: {val_iou:.4f}, Acc: {val_acc:.4f}\")\n",
        "\n",
        "    if val_iou > best_val_iou:\n",
        "        best_val_iou = val_iou\n",
        "        torch.save(model.state_dict(), \"/content/drive/MyDrive/PKG - MU-Glioma-Post/Models/UNet/best_unet_model.pth\")\n",
        "        print(\"  Saved best model!\")\n",
        "\n",
        "    # Salvăm imaginile din ultimul batch de validare la ultima epocă\n",
        "    if epoch >= num_epochs - 5:\n",
        "        last_val_batch_imgs = val_imgs\n",
        "        plot_prediction(last_val_batch_imgs)\n",
        "\n",
        "# Afișăm predicția vs adevărul pentru ultimul batch\n",
        "if last_val_batch_imgs is not None:\n",
        "    plot_prediction(last_val_batch_imgs)\n",
        "\n",
        "# Salvează graficele metricilor\n",
        "plot_metrics(train_metrics, val_metrics, \"/content/drive/MyDrive/PKG - MU-Glioma-Post/Graphs/training_metrics_UNet.png\")"
      ],
      "metadata": {
        "id": "4tqJ6r1X7hoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import segmentation_models_pytorch as smp\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import jaccard_score\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "bert_model_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "bert_num_labels = len(dataset.labels)\n",
        "bert_model = BioBERTMultiLabelClassifier(bert_model_name, bert_num_labels)\n",
        "bert_state_dict = torch.load(\"/content/drive/MyDrive/PKG - MU-Glioma-Post/Models/BioClinicalBert/best.pt\")\n",
        "bert_model.load_state_dict(bert_state_dict)\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
        "\n",
        "unet_model = smp.Unet(\n",
        "    encoder_name=\"resnet34\",\n",
        "    encoder_weights=None,\n",
        "    in_channels=2,\n",
        "    classes=1,\n",
        ")\n",
        "unet_state_dict = torch.load(\"/content/drive/MyDrive/PKG - MU-Glioma-Post/Models/UNet/best_unet_model.pth\")\n",
        "unet_model.load_state_dict(unet_state_dict)\n",
        "\n",
        "train_loader_v2 = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=glioma_collate_fn)\n",
        "\n",
        "class ClipModel(nn.Module):\n",
        "    def __init__(self, bert_model, bert_tokenizer, unet_model, embed_dim=512):\n",
        "        super().__init__()\n",
        "        self.bert_model = bert_model\n",
        "        self.unet_model = unet_model\n",
        "        self.bert_tokenizer = bert_tokenizer\n",
        "        self.text_projection = nn.Linear(768, embed_dim)\n",
        "        self.image_projection = nn.Linear(512, embed_dim)\n",
        "\n",
        "        self.log_sigma_clip = nn.Parameter(torch.tensor(0.0))\n",
        "        self.log_sigma_seg = nn.Parameter(torch.tensor(0.0))\n",
        "\n",
        "    def forward(self, text, mri, regions):\n",
        "        encodings = tokenize_reports(text, self.bert_tokenizer)\n",
        "        input_ids = encodings['input_ids'].to(device)\n",
        "        attention_mask = encodings['attention_mask'].to(device)\n",
        "\n",
        "        # Get Text embeddings\n",
        "        outputs = self.bert_model.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        text_embeddings = self.text_projection(pooled_output)\n",
        "        text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)\n",
        "        # print(f\"Text embeddings size: {text_embeddings.shape}\")\n",
        "\n",
        "        # Get Image embeddings\n",
        "        x1 = mri.float().unsqueeze(1).to(device)\n",
        "        x2 = regions.float().unsqueeze(1).to(device)\n",
        "        x = torch.cat([x1, x2], dim=1)\n",
        "        image_embeddings = self.unet_model.encoder(x)[-1]\n",
        "        image_embeddings = image_embeddings.mean(dim=[2, 3])\n",
        "        image_embeddings = self.image_projection(image_embeddings)\n",
        "        image_embeddings = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)\n",
        "        # print(f\"Image embeddings size: {image_embeddings.shape}\")\n",
        "\n",
        "        segmentation_result = self.unet_model(x)\n",
        "        # print(f\"Segmentation result size: {segmentation_result.shape}\")\n",
        "\n",
        "        return text_embeddings, image_embeddings, segmentation_result\n",
        "\n",
        "    def clip_contrastive_loss(self, text_embeddings, image_embeddings, temperature=0.07):\n",
        "      logits = torch.matmul(text_embeddings, image_embeddings.T) * torch.exp(torch.tensor(temperature))\n",
        "\n",
        "      labels = torch.arange(text_embeddings.shape[0]).to(device)\n",
        "      loss_i = F.cross_entropy(logits, labels)\n",
        "      loss_t = F.cross_entropy(logits.T, labels)\n",
        "\n",
        "\n",
        "      loss = (loss_i + loss_t) / 2\n",
        "      return loss\n",
        "\n",
        "    def segmentation_loss(self, segmentation_result, target, criterion):\n",
        "      target = target.float().unsqueeze(1).to(device)\n",
        "      loss = criterion(segmentation_result, target)\n",
        "      return loss\n",
        "\n",
        "    def combined_loss(self, text_embeddings, image_embeddings, segmentation_result, target, criterion, temperature=0.07):\n",
        "      clip_loss = self.clip_contrastive_loss(text_embeddings, image_embeddings, temperature)\n",
        "      seg_loss = self.segmentation_loss(segmentation_result, target, criterion)\n",
        "      loss = (\n",
        "            (1.0 / (2.0 * torch.exp(self.log_sigma_clip) ** 2)) * clip_loss +\n",
        "            (1.0 / (2.0 * torch.exp(self.log_sigma_seg) ** 2)) * seg_loss +\n",
        "            self.log_sigma_clip + self.log_sigma_seg\n",
        "        )\n",
        "      return loss\n",
        "\n",
        "\n",
        "clipModel = ClipModel(bert_model, bert_tokenizer, unet_model).to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "sVma_pO-7i_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_metrics(y_true, y_pred):\n",
        "    \"\"\" Calculează Dice, IoU, Accuracy pe tensori numpy binarizați \"\"\"\n",
        "    y_true = y_true.flatten()\n",
        "    y_pred = y_pred.flatten()\n",
        "    dice = 2 * (y_true * y_pred).sum() / (y_true.sum() + y_pred.sum() + 1e-8)\n",
        "    iou = jaccard_score(y_true, y_pred, average='binary')\n",
        "    acc = (y_true == y_pred).mean()\n",
        "    return dice, iou, acc\n",
        "\n",
        "\n",
        "def plot_prediction(imgs):\n",
        "    # imgs conține: mri, regions, y_true, y_pred; toate în format numpy\n",
        "    mri = imgs['mri'][0,0]        # [B,1,H,W], selectăm prima imagine și canalul 0\n",
        "    regions = imgs['regions'][0,0]\n",
        "    y_true = imgs['y_true'][0,0]\n",
        "    y_pred = imgs['y_pred'][0,0]\n",
        "\n",
        "    fig, axs = plt.subplots(1,4, figsize=(20,5))\n",
        "    axs[0].imshow(mri, cmap='gray')\n",
        "    axs[0].set_title(\"MRI\")\n",
        "    axs[1].imshow(regions, cmap='gray')\n",
        "    axs[1].set_title(\"Regions\")\n",
        "    axs[2].imshow(y_true, cmap='gray')\n",
        "    axs[2].set_title(\"Ground Truth Mask\")\n",
        "    axs[3].imshow(y_pred, cmap='gray')\n",
        "    axs[3].set_title(\"Predicted Mask\")\n",
        "    for ax in axs:\n",
        "        ax.axis('off')\n",
        "    plt.savefig(\"/content/drive/MyDrive/PKG - MU-Glioma-Post/Graphs/sample_prediction_CLIP.png\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def plot_metrics(train_metrics, val_metrics, save_path):\n",
        "    epochs = range(1, len(train_metrics['loss']) + 1)\n",
        "    plt.figure(figsize=(16, 12))\n",
        "\n",
        "    plt.subplot(2,2,1)\n",
        "    plt.plot(epochs, train_metrics['loss'], label='Train Loss')\n",
        "    plt.plot(epochs, val_metrics['loss'], label='Val Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Loss')\n",
        "\n",
        "    plt.subplot(2,2,2)\n",
        "    plt.plot(epochs, train_metrics['dice'], label='Train Dice')\n",
        "    plt.plot(epochs, val_metrics['dice'], label='Val Dice')\n",
        "    plt.legend()\n",
        "    plt.title('Dice Score')\n",
        "\n",
        "    plt.subplot(2,2,3)\n",
        "    plt.plot(epochs, train_metrics['iou'], label='Train IoU')\n",
        "    plt.plot(epochs, val_metrics['iou'], label='Val IoU')\n",
        "    plt.legend()\n",
        "    plt.title('IoU')\n",
        "\n",
        "    plt.subplot(2,2,4)\n",
        "    plt.plot(epochs, train_metrics['acc'], label='Train Accuracy')\n",
        "    plt.plot(epochs, val_metrics['acc'], label='Val Accuracy')\n",
        "    plt.legend()\n",
        "    plt.title('Accuracy')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    plt.show()\n",
        "\n",
        "def train_one_epoch_clip(model, dataloader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss, total_dice, total_iou, total_acc = 0, 0, 0, 0\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Input: concatenăm MRI și Regions pe canal\n",
        "        mri = batch['mri']\n",
        "        regions = batch['regions']\n",
        "\n",
        "        tumor = batch['tumor']\n",
        "        text = batch['report']\n",
        "\n",
        "        text_embeddings, image_embeddings, segmentation_result = model(text, mri, regions)\n",
        "\n",
        "        loss = model.combined_loss(text_embeddings, image_embeddings, segmentation_result, tumor, criterion)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # preds: logits -> sigmoid -> binarizare prag 0.5\n",
        "        preds_prob = torch.sigmoid(segmentation_result).detach().cpu().numpy() > 0.5\n",
        "        y_np = tumor.float().unsqueeze(1).to(device).detach().cpu().numpy() > 0.5\n",
        "\n",
        "        dice, iou, acc = calc_metrics(y_np, preds_prob)\n",
        "\n",
        "        total_dice += dice\n",
        "        total_iou += iou\n",
        "        total_acc += acc\n",
        "\n",
        "    n = len(dataloader)\n",
        "    return total_loss / n, total_dice / n, total_iou / n, total_acc / n\n",
        "\n",
        "\n",
        "def eval_one_epoch_clip(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    total_loss, total_dice, total_iou, total_acc = 0, 0, 0, 0\n",
        "    last_batch_imgs = None  # pentru a salva imagini de afișat\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            mri = batch['mri']\n",
        "            regions = batch['regions']\n",
        "\n",
        "            tumor = batch['tumor']\n",
        "            text = batch['report']\n",
        "\n",
        "            text_embeddings, image_embeddings, segmentation_result = model(text, mri, regions)\n",
        "\n",
        "            loss = model.combined_loss(text_embeddings, image_embeddings, segmentation_result, tumor, criterion)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds_prob = torch.sigmoid(segmentation_result).cpu().numpy() > 0.5\n",
        "            y_np = tumor.float().unsqueeze(1).to(device).detach().cpu().numpy() > 0.5\n",
        "\n",
        "            dice, iou, acc = calc_metrics(y_np, preds_prob)\n",
        "            total_dice += dice\n",
        "            total_iou += iou\n",
        "            total_acc += acc\n",
        "\n",
        "            # Salvăm batch-ul curent pentru afișare (ultimul batch)\n",
        "            last_batch_imgs = {\n",
        "                'mri': mri.float().unsqueeze(1).to(device).cpu().numpy(),\n",
        "                'regions': regions.float().unsqueeze(1).to(device).cpu().numpy(),\n",
        "                'y_true': tumor.float().unsqueeze(1).to(device).cpu().numpy(),\n",
        "                'y_pred': preds_prob\n",
        "            }\n",
        "\n",
        "    n = len(dataloader)\n",
        "    return total_loss / n, total_dice / n, total_iou / n, total_acc / n, last_batch_imgs\n",
        "\n",
        "model = clipModel\n",
        "\n",
        "criterion = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.0, total_iters=50)\n",
        "\n",
        "num_epochs = 50\n",
        "best_val_iou = 0\n",
        "train_metrics = {'loss': [], 'dice': [], 'iou': [], 'acc': []}\n",
        "val_metrics = {'loss': [], 'dice': [], 'iou': [], 'acc': []}\n",
        "last_val_batch_imgs = None  # aici salvăm imaginile pentru ultima evaluare\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train_loss, train_dice, train_iou, train_acc = train_one_epoch_clip(model, train_loader, optimizer, criterion)\n",
        "    val_loss, val_dice, val_iou, val_acc, val_imgs = eval_one_epoch_clip(model, test_loader, criterion)\n",
        "    scheduler.step()\n",
        "\n",
        "    train_metrics['loss'].append(train_loss)\n",
        "    train_metrics['dice'].append(train_dice)\n",
        "    train_metrics['iou'].append(train_iou)\n",
        "    train_metrics['acc'].append(train_acc)\n",
        "\n",
        "    val_metrics['loss'].append(val_loss)\n",
        "    val_metrics['dice'].append(val_dice)\n",
        "    val_metrics['iou'].append(val_iou)\n",
        "    val_metrics['acc'].append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch}/{num_epochs}\")\n",
        "    print(f\" Train   - Loss: {train_loss:.4f}, Dice: {train_dice:.4f}, IoU: {train_iou:.4f}, Acc: {train_acc:.4f}\")\n",
        "    print(f\" Valid   - Loss: {val_loss:.4f}, Dice: {val_dice:.4f}, IoU: {val_iou:.4f}, Acc: {val_acc:.4f}\")\n",
        "\n",
        "    if val_iou > best_val_iou:\n",
        "        best_val_iou = val_iou\n",
        "        torch.save(model.state_dict(), \"/content/drive/MyDrive/PKG - MU-Glioma-Post/Models/CLIP_Based/best_clip_model.pth\")\n",
        "        print(\"  Saved best model!\")\n",
        "\n",
        "    # Salvăm imaginile din ultimul batch de validare la ultima epocă\n",
        "    if epoch >= num_epochs - 5:\n",
        "      last_val_batch_imgs = val_imgs\n",
        "      plot_prediction(last_val_batch_imgs)\n",
        "\n",
        "# Afișăm predicția vs adevărul pentru ultimul batch\n",
        "if last_val_batch_imgs is not None:\n",
        "    plot_prediction(last_val_batch_imgs)\n",
        "\n",
        "# Salvează graficele metricilor\n",
        "plot_metrics(train_metrics, val_metrics, \"/content/drive/MyDrive/PKG - MU-Glioma-Post/Graphs/training_metrics_CLIP.png\")"
      ],
      "metadata": {
        "id": "lNr1Mq7W7kDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>Libraries<h2>**"
      ],
      "metadata": {
        "id": "JghFp_TJEu2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segmentation_models_pytorch"
      ],
      "metadata": {
        "id": "PnOlsFBMkQYN",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **M1 - Data Preprocessing**"
      ],
      "metadata": {
        "id": "RYfjGgr9E0Su"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load all files\n",
        "mri = np.load(\"/content/PatientID_0036_mri.npy\")\n",
        "tumor = np.load(\"/content/PatientID_0036_tumor.npy\")\n",
        "atlas = np.load(\"/content/PatientID_0036_regions.npy\")\n",
        "\n",
        "# Print shapes\n",
        "print(\"MRI shape:\", mri.shape)\n",
        "print(\"Tumor shape:\", tumor.shape)\n",
        "print(\"Atlas shape:\", atlas.shape)\n",
        "\n",
        "# Plot them together\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# MRI\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(mri, cmap=\"gray\")\n",
        "plt.title(\"MRI\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "# Tumor mask\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(tumor, cmap=\"hot\")\n",
        "plt.title(\"Tumor mask\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "# Atlas\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(atlas, cmap=\"nipy_spectral\")\n",
        "plt.title(\"Atlas regions\")\n",
        "plt.axis(\"off\")\n",
        "plt.colorbar()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KJRjNQW9FGwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***MRI (left image)***\n",
        "\n",
        "- This is the actual brain scan.\n",
        "\n",
        "- It shows the anatomical structures and tissue appearance.\n",
        "\n",
        "- The model uses this image as the input.\n",
        "\n",
        "--------------------------------------------\n",
        "***Tumor Mask (middle image)***\n",
        "\n",
        "This is the ground-truth annotation.\n",
        "\n",
        "- Yellow = whole tumor region\n",
        "\n",
        "- Red = core or high-confidence tumor area\n",
        "\n",
        "- Black = background\n",
        "\n",
        "The model tries to predict this mask.\n",
        "\n",
        "--------------------------------------------\n",
        "***Atlas Regions (right image)***\n",
        "\n",
        "- This is a brain anatomical atlas aligned to the MRI.\n",
        "\n",
        "- Each color corresponds to a different anatomical region.\n",
        "\n",
        "- It gives the model context about where in the brain each pixel is located.\n"
      ],
      "metadata": {
        "id": "Zn2kMfdIK4et"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/Preprocessed-Data.zip -d /content/Preprocessed-Data"
      ],
      "metadata": {
        "id": "KFOZ6KnBLrIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "DATA_ROOT = \"/content/Preprocessed-Data\"\n",
        "USE_ATLAS = True\n",
        "\n",
        "rows = []\n",
        "\n",
        "# 1. Iterate over all patient folders in Preprocessed-Data\n",
        "for pid in sorted(os.listdir(DATA_ROOT)):\n",
        "    patient_dir = os.path.join(DATA_ROOT, pid)\n",
        "    if not os.path.isdir(patient_dir):\n",
        "        continue\n",
        "\n",
        "    tumor_path = os.path.join(patient_dir, f\"{pid}_tumor.npy\")\n",
        "    atlas_path = os.path.join(patient_dir, f\"{pid}_regions.npy\")\n",
        "\n",
        "    # Skip if tumor mask is missing\n",
        "    if not os.path.isfile(tumor_path):\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # Load tumor mask\n",
        "        tumor = np.load(tumor_path).astype(np.float32)\n",
        "        mask = tumor > 0.5\n",
        "\n",
        "        # Tumor area and presence\n",
        "        area = float(mask.sum())\n",
        "        has_tumor = 1 if area >= 1 else 0\n",
        "\n",
        "        # Dominant atlas region (inside tumor) if atlas exists\n",
        "        dom_region = -1\n",
        "        if USE_ATLAS and os.path.isfile(atlas_path) and mask.any():\n",
        "            atlas = np.load(atlas_path).astype(np.int32)\n",
        "            vals, counts = np.unique(atlas[mask], return_counts=True)\n",
        "            if len(vals) > 0:\n",
        "                dom_region = int(vals[np.argmax(counts)])\n",
        "\n",
        "        rows.append({\n",
        "            \"pid\": pid,\n",
        "            \"has_tumor\": has_tumor,\n",
        "            \"area\": area,\n",
        "            \"dom\": dom_region,\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Failed for {pid}: {e}\")\n",
        "\n",
        "# 2. Build DataFrame with metadata\n",
        "meta = pd.DataFrame(rows)\n",
        "print(\"Metadata head:\")\n",
        "print(meta.head())\n",
        "print(\"\\nColumns:\", meta.columns.tolist())\n",
        "print(\"\\nNumber of patients:\", len(meta))\n",
        "\n",
        "# 3. Compute size_bin (tumor size bins) like in your partitioning\n",
        "meta[\"size_bin\"] = 0\n",
        "mask_has_tumor = meta[\"has_tumor\"] == 1\n",
        "\n",
        "if mask_has_tumor.any():\n",
        "    areas = meta.loc[mask_has_tumor, \"area\"].values\n",
        "    qs = np.quantile(areas, np.linspace(0, 1, 4))\n",
        "    qs = np.unique(qs)\n",
        "    if len(qs) > 2:\n",
        "        bins = np.digitize(areas, qs[1:-1], right=True)\n",
        "    else:\n",
        "        bins = np.zeros_like(areas, dtype=int)\n",
        "    meta.loc[mask_has_tumor, \"size_bin\"] = bins.astype(int)\n",
        "\n",
        "print(\"\\nSize bin value counts:\")\n",
        "print(meta[\"size_bin\"].value_counts().sort_index())\n",
        "\n",
        "# 4. Plot the distributions used in data partitioning\n",
        "\n",
        "fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "# Tumor size bins\n",
        "meta[\"size_bin\"].value_counts().sort_index().plot(kind=\"bar\", ax=axs[0, 0])\n",
        "axs[0, 0].set_title(\"Tumor size bins\")\n",
        "axs[0, 0].set_xlabel(\"size_bin\")\n",
        "axs[0, 0].set_ylabel(\"Number of patients\")\n",
        "\n",
        "# Has tumor (0/1)\n",
        "meta[\"has_tumor\"].value_counts().sort_index().plot(kind=\"bar\", ax=axs[0, 1])\n",
        "axs[0, 1].set_title(\"Has tumor\")\n",
        "axs[0, 1].set_xlabel(\"has_tumor (0 = no, 1 = yes)\")\n",
        "axs[0, 1].set_ylabel(\"Number of patients\")\n",
        "\n",
        "# Tumor area histogram\n",
        "meta[\"area\"].hist(bins=30, ax=axs[1, 0])\n",
        "axs[1, 0].set_title(\"Tumor area (voxel count)\")\n",
        "axs[1, 0].set_xlabel(\"area\")\n",
        "axs[1, 0].set_ylabel(\"Number of patients\")\n",
        "\n",
        "# Dominant atlas regions (top 10)\n",
        "meta[\"dom\"].value_counts().head(10).plot(kind=\"bar\", ax=axs[1, 1])\n",
        "axs[1, 1].set_title(\"Top 10 dominant atlas regions\")\n",
        "axs[1, 1].set_xlabel(\"region label\")\n",
        "axs[1, 1].set_ylabel(\"Number of patients\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AsYhGMN5OMs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Tumor size bins***\n",
        "\n",
        "- These bins represent three tumor-size groups created using quantiles of the tumor voxel count.\n",
        "\n",
        "- They ensure that small, medium, and large tumors are balanced across the dataset and across client splits.\n",
        "\n",
        "------------------------------------\n",
        "***Tumor area (voxel count)***\n",
        "\n",
        "- This histogram shows the raw distribution of tumor sizes in the dataset.\n",
        "\n",
        "- It highlights the variability in tumor burden, from small lesions to very large ones.\n",
        "\n",
        "------------------------------------\n",
        "***Top dominant atlas regions***\n",
        "\n",
        "- Each tumor belongs mostly to one anatomical region in the brain atlas.\n",
        "\n",
        "- This plot shows which regions occur most frequently and helps guide stratification, since some regions are common while others are rare.\n"
      ],
      "metadata": {
        "id": "Rxd1FA6yPLhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the cleaned dataframe from pickle\n",
        "with open(\"cleaned_df.pkl\", \"rb\") as f:\n",
        "    df = pickle.load(f)\n",
        "\n",
        "# Print all column names\n",
        "print(\"Columns in cleaned_df:\")\n",
        "for c in df.columns:\n",
        "    print(\" -\", c)\n",
        "\n",
        "# Column names used for plotting\n",
        "col_sex  = \"Sex at Birth\"\n",
        "col_race = \"Race\"\n",
        "col_age  = \"Age at diagnosis\"\n",
        "col_diag = \"Primary Diagnosis\"\n",
        "\n",
        "# Create a 2x2 grid of subplots\n",
        "fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Pie chart: Sex at Birth\n",
        "df[col_sex].value_counts(dropna=False).plot(\n",
        "    kind=\"pie\", autopct='%1.1f%%', ax=axs[0,0], ylabel=\"\"\n",
        ")\n",
        "axs[0,0].set_title(\"Sex at Birth\")\n",
        "\n",
        "# Pie chart: Race\n",
        "df[col_race].value_counts(dropna=False).plot(\n",
        "    kind=\"pie\", autopct='%1.1f%%', ax=axs[0,1], ylabel=\"\"\n",
        ")\n",
        "axs[0,1].set_title(\"Race\")\n",
        "\n",
        "# Pie chart: Primary Diagnosis\n",
        "df[col_diag].value_counts(dropna=False).plot(\n",
        "    kind=\"pie\", autopct='%1.1f%%', ax=axs[1,0], ylabel=\"\"\n",
        ")\n",
        "axs[1,0].set_title(\"Primary Diagnosis\")\n",
        "\n",
        "# Create age bins and pie chart: Age at Diagnosis\n",
        "age_bins = pd.cut(df[col_age], bins=[0, 20, 40, 60, 80, 120])\n",
        "age_bins.value_counts().sort_index().plot(\n",
        "    kind=\"pie\", autopct='%1.1f%%', ax=axs[1,1], ylabel=\"\"\n",
        ")\n",
        "axs[1,1].set_title(\"Age at Diagnosis (binned)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NWJbDT_JSujU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>Data Partitioning<h1>**"
      ],
      "metadata": {
        "id": "8GZTx9vVNTPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>One client, all data<h2>"
      ],
      "metadata": {
        "id": "R9y8S3ShnIyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_prep_split.py\n",
        "import os, json, pickle, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Tuple\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Paths and global configuration\n",
        "DATA_ROOT = \"Preprocessed-Data\"\n",
        "METADATA_DF_PATH = \"cleaned_df.pkl\"\n",
        "OUT_BASE = \"AITDM\"\n",
        "CLIENT_DIR = os.path.join(OUT_BASE, \"client\")\n",
        "os.makedirs(CLIENT_DIR, exist_ok=True)\n",
        "\n",
        "USE_ATLAS = True\n",
        "N_CLIENTS = 1\n",
        "VAL_FRAC_PER_CLIENT = 0.2\n",
        "BATCH_SIZE = 8\n",
        "NUM_WORKERS = 2\n",
        "SEED = 42\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    \"\"\"Set all relevant random seeds for reproducibility.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(SEED)\n",
        "\n",
        "class ImageOnlyGliomaDataset(Dataset):\n",
        "    \"\"\"Dataset that loads MRI, tumor mask and optional atlas for each patient.\"\"\"\n",
        "\n",
        "    def __init__(self, metadata_df_path, data_root, use_atlas=True, exclude_ids=None, transform=None):\n",
        "        with open(metadata_df_path, \"rb\") as f:\n",
        "            df = pickle.load(f)\n",
        "\n",
        "        if exclude_ids is None:\n",
        "            exclude_ids = [\"PatientID_0191\"]\n",
        "\n",
        "        # Filter out excluded patient IDs\n",
        "        self.df = df[~df[\"Patient_ID\"].isin(exclude_ids)].reset_index(drop=True)\n",
        "        self.data_root = data_root\n",
        "        self.use_atlas = use_atlas\n",
        "        self.transform = transform\n",
        "\n",
        "        # Keep only patients for which all required .npy files exist\n",
        "        self.patient_ids = []\n",
        "        for pid in sorted(self.df[\"Patient_ID\"].tolist()):\n",
        "            mri_p = os.path.join(self.data_root, pid, f\"{pid}_mri.npy\")\n",
        "            tumor_p = os.path.join(self.data_root, pid, f\"{pid}_tumor.npy\")\n",
        "            if self.use_atlas:\n",
        "                reg_p = os.path.join(self.data_root, pid, f\"{pid}_regions.npy\")\n",
        "                if os.path.isfile(mri_p) and os.path.isfile(tumor_p) and os.path.isfile(reg_p):\n",
        "                    self.patient_ids.append(pid)\n",
        "            else:\n",
        "                if os.path.isfile(mri_p) and os.path.isfile(tumor_p):\n",
        "                    self.patient_ids.append(pid)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patient_ids)\n",
        "\n",
        "    @staticmethod\n",
        "    def _minmax(x):\n",
        "        \"\"\"Apply simple min-max normalization to a numpy array.\"\"\"\n",
        "        x = x.astype(np.float32)\n",
        "        mn, mx = np.min(x), np.max(x)\n",
        "        return (x - mn) / (mx - mn) if mx > mn else np.zeros_like(x, dtype=np.float32)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Load one patient sample (MRI, tumor mask, and optional atlas).\"\"\"\n",
        "        pid = self.patient_ids[idx]\n",
        "        base = os.path.join(self.data_root, pid)\n",
        "\n",
        "        # Load MRI and tumor mask\n",
        "        mri = np.load(os.path.join(base, f\"{pid}_mri.npy\")).astype(np.float32)\n",
        "        tumor = np.load(os.path.join(base, f\"{pid}_tumor.npy\")).astype(np.float32)\n",
        "\n",
        "        # Normalize MRI and binarize tumor mask\n",
        "        mri = self._minmax(mri)\n",
        "        tumor = (tumor > 0.5).astype(np.float32)\n",
        "\n",
        "        sample = {\"patient_id\": pid, \"mri\": mri, \"tumor\": tumor}\n",
        "\n",
        "        # Optionally load and normalize atlas regions\n",
        "        if self.use_atlas:\n",
        "            regions = np.load(os.path.join(base, f\"{pid}_regions.npy\")).astype(np.float32)\n",
        "            regions = self._minmax(regions)\n",
        "            sample[\"regions\"] = regions\n",
        "\n",
        "        return sample\n",
        "\n",
        "def image_only_collate_fn(batch, use_atlas=True):\n",
        "    \"\"\"\n",
        "    Custom collate function to build batched tensors:\n",
        "    x: [B, C, H, W], y: [B, 1, H, W], pid: list of patient IDs.\n",
        "    \"\"\"\n",
        "    mri = torch.stack([torch.tensor(it[\"mri\"]) for it in batch]).unsqueeze(1)\n",
        "    y = torch.stack([torch.tensor(it[\"tumor\"]) for it in batch]).unsqueeze(1)\n",
        "\n",
        "    if use_atlas:\n",
        "        regs = torch.stack([torch.tensor(it[\"regions\"]) for it in batch]).unsqueeze(1)\n",
        "        x = torch.cat([mri.float(), regs.float()], dim=1)\n",
        "    else:\n",
        "        x = mri.float()\n",
        "\n",
        "    pids = [it[\"patient_id\"] for it in batch]\n",
        "    return {\"x\": x, \"y\": y.float(), \"pid\": pids}\n",
        "\n",
        "def patient_meta(pid: str) -> Tuple[int, float, int]:\n",
        "    \"\"\"\n",
        "    Compute simple metadata for one patient:\n",
        "    - has_tumor: 0/1 depending on tumor area\n",
        "    - area: tumor voxel count\n",
        "    - dom_region: dominant atlas region inside tumor mask\n",
        "    \"\"\"\n",
        "    base = os.path.join(DATA_ROOT, pid)\n",
        "\n",
        "    tumor = np.load(os.path.join(base, f\"{pid}_tumor.npy\")).astype(np.float32)\n",
        "    mask = tumor > 0.5\n",
        "    area = float(mask.sum())\n",
        "    has_tumor = 1 if area >= 1 else 0\n",
        "\n",
        "    dom_region = -1\n",
        "    reg_path = os.path.join(base, f\"{pid}_regions.npy\")\n",
        "\n",
        "    # If atlas is present and tumor exists, find region with most tumor voxels\n",
        "    if USE_ATLAS and os.path.isfile(reg_path) and mask.any():\n",
        "        regs = np.load(reg_path).astype(np.int32)\n",
        "        vals, counts = np.unique(regs[mask], return_counts=True)\n",
        "        if len(vals) > 0:\n",
        "            dom_region = int(vals[np.argmax(counts)])\n",
        "\n",
        "    return has_tumor, area, dom_region\n",
        "\n",
        "def build_meta_for(dataset: ImageOnlyGliomaDataset) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Build a DataFrame with metadata for each patient:\n",
        "    - pid\n",
        "    - has_tumor\n",
        "    - area\n",
        "    - dom (dominant region)\n",
        "    - size_bin (tumor size bin)\n",
        "    - strat_label (full stratification label)\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    for pid in dataset.patient_ids:\n",
        "        try:\n",
        "            ht, area, dom = patient_meta(pid)\n",
        "            rows.append({\"pid\": pid, \"has_tumor\": ht, \"area\": area, \"dom\": dom})\n",
        "        except Exception:\n",
        "            # Skip patients that fail to load or compute\n",
        "            pass\n",
        "\n",
        "    meta = pd.DataFrame(rows)\n",
        "\n",
        "    # Default size bin = 0 (no tumor or very small)\n",
        "    meta[\"size_bin\"] = 0\n",
        "    m = meta[\"has_tumor\"] == 1\n",
        "\n",
        "    # For patients with tumor, compute quantile-based size bins\n",
        "    if m.any():\n",
        "        areas = meta.loc[m, \"area\"].values\n",
        "        qs = np.quantile(areas, np.linspace(0, 1, 4))\n",
        "        qs = np.unique(qs)\n",
        "        if len(qs) > 2:\n",
        "            bins = np.digitize(areas, qs[1:-1], right=True)\n",
        "        else:\n",
        "            bins = np.zeros_like(areas, dtype=int)\n",
        "        meta.loc[m, \"size_bin\"] = bins.astype(int)\n",
        "\n",
        "    # Full stratification label combining several attributes\n",
        "    meta[\"strat_label\"] = [\n",
        "        f\"{int(ht)}_{int(dom)}_{int(sb)}\"\n",
        "        for ht, dom, sb in zip(meta[\"has_tumor\"], meta[\"dom\"], meta[\"size_bin\"])\n",
        "    ]\n",
        "    return meta\n",
        "\n",
        "def build_label(meta_df: pd.DataFrame, level: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Build a stratification label at different granularity levels:\n",
        "    - \"full\": has_tumor + dom + size_bin\n",
        "    - \"ht_dom\": has_tumor + dom\n",
        "    - \"ht_size\": has_tumor + size_bin\n",
        "    - \"ht\": has_tumor only\n",
        "    \"\"\"\n",
        "    if level == \"full\":\n",
        "        return np.array([\n",
        "            f\"{int(ht)}_{int(dom)}_{int(sb)}\"\n",
        "            for ht, dom, sb in zip(meta_df[\"has_tumor\"], meta_df[\"dom\"], meta_df[\"size_bin\"])\n",
        "        ])\n",
        "    if level == \"ht_dom\":\n",
        "        return np.array([\n",
        "            f\"{int(ht)}_{int(dom)}\"\n",
        "            for ht, dom in zip(meta_df[\"has_tumor\"], meta_df[\"dom\"])\n",
        "        ])\n",
        "    if level == \"ht_size\":\n",
        "        return np.array([\n",
        "            f\"{int(ht)}_{int(sb)}\"\n",
        "            for ht, sb in zip(meta_df[\"has_tumor\"], meta_df[\"size_bin\"])\n",
        "        ])\n",
        "    if level == \"ht\":\n",
        "        return meta_df[\"has_tumor\"].astype(str).values\n",
        "    raise ValueError(level)\n",
        "\n",
        "def pick_strat_labels_for_client(meta_client_df: pd.DataFrame, min_per_class: int = 2):\n",
        "    \"\"\"\n",
        "    Try different stratification levels and pick the most detailed one\n",
        "    that has at least `min_per_class` samples for each class.\n",
        "    \"\"\"\n",
        "    for level in [\"full\", \"ht_dom\", \"ht_size\", \"ht\"]:\n",
        "        y = build_label(meta_client_df, level)\n",
        "        counts = pd.Series(y).value_counts()\n",
        "        if (counts >= min_per_class).all():\n",
        "            print(f\"[INFO] Using client train/val stratification level: {level}\")\n",
        "            return y, level\n",
        "    print(\"[WARN] Falling back to NON-stratified train/val.\")\n",
        "    return None, \"none\"\n",
        "\n",
        "# Load full dataset and build metadata\n",
        "dataset = ImageOnlyGliomaDataset(METADATA_DF_PATH, DATA_ROOT, use_atlas=USE_ATLAS, exclude_ids=[\"PatientID_0191\"])\n",
        "meta = build_meta_for(dataset)\n",
        "\n",
        "clients: List[Dict[str, List[str]]] = []\n",
        "client_metas = []\n",
        "\n",
        "# In this script we only create a single \"client_0\" with a stratified train/val split\n",
        "meta_c = meta.reset_index(drop=True)\n",
        "Xc = meta_c[\"pid\"].values\n",
        "\n",
        "y_client, tv_level = pick_strat_labels_for_client(meta_c, min_per_class=2)\n",
        "\n",
        "if tv_level != \"none\":\n",
        "    # Stratified train/validation split\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, test_size=VAL_FRAC_PER_CLIENT, random_state=SEED)\n",
        "    (tr_idx, va_idx), = sss.split(Xc, y_client)\n",
        "    train_pids = Xc[tr_idx].tolist()\n",
        "    val_pids   = Xc[va_idx].tolist()\n",
        "else:\n",
        "    # Fallback: random split without stratification\n",
        "    rng = np.random.RandomState(SEED)\n",
        "    perm = rng.permutation(len(Xc))\n",
        "    split_at = int((1.0 - VAL_FRAC_PER_CLIENT) * len(Xc))\n",
        "    train_pids = Xc[perm[:split_at]].tolist()\n",
        "    val_pids   = Xc[perm[split_at:]].tolist()\n",
        "\n",
        "# Save client_0 train/val patient ID lists\n",
        "cdir = os.path.join(CLIENT_DIR, \"client_0\")\n",
        "os.makedirs(cdir, exist_ok=True)\n",
        "with open(os.path.join(cdir, \"train_pids.json\"), \"w\") as f:\n",
        "    json.dump(train_pids, f, indent=2)\n",
        "with open(os.path.join(cdir, \"val_pids.json\"), \"w\") as f:\n",
        "    json.dump(val_pids, f, indent=2)\n",
        "\n",
        "clients.append({\"train\": train_pids, \"val\": val_pids})\n",
        "client_metas.append(meta_c)\n",
        "\n",
        "# Save basic manifest of the split configuration\n",
        "manifest = {\n",
        "    \"seed\": SEED,\n",
        "    \"use_atlas\": USE_ATLAS,\n",
        "    \"n_clients\": 1,\n",
        "    \"val_frac_per_client\": VAL_FRAC_PER_CLIENT,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"num_workers\": NUM_WORKERS,\n",
        "    \"kfold_level\": \"single_client\"\n",
        "}\n",
        "with open(os.path.join(CLIENT_DIR, \"manifest.json\"), \"w\") as f:\n",
        "    json.dump(manifest, f, indent=2)\n",
        "\n",
        "class SubsetByPIDs(Dataset):\n",
        "    \"\"\"Simple subset wrapper that restricts the dataset to a given list of patient IDs.\"\"\"\n",
        "\n",
        "    def __init__(self, full_dataset: ImageOnlyGliomaDataset, pid_list: List[str]):\n",
        "        self.ds = full_dataset\n",
        "        pid_to_idx = {pid: i for i, pid in enumerate(self.ds.patient_ids)}\n",
        "        # Keep only IDs that exist in the base dataset\n",
        "        self.indices = [pid_to_idx[p] for p in pid_list if p in pid_to_idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.ds[self.indices[i]]\n",
        "\n",
        "def make_loader(ds, shuffle):\n",
        "    \"\"\"Create a DataLoader with the custom collate function.\"\"\"\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "        collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "        generator=torch.Generator().manual_seed(SEED),\n",
        "    )\n",
        "\n",
        "# Build small preview loaders for sanity checks\n",
        "preview_loaders = []\n",
        "for cid, cl in enumerate(clients):\n",
        "    ds_tr = SubsetByPIDs(dataset, cl[\"train\"])\n",
        "    ds_va = SubsetByPIDs(dataset, cl[\"val\"])\n",
        "    ld_tr = make_loader(ds_tr, shuffle=True)\n",
        "    ld_va = make_loader(ds_va, shuffle=False)\n",
        "    preview_loaders.append((ld_tr, ld_va))\n",
        "\n",
        "def summarize(meta_df: pd.DataFrame, name: str) -> Dict:\n",
        "    \"\"\"\n",
        "    Create a compact summary of a metadata subset:\n",
        "    - number of patients\n",
        "    - tumor presence counts\n",
        "    - size bin counts\n",
        "    - top 5 dominant regions\n",
        "    \"\"\"\n",
        "    s = {\"name\": name, \"n\": int(len(meta_df))}\n",
        "    s[\"has_tumor_counts\"] = meta_df[\"has_tumor\"].value_counts().to_dict()\n",
        "    s[\"size_bin_counts\"] = meta_df[\"size_bin\"].value_counts().to_dict()\n",
        "    s[\"dom_region_top5\"] = meta_df[\"dom\"].value_counts().head(5).to_dict()\n",
        "    return s\n",
        "\n",
        "# Global summary\n",
        "summary_all = summarize(meta, \"ALL\")\n",
        "print(\"\\n=== Global summary ===\")\n",
        "print(summary_all)\n",
        "\n",
        "# Per-client train/val summaries (here only one client)\n",
        "per_client_summaries = []\n",
        "for cid, cl in enumerate(clients):\n",
        "    meta_c = client_metas[cid]\n",
        "    tr = meta_c[meta_c[\"pid\"].isin(cl[\"train\"])]\n",
        "    va = meta_c[meta_c[\"pid\"].isin(cl[\"val\"])]\n",
        "    s_client = {\n",
        "        \"client\": \"client\",\n",
        "        \"train\": summarize(tr, \"client_train\"),\n",
        "        \"val\": summarize(va, \"client_val\"),\n",
        "    }\n",
        "    per_client_summaries.append(s_client)\n",
        "    print(\"\\n=== Client ===\")\n",
        "    print(s_client)\n",
        "\n",
        "# Save all summaries to disk\n",
        "with open(os.path.join(CLIENT_DIR, \"summary.json\"), \"w\") as f:\n",
        "    json.dump({\"global\": summary_all, \"per_client\": per_client_summaries}, f, indent=2)\n",
        "\n",
        "print(f\"\\nSaved client split and summaries to: {CLIENT_DIR}\")"
      ],
      "metadata": {
        "id": "7JG6B5jVm5yH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Distribution**"
      ],
      "metadata": {
        "id": "if1Ip7f-Gn01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"global\": {\n",
        "    \"name\": \"ALL\",\n",
        "    \"n\": 202,\n",
        "    \"has_tumor_counts\": {\n",
        "      \"1\": 202\n",
        "    },\n",
        "    \"size_bin_counts\": {\n",
        "      \"0\": 68,\n",
        "      \"1\": 67,\n",
        "      \"2\": 67\n",
        "    },\n",
        "    \"dom_region_top5\": {\n",
        "      \"61\": 85,\n",
        "      \"50\": 81,\n",
        "      \"0\": 10,\n",
        "      \"10\": 4,\n",
        "      \"22\": 3\n",
        "    }\n",
        "  },\n",
        "  \"per_client\": [\n",
        "    {\n",
        "      \"client\": \"client\",\n",
        "      \"train\": {\n",
        "        \"name\": \"client_train\",\n",
        "        \"n\": 161,\n",
        "        \"has_tumor_counts\": {\n",
        "          \"1\": 161\n",
        "        },\n",
        "        \"size_bin_counts\": {\n",
        "          \"2\": 54,\n",
        "          \"0\": 54,\n",
        "          \"1\": 53\n",
        "        },\n",
        "        \"dom_region_top5\": {\n",
        "          \"61\": 68,\n",
        "          \"50\": 64,\n",
        "          \"0\": 9,\n",
        "          \"10\": 3,\n",
        "          \"15\": 2\n",
        "        }\n",
        "      },\n",
        "      \"val\": {\n",
        "        \"name\": \"client_val\",\n",
        "        \"n\": 41,\n",
        "        \"has_tumor_counts\": {\n",
        "          \"1\": 41\n",
        "        },\n",
        "        \"size_bin_counts\": {\n",
        "          \"1\": 14,\n",
        "          \"0\": 14,\n",
        "          \"2\": 13\n",
        "        },\n",
        "        \"dom_region_top5\": {\n",
        "          \"61\": 17,\n",
        "          \"50\": 17,\n",
        "          \"31\": 1,\n",
        "          \"53\": 1,\n",
        "          \"10\": 1\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "id": "f6DL3ct7luT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Same distribution between 3 clients<h2>"
      ],
      "metadata": {
        "id": "Iqd4YVbAmZe-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_prep_split.py\n",
        "import os, json, pickle, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Tuple\n",
        "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Global paths and configuration\n",
        "DATA_ROOT = \"Preprocessed-Data\"\n",
        "METADATA_DF_PATH = \"cleaned_df.pkl\"\n",
        "OUT_BASE = \"AITDM\"\n",
        "CLIENT_DIR = os.path.join(OUT_BASE, \"client\")\n",
        "os.makedirs(CLIENT_DIR, exist_ok=True)\n",
        "\n",
        "USE_ATLAS = True\n",
        "N_CLIENTS = 3\n",
        "VAL_FRAC_PER_CLIENT = 0.2\n",
        "BATCH_SIZE = 8\n",
        "NUM_WORKERS = 2\n",
        "SEED = 42\n",
        "\n",
        "# Set all random seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(SEED)\n",
        "\n",
        "# Dataset that loads MRI, tumor mask and optional atlas for each patient\n",
        "class ImageOnlyGliomaDataset(Dataset):\n",
        "    def __init__(self, metadata_df_path, data_root, use_atlas=True, exclude_ids=None, transform=None):\n",
        "        with open(metadata_df_path, \"rb\") as f:\n",
        "            df = pickle.load(f)\n",
        "        if exclude_ids is None:\n",
        "            exclude_ids = [\"PatientID_0191\"]\n",
        "        self.df = df[~df[\"Patient_ID\"].isin(exclude_ids)].reset_index(drop=True)\n",
        "        self.data_root = data_root\n",
        "        self.use_atlas = use_atlas\n",
        "        self.transform = transform\n",
        "\n",
        "        # Collect patient IDs that have all required .npy files\n",
        "        self.patient_ids = []\n",
        "        for pid in sorted(self.df[\"Patient_ID\"].tolist()):\n",
        "            mri_p = os.path.join(self.data_root, pid, f\"{pid}_mri.npy\")\n",
        "            tumor_p = os.path.join(self.data_root, pid, f\"{pid}_tumor.npy\")\n",
        "            if self.use_atlas:\n",
        "                reg_p = os.path.join(self.data_root, pid, f\"{pid}_regions.npy\")\n",
        "                if os.path.isfile(mri_p) and os.path.isfile(tumor_p) and os.path.isfile(reg_p):\n",
        "                    self.patient_ids.append(pid)\n",
        "            else:\n",
        "                if os.path.isfile(mri_p) and os.path.isfile(tumor_p):\n",
        "                    self.patient_ids.append(pid)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patient_ids)\n",
        "\n",
        "    # Simple min–max normalization\n",
        "    @staticmethod\n",
        "    def _minmax(x):\n",
        "        x = x.astype(np.float32)\n",
        "        mn, mx = np.min(x), np.max(x)\n",
        "        return (x - mn) / (mx - mn) if mx > mn else np.zeros_like(x, dtype=np.float32)\n",
        "\n",
        "    # Load a single patient sample\n",
        "    def __getitem__(self, idx):\n",
        "        pid = self.patient_ids[idx]\n",
        "        base = os.path.join(self.data_root, pid)\n",
        "        mri = np.load(os.path.join(base, f\"{pid}_mri.npy\")).astype(np.float32)\n",
        "        tumor = np.load(os.path.join(base, f\"{pid}_tumor.npy\")).astype(np.float32)\n",
        "        mri = self._minmax(mri)\n",
        "        tumor = (tumor > 0.5).astype(np.float32)\n",
        "        sample = {\"patient_id\": pid, \"mri\": mri, \"tumor\": tumor}\n",
        "        if self.use_atlas:\n",
        "            regions = np.load(os.path.join(base, f\"{pid}_regions.npy\")).astype(np.float32)\n",
        "            regions = self._minmax(regions)\n",
        "            sample[\"regions\"] = regions\n",
        "        return sample\n",
        "\n",
        "# Custom collate function building batched tensors and patient IDs\n",
        "def image_only_collate_fn(batch, use_atlas=True):\n",
        "    mri = torch.stack([torch.tensor(it[\"mri\"]) for it in batch]).unsqueeze(1)\n",
        "    y = torch.stack([torch.tensor(it[\"tumor\"]) for it in batch]).unsqueeze(1)\n",
        "    if use_atlas:\n",
        "        regs = torch.stack([torch.tensor(it[\"regions\"]) for it in batch]).unsqueeze(1)\n",
        "        x = torch.cat([mri.float(), regs.float()], dim=1)\n",
        "    else:\n",
        "        x = mri.float()\n",
        "    pids = [it[\"patient_id\"] for it in batch]\n",
        "    return {\"x\": x, \"y\": y.float(), \"pid\": pids}\n",
        "\n",
        "# Compute basic metadata for a patient (tumor presence, area, dominant atlas region)\n",
        "def patient_meta(pid: str) -> Tuple[int, float, int]:\n",
        "    base = os.path.join(DATA_ROOT, pid)\n",
        "    tumor = np.load(os.path.join(base, f\"{pid}_tumor.npy\")).astype(np.float32)\n",
        "    mask = tumor > 0.5\n",
        "    area = float(mask.sum())\n",
        "    has_tumor = 1 if area >= 1 else 0\n",
        "\n",
        "    dom_region = -1\n",
        "    reg_path = os.path.join(base, f\"{pid}_regions.npy\")\n",
        "    if USE_ATLAS and os.path.isfile(reg_path) and mask.any():\n",
        "        regs = np.load(reg_path).astype(np.int32)\n",
        "        vals, counts = np.unique(regs[mask], return_counts=True)\n",
        "        if len(vals) > 0:\n",
        "            dom_region = int(vals[np.argmax(counts)])\n",
        "    return has_tumor, area, dom_region\n",
        "\n",
        "# Build a metadata DataFrame for all patients in the dataset\n",
        "def build_meta_for(dataset: ImageOnlyGliomaDataset) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for pid in dataset.patient_ids:\n",
        "        try:\n",
        "            ht, area, dom = patient_meta(pid)\n",
        "            rows.append({\"pid\": pid, \"has_tumor\": ht, \"area\": area, \"dom\": dom})\n",
        "        except Exception:\n",
        "            pass\n",
        "    meta = pd.DataFrame(rows)\n",
        "\n",
        "    # Compute tumor size bins for patients with tumor\n",
        "    meta[\"size_bin\"] = 0\n",
        "    m = meta[\"has_tumor\"] == 1\n",
        "    if m.any():\n",
        "        areas = meta.loc[m, \"area\"].values\n",
        "        qs = np.quantile(areas, np.linspace(0, 1, 4))\n",
        "        qs = np.unique(qs)\n",
        "        bins = np.digitize(areas, qs[1:-1], right=True) if len(qs) > 2 else np.zeros_like(areas, dtype=int)\n",
        "        meta.loc[m, \"size_bin\"] = bins.astype(int)\n",
        "\n",
        "    # Full stratification label combining several attributes\n",
        "    meta[\"strat_label\"] = [\n",
        "        f\"{int(ht)}_{int(dom)}_{int(sb)}\"\n",
        "        for ht, dom, sb in zip(meta[\"has_tumor\"], meta[\"dom\"], meta[\"size_bin\"])\n",
        "    ]\n",
        "    return meta\n",
        "\n",
        "# Build a stratification label with different levels of granularity\n",
        "def build_label(meta_df: pd.DataFrame, level: str) -> np.ndarray:\n",
        "    if level == \"full\":\n",
        "        return np.array([\n",
        "            f\"{int(ht)}_{int(dom)}_{int(sb)}\"\n",
        "            for ht, dom, sb in zip(meta_df[\"has_tumor\"], meta_df[\"dom\"], meta_df[\"size_bin\"])\n",
        "        ])\n",
        "    if level == \"ht_dom\":\n",
        "        return np.array([\n",
        "            f\"{int(ht)}_{int(dom)}\"\n",
        "            for ht, dom in zip(meta_df[\"has_tumor\"], meta_df[\"dom\"])\n",
        "        ])\n",
        "    if level == \"ht_size\":\n",
        "        return np.array([\n",
        "            f\"{int(ht)}_{int(sb)}\"\n",
        "            for ht, sb in zip(meta_df[\"has_tumor\"], meta_df[\"size_bin\"])\n",
        "        ])\n",
        "    if level == \"ht\":\n",
        "        return meta_df[\"has_tumor\"].astype(str).values\n",
        "    raise ValueError(level)\n",
        "\n",
        "# Choose the stratification labels level for K-Fold client split\n",
        "def pick_strat_labels_for_kfold(meta_df: pd.DataFrame, n_splits: int):\n",
        "    for level in [\"full\", \"ht_dom\", \"ht_size\", \"ht\"]:\n",
        "        y = build_label(meta_df, level)\n",
        "        counts = pd.Series(y).value_counts()\n",
        "        if (counts >= n_splits).all():\n",
        "            print(f\"[INFO] Using K-Fold stratification level: {level}\")\n",
        "            return y, level\n",
        "        else:\n",
        "            rare = counts[counts < n_splits]\n",
        "            print(f\"[WARN] Level '{level}' has rare classes (<{n_splits}): {rare.to_dict()} -> trying coarser...\")\n",
        "    print(\"[WARN] Falling back to NON-stratified K-Fold (insufficient counts at all levels).\")\n",
        "    return None, \"none\"\n",
        "\n",
        "# Choose the stratification labels level for the train/val split inside each client\n",
        "def pick_strat_labels_for_client(meta_client_df: pd.DataFrame, min_per_class: int = 2):\n",
        "    for level in [\"full\", \"ht_dom\", \"ht_size\", \"ht\"]:\n",
        "        y = build_label(meta_client_df, level)\n",
        "        counts = pd.Series(y).value_counts()\n",
        "        if (counts >= min_per_class).all():\n",
        "            print(f\"[INFO] Using client train/val stratification level: {level}\")\n",
        "            return y, level\n",
        "        else:\n",
        "            rare = counts[counts < min_per_class]\n",
        "            print(f\"[WARN] Client level '{level}' rare classes (<{min_per_class}): {rare.to_dict()} -> trying coarser...\")\n",
        "    print(\"[WARN] Falling back to NON-stratified train/val (insufficient counts at all levels).\")\n",
        "    return None, \"none\"\n",
        "\n",
        "# Load dataset and metadata for all patients\n",
        "dataset = ImageOnlyGliomaDataset(METADATA_DF_PATH, DATA_ROOT, use_atlas=USE_ATLAS, exclude_ids=[\"PatientID_0191\"])\n",
        "meta = build_meta_for(dataset)\n",
        "\n",
        "# Prepare global patient IDs and K-Fold labels\n",
        "X_all = meta[\"pid\"].values\n",
        "y_all_full, kfold_level = pick_strat_labels_for_kfold(meta, n_splits=N_CLIENTS)\n",
        "\n",
        "clients: List[Dict[str, List[str]]] = []\n",
        "client_metas = []\n",
        "\n",
        "# Create client splits and per-client train/val splits\n",
        "if kfold_level != \"none\":\n",
        "    skf = StratifiedKFold(n_splits=N_CLIENTS, shuffle=True, random_state=SEED)\n",
        "    for split_idx, (_, idx) in enumerate(skf.split(X_all, y_all_full)):\n",
        "        pids_client = X_all[idx]\n",
        "        meta_c = meta[meta[\"pid\"].isin(pids_client)].reset_index(drop=True)\n",
        "\n",
        "        y_client, tv_level = pick_strat_labels_for_client(meta_c, min_per_class=2)\n",
        "        Xc = meta_c[\"pid\"].values\n",
        "\n",
        "        if tv_level != \"none\":\n",
        "            sss = StratifiedShuffleSplit(n_splits=1, test_size=VAL_FRAC_PER_CLIENT, random_state=SEED)\n",
        "            (tr_idx, va_idx), = sss.split(Xc, y_client)\n",
        "            train_pids = Xc[tr_idx].tolist()\n",
        "            val_pids   = Xc[va_idx].tolist()\n",
        "        else:\n",
        "            rng = np.random.RandomState(SEED)\n",
        "            perm = rng.permutation(len(Xc))\n",
        "            split_at = int((1.0 - VAL_FRAC_PER_CLIENT) * len(Xc))\n",
        "            train_pids = Xc[perm[:split_at]].tolist()\n",
        "            val_pids   = Xc[perm[split_at:]].tolist()\n",
        "\n",
        "        cdir = os.path.join(CLIENT_DIR, f\"client_{split_idx}\")\n",
        "        os.makedirs(cdir, exist_ok=True)\n",
        "        with open(os.path.join(cdir, \"train_pids.json\"), \"w\") as f:\n",
        "            json.dump(train_pids, f, indent=2)\n",
        "        with open(os.path.join(cdir, \"val_pids.json\"), \"w\") as f:\n",
        "            json.dump(val_pids, f, indent=2)\n",
        "\n",
        "        clients.append({\"train\": train_pids, \"val\": val_pids})\n",
        "        client_metas.append(meta_c)\n",
        "else:\n",
        "    print(\"[INFO] Non-stratified 3-way split (deterministic) for clients.\")\n",
        "    rng = np.random.RandomState(SEED)\n",
        "    perm = rng.permutation(len(X_all))\n",
        "    sizes = [len(X_all) // N_CLIENTS] * N_CLIENTS\n",
        "    sizes[-1] += len(X_all) - sum(sizes)\n",
        "    start = 0\n",
        "    for split_idx, sz in enumerate(sizes):\n",
        "        pids_client = X_all[perm[start:start + sz]]\n",
        "        start += sz\n",
        "\n",
        "        meta_c = meta[meta[\"pid\"].isin(pids_client)].reset_index(drop=True)\n",
        "        Xc = meta_c[\"pid\"].values\n",
        "\n",
        "        perm_c = rng.permutation(len(Xc))\n",
        "        split_at = int((1.0 - VAL_FRAC_PER_CLIENT) * len(Xc))\n",
        "        train_pids = Xc[perm_c[:split_at]].tolist()\n",
        "        val_pids   = Xc[perm_c[split_at:]].tolist()\n",
        "\n",
        "        cdir = os.path.join(CLIENT_DIR, f\"client_{split_idx}\")\n",
        "        os.makedirs(cdir, exist_ok=True)\n",
        "        with open(os.path.join(cdir, \"train_pids.json\"), \"w\") as f:\n",
        "            json.dump(train_pids, f, indent=2)\n",
        "        with open(os.path.join(cdir, \"val_pids.json\"), \"w\") as f:\n",
        "            json.dump(val_pids, f, indent=2)\n",
        "\n",
        "        clients.append({\"train\": train_pids, \"val\": val_pids})\n",
        "        client_metas.append(meta_c)\n",
        "\n",
        "# Save a small manifest describing the split configuration\n",
        "manifest = {\n",
        "    \"seed\": SEED,\n",
        "    \"use_atlas\": USE_ATLAS,\n",
        "    \"n_clients\": N_CLIENTS,\n",
        "    \"val_frac_per_client\": VAL_FRAC_PER_CLIENT,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"num_workers\": NUM_WORKERS,\n",
        "    \"kfold_level\": kfold_level\n",
        "}\n",
        "with open(os.path.join(CLIENT_DIR, \"manifest.json\"), \"w\") as f:\n",
        "    json.dump(manifest, f, indent=2)\n",
        "\n",
        "# Dataset wrapper that restricts to a given list of patient IDs\n",
        "class SubsetByPIDs(Dataset):\n",
        "    def __init__(self, full_dataset: ImageOnlyGliomaDataset, pid_list: List[str]):\n",
        "        self.ds = full_dataset\n",
        "        pid_to_idx = {pid: i for i, pid in enumerate(self.ds.patient_ids)}\n",
        "        self.indices = [pid_to_idx[p] for p in pid_list if p in pid_to_idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.ds[self.indices[i]]\n",
        "\n",
        "# Helper to create a DataLoader with the custom collate function\n",
        "def make_loader(ds, shuffle):\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "        collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "        generator=torch.Generator().manual_seed(SEED),\n",
        "    )\n",
        "\n",
        "# Optional preview loaders for quick sanity checks\n",
        "preview_loaders = []\n",
        "for cid, cl in enumerate(clients):\n",
        "    ds_tr = SubsetByPIDs(dataset, cl[\"train\"])\n",
        "    ds_va = SubsetByPIDs(dataset, cl[\"val\"])\n",
        "    ld_tr = make_loader(ds_tr, shuffle=True)\n",
        "    ld_va = make_loader(ds_va, shuffle=False)\n",
        "    preview_loaders.append((ld_tr, ld_va))\n",
        "\n",
        "# Summarize metadata distribution for a subset\n",
        "def summarize(meta_df: pd.DataFrame, name: str) -> Dict:\n",
        "    s = {\"name\": name, \"n\": int(len(meta_df))}\n",
        "    s[\"has_tumor_counts\"] = meta_df[\"has_tumor\"].value_counts().to_dict()\n",
        "    s[\"size_bin_counts\"] = meta_df[\"size_bin\"].value_counts().to_dict()\n",
        "    s[\"dom_region_top5\"] = meta_df[\"dom\"].value_counts().head(5).to_dict()\n",
        "    return s\n",
        "\n",
        "# Global metadata summary\n",
        "summary_all = summarize(meta, \"ALL\")\n",
        "print(\"\\n=== Global summary ===\")\n",
        "print(summary_all)\n",
        "\n",
        "# Per-client train/val metadata summaries\n",
        "per_client_summaries = []\n",
        "for cid, cl in enumerate(clients):\n",
        "    meta_c = client_metas[cid]\n",
        "    tr = meta_c[meta_c[\"pid\"].isin(cl[\"train\"])]\n",
        "    va = meta_c[meta_c[\"pid\"].isin(cl[\"val\"])]\n",
        "    s_client = {\n",
        "        \"client\": cid,\n",
        "        \"train\": summarize(tr, f\"client_{cid}_train\"),\n",
        "        \"val\": summarize(va, f\"client_{cid}_val\"),\n",
        "    }\n",
        "    per_client_summaries.append(s_client)\n",
        "    print(f\"\\n=== Client {cid} ===\")\n",
        "    print(s_client)\n",
        "\n",
        "# Save all summaries to disk\n",
        "with open(os.path.join(CLIENT_DIR, \"summary.json\"), \"w\") as f:\n",
        "    json.dump({\"global\": summary_all, \"per_client\": per_client_summaries}, f, indent=2)\n",
        "\n",
        "print(f\"\\nSaved 3 client splits and summaries to: {CLIENT_DIR}\")"
      ],
      "metadata": {
        "id": "3FHiqxXmkUeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Distribution**"
      ],
      "metadata": {
        "id": "VfJlyrz0HEQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"global\": {\n",
        "    \"name\": \"ALL\",\n",
        "    \"n\": 202,\n",
        "    \"has_tumor_counts\": {\n",
        "      \"1\": 202\n",
        "    },\n",
        "    \"size_bin_counts\": {\n",
        "      \"0\": 68,\n",
        "      \"1\": 67,\n",
        "      \"2\": 67\n",
        "    },\n",
        "    \"dom_region_top5\": {\n",
        "      \"61\": 85,\n",
        "      \"50\": 81,\n",
        "      \"0\": 10,\n",
        "      \"10\": 4,\n",
        "      \"22\": 3\n",
        "    }\n",
        "  },\n",
        "  \"per_client\": [\n",
        "    {\n",
        "      \"client\": 0,\n",
        "      \"train\": {\n",
        "        \"name\": \"client_0_train\",\n",
        "        \"n\": 54,\n",
        "        \"has_tumor_counts\": {\n",
        "          \"1\": 54\n",
        "        },\n",
        "        \"size_bin_counts\": {\n",
        "          \"1\": 18,\n",
        "          \"2\": 18,\n",
        "          \"0\": 18\n",
        "        },\n",
        "        \"dom_region_top5\": {\n",
        "          \"61\": 29,\n",
        "          \"50\": 15,\n",
        "          \"0\": 4,\n",
        "          \"7\": 1,\n",
        "          \"17\": 1\n",
        "        }\n",
        "      },\n",
        "      \"val\": {\n",
        "        \"name\": \"client_0_val\",\n",
        "        \"n\": 14,\n",
        "        \"has_tumor_counts\": {\n",
        "          \"1\": 14\n",
        "        },\n",
        "        \"size_bin_counts\": {\n",
        "          \"0\": 5,\n",
        "          \"1\": 5,\n",
        "          \"2\": 4\n",
        "        },\n",
        "        \"dom_region_top5\": {\n",
        "          \"50\": 9,\n",
        "          \"53\": 1,\n",
        "          \"10\": 1,\n",
        "          \"61\": 1,\n",
        "          \"15\": 1\n",
        "        }\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"client\": 1,\n",
        "      \"train\": {\n",
        "        \"name\": \"client_1_train\",\n",
        "        \"n\": 53,\n",
        "        \"has_tumor_counts\": {\n",
        "          \"1\": 53\n",
        "        },\n",
        "        \"size_bin_counts\": {\n",
        "          \"1\": 18,\n",
        "          \"2\": 18,\n",
        "          \"0\": 17\n",
        "        },\n",
        "        \"dom_region_top5\": {\n",
        "          \"50\": 22,\n",
        "          \"61\": 19,\n",
        "          \"0\": 3,\n",
        "          \"22\": 2,\n",
        "          \"10\": 1\n",
        "        }\n",
        "      },\n",
        "      \"val\": {\n",
        "        \"name\": \"client_1_val\",\n",
        "        \"n\": 14,\n",
        "        \"has_tumor_counts\": {\n",
        "          \"1\": 14\n",
        "        },\n",
        "        \"size_bin_counts\": {\n",
        "          \"2\": 5,\n",
        "          \"0\": 5,\n",
        "          \"1\": 4\n",
        "        },\n",
        "        \"dom_region_top5\": {\n",
        "          \"50\": 6,\n",
        "          \"61\": 6,\n",
        "          \"10\": 1,\n",
        "          \"22\": 1\n",
        "        }\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"client\": 2,\n",
        "      \"train\": {\n",
        "        \"name\": \"client_2_train\",\n",
        "        \"n\": 53,\n",
        "        \"has_tumor_counts\": {\n",
        "          \"1\": 53\n",
        "        },\n",
        "        \"size_bin_counts\": {\n",
        "          \"2\": 18,\n",
        "          \"0\": 18,\n",
        "          \"1\": 17\n",
        "        },\n",
        "        \"dom_region_top5\": {\n",
        "          \"61\": 25,\n",
        "          \"50\": 22,\n",
        "          \"23\": 1,\n",
        "          \"31\": 1,\n",
        "          \"62\": 1\n",
        "        }\n",
        "      },\n",
        "      \"val\": {\n",
        "        \"name\": \"client_2_val\",\n",
        "        \"n\": 14,\n",
        "        \"has_tumor_counts\": {\n",
        "          \"1\": 14\n",
        "        },\n",
        "        \"size_bin_counts\": {\n",
        "          \"0\": 5,\n",
        "          \"1\": 5,\n",
        "          \"2\": 4\n",
        "        },\n",
        "        \"dom_region_top5\": {\n",
        "          \"50\": 7,\n",
        "          \"61\": 5,\n",
        "          \"10\": 1,\n",
        "          \"0\": 1\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "id": "-m02RicJl5mC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>non-IID between 3 clients<h2>"
      ],
      "metadata": {
        "id": "gV6kW5udsrGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_prep_split.py\n",
        "import os, json, pickle, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Global configuration and paths\n",
        "DATA_ROOT = \"Preprocessed-Data\"\n",
        "METADATA_DF_PATH = \"cleaned_df.pkl\"\n",
        "OUT_BASE = \"AITDM\"\n",
        "CLIENT_DIR = os.path.join(OUT_BASE, \"client\")\n",
        "os.makedirs(CLIENT_DIR, exist_ok=True)\n",
        "\n",
        "USE_ATLAS = True\n",
        "N_CLIENTS = 3\n",
        "VAL_FRAC_PER_CLIENT = 0.2\n",
        "BATCH_SIZE = 8\n",
        "NUM_WORKERS = 2\n",
        "SEED = 42\n",
        "\n",
        "# Dirichlet alpha controls degree of non-IID distribution\n",
        "DIRICHLET_ALPHA = 0.3\n",
        "\n",
        "# Ensure reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(SEED)\n",
        "\n",
        "# Dataset class that loads MRI, tumor masks, and optional atlas data\n",
        "class ImageOnlyGliomaDataset(Dataset):\n",
        "    def __init__(self, metadata_df_path, data_root, use_atlas=True, exclude_ids=None, transform=None):\n",
        "        with open(metadata_df_path, \"rb\") as f:\n",
        "            df = pickle.load(f)\n",
        "        if exclude_ids is None:\n",
        "            exclude_ids = [\"PatientID_0191\"]\n",
        "\n",
        "        self.df = df[~df[\"Patient_ID\"].isin(exclude_ids)].reset_index(drop=True)\n",
        "        self.data_root = data_root\n",
        "        self.use_atlas = use_atlas\n",
        "        self.transform = transform\n",
        "\n",
        "        # Collect only patients with required files\n",
        "        self.patient_ids = []\n",
        "        for pid in sorted(self.df[\"Patient_ID\"].tolist()):\n",
        "            mri_p = os.path.join(self.data_root, pid, f\"{pid}_mri.npy\")\n",
        "            tumor_p = os.path.join(self.data_root, pid, f\"{pid}_tumor.npy\")\n",
        "            if self.use_atlas:\n",
        "                reg_p = os.path.join(self.data_root, pid, f\"{pid}_regions.npy\")\n",
        "                if os.path.isfile(mri_p) and os.path.isfile(tumor_p) and os.path.isfile(reg_p):\n",
        "                    self.patient_ids.append(pid)\n",
        "            else:\n",
        "                if os.path.isfile(mri_p) and os.path.isfile(tumor_p):\n",
        "                    self.patient_ids.append(pid)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patient_ids)\n",
        "\n",
        "    # Min–max normalization\n",
        "    @staticmethod\n",
        "    def _minmax(x):\n",
        "        x = x.astype(np.float32)\n",
        "        mn, mx = np.min(x), np.max(x)\n",
        "        return (x - mn) / (mx - mn) if mx > mn else np.zeros_like(x, dtype=np.float32)\n",
        "\n",
        "    # Load a single patient's MRI + masks\n",
        "    def __getitem__(self, idx):\n",
        "        pid = self.patient_ids[idx]\n",
        "        base = os.path.join(self.data_root, pid)\n",
        "\n",
        "        mri = np.load(os.path.join(base, f\"{pid}_mri.npy\")).astype(np.float32)\n",
        "        tumor = np.load(os.path.join(base, f\"{pid}_tumor.npy\")).astype(np.float32)\n",
        "\n",
        "        mri = self._minmax(mri)\n",
        "        tumor = (tumor > 0.5).astype(np.float32)\n",
        "\n",
        "        sample = {\"patient_id\": pid, \"mri\": mri, \"tumor\": tumor}\n",
        "\n",
        "        if self.use_atlas:\n",
        "            regions = np.load(os.path.join(base, f\"{pid}_regions.npy\")).astype(np.float32)\n",
        "            regions = self._minmax(regions)\n",
        "            sample[\"regions\"] = regions\n",
        "\n",
        "        return sample\n",
        "\n",
        "# Collate function used in DataLoaders\n",
        "def image_only_collate_fn(batch, use_atlas=True):\n",
        "    mri = torch.stack([torch.tensor(it[\"mri\"]) for it in batch]).unsqueeze(1)\n",
        "    y = torch.stack([torch.tensor(it[\"tumor\"]) for it in batch]).unsqueeze(1)\n",
        "\n",
        "    if use_atlas:\n",
        "        regs = torch.stack([torch.tensor(it[\"regions\"]) for it in batch]).unsqueeze(1)\n",
        "        x = torch.cat([mri.float(), regs.float()], dim=1)\n",
        "    else:\n",
        "        x = mri.float()\n",
        "\n",
        "    pids = [it[\"patient_id\"] for it in batch]\n",
        "    return {\"x\": x, \"y\": y.float(), \"pid\": pids}\n",
        "\n",
        "# Compute tumor presence, area and dominant atlas region\n",
        "def patient_meta(pid: str) -> Tuple[int, float, int]:\n",
        "    base = os.path.join(DATA_ROOT, pid)\n",
        "    tumor = np.load(os.path.join(base, f\"{pid}_tumor.npy\")).astype(np.float32)\n",
        "    mask = tumor > 0.5\n",
        "\n",
        "    area = float(mask.sum())\n",
        "    has_tumor = 1 if area >= 1 else 0\n",
        "\n",
        "    dom_region = -1\n",
        "    reg_path = os.path.join(base, f\"{pid}_regions.npy\")\n",
        "\n",
        "    if USE_ATLAS and os.path.isfile(reg_path) and mask.any():\n",
        "        regs = np.load(reg_path).astype(np.int32)\n",
        "        vals, counts = np.unique(regs[mask], return_counts=True)\n",
        "        if len(vals) > 0:\n",
        "            dom_region = int(vals[np.argmax(counts)])\n",
        "\n",
        "    return has_tumor, area, dom_region\n",
        "\n",
        "# Build metadata for all patients\n",
        "def build_meta_for(dataset: ImageOnlyGliomaDataset) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for pid in dataset.patient_ids:\n",
        "        try:\n",
        "            ht, area, dom = patient_meta(pid)\n",
        "            rows.append({\"pid\": pid, \"has_tumor\": ht, \"area\": area, \"dom\": dom})\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    meta = pd.DataFrame(rows)\n",
        "\n",
        "    # Create tumor size bins\n",
        "    meta[\"size_bin\"] = 0\n",
        "    m = meta[\"has_tumor\"] == 1\n",
        "    if m.any():\n",
        "        areas = meta.loc[m, \"area\"].values\n",
        "        qs = np.quantile(areas, np.linspace(0, 1, 4))\n",
        "        qs = np.unique(qs)\n",
        "        bins = np.digitize(areas, qs[1:-1], right=True) if len(qs) > 2 else np.zeros_like(areas, int)\n",
        "        meta.loc[m, \"size_bin\"] = bins.astype(int)\n",
        "\n",
        "    return meta\n",
        "\n",
        "# Load dataset + metadata\n",
        "dataset = ImageOnlyGliomaDataset(METADATA_DF_PATH, DATA_ROOT, use_atlas=USE_ATLAS, exclude_ids=[\"PatientID_0191\"])\n",
        "meta = build_meta_for(dataset)\n",
        "rng = np.random.RandomState(SEED)\n",
        "\n",
        "# Group patients by class (tumor size bin)\n",
        "class_to_pids = defaultdict(list)\n",
        "for _, row in meta.iterrows():\n",
        "    label = row[\"size_bin\"]\n",
        "    class_to_pids[label].append(row[\"pid\"])\n",
        "\n",
        "# Allocate patients to clients using Dirichlet sampling\n",
        "client_pid_sets = [[] for _ in range(N_CLIENTS)]\n",
        "\n",
        "for label, pids in class_to_pids.items():\n",
        "    pids = rng.permutation(pids)\n",
        "    proportions = rng.dirichlet([DIRICHLET_ALPHA] * N_CLIENTS)\n",
        "    counts = (proportions * len(pids)).astype(int)\n",
        "\n",
        "    # Adjust counts to cover all samples\n",
        "    while counts.sum() < len(pids):\n",
        "        counts[rng.randint(0, N_CLIENTS)] += 1\n",
        "\n",
        "    start = 0\n",
        "    for cid, count in enumerate(counts):\n",
        "        subset = pids[start:start + count]\n",
        "        client_pid_sets[cid].extend(subset)\n",
        "        start += count\n",
        "\n",
        "# Create clients + train/val splits\n",
        "clients = []\n",
        "client_metas = []\n",
        "\n",
        "for cid in range(N_CLIENTS):\n",
        "    Xc = np.array(client_pid_sets[cid])\n",
        "\n",
        "    rng_c = np.random.RandomState(SEED + cid)\n",
        "    perm_c = rng_c.permutation(len(Xc))\n",
        "\n",
        "    split_at = max(1, int((1.0 - VAL_FRAC_PER_CLIENT) * len(Xc)))\n",
        "    split_at = min(split_at, len(Xc) - 1)\n",
        "\n",
        "    train_pids = Xc[perm_c[:split_at]].tolist()\n",
        "    val_pids   = Xc[perm_c[split_at:]].tolist()\n",
        "\n",
        "    meta_c = meta[meta[\"pid\"].isin(Xc)].reset_index(drop=True)\n",
        "\n",
        "    cdir = os.path.join(CLIENT_DIR, f\"client_{cid}\")\n",
        "    os.makedirs(cdir, exist_ok=True)\n",
        "\n",
        "    with open(os.path.join(cdir, \"train_pids.json\"), \"w\") as f:\n",
        "        json.dump(train_pids, f, indent=2)\n",
        "\n",
        "    with open(os.path.join(cdir, \"val_pids.json\"), \"w\") as f:\n",
        "        json.dump(val_pids, f, indent=2)\n",
        "\n",
        "    clients.append({\"train\": train_pids, \"val\": val_pids})\n",
        "    client_metas.append(meta_c)\n",
        "\n",
        "print(f\"[INFO] Created {N_CLIENTS} non-IID clients using Dirichlet split (alpha={DIRICHLET_ALPHA}).\")\n",
        "\n",
        "# Save manifest describing split configuration\n",
        "manifest = {\n",
        "    \"seed\": SEED,\n",
        "    \"use_atlas\": USE_ATLAS,\n",
        "    \"n_clients\": N_CLIENTS,\n",
        "    \"val_frac_per_client\": VAL_FRAC_PER_CLIENT,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"num_workers\": NUM_WORKERS,\n",
        "    \"split_type\": \"non-IID_Dirichlet\",\n",
        "    \"dirichlet_alpha\": DIRICHLET_ALPHA\n",
        "}\n",
        "\n",
        "with open(os.path.join(CLIENT_DIR, \"manifest.json\"), \"w\") as f:\n",
        "    json.dump(manifest, f, indent=2)\n",
        "\n",
        "# Dataset wrapper for selecting specific patient IDs\n",
        "class SubsetByPIDs(Dataset):\n",
        "    def __init__(self, full_dataset: ImageOnlyGliomaDataset, pid_list: List[str]):\n",
        "        self.ds = full_dataset\n",
        "        pid_to_idx = {pid: i for i, pid in enumerate(self.ds.patient_ids)}\n",
        "        self.indices = [pid_to_idx[p] for p in pid_list if p in pid_to_idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.ds[self.indices[i]]\n",
        "\n",
        "# Helper to build DataLoaders\n",
        "def make_loader(ds, shuffle):\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "        collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "        generator=torch.Generator().manual_seed(SEED),\n",
        "    )\n",
        "\n",
        "# Build preview loaders for validation\n",
        "preview_loaders = []\n",
        "for cid, cl in enumerate(clients):\n",
        "    ds_tr = SubsetByPIDs(dataset, cl[\"train\"])\n",
        "    ds_va = SubsetByPIDs(dataset, cl[\"val\"])\n",
        "    ld_tr = make_loader(ds_tr, shuffle=True)\n",
        "    ld_va = make_loader(ds_va, shuffle=False)\n",
        "    preview_loaders.append((ld_tr, ld_va))\n",
        "\n",
        "# Produce metadata summaries\n",
        "def summarize(meta_df: pd.DataFrame, name: str) -> Dict:\n",
        "    s = {\"name\": name, \"n\": int(len(meta_df))}\n",
        "    s[\"has_tumor_counts\"] = meta_df[\"has_tumor\"].value_counts().to_dict()\n",
        "    s[\"size_bin_counts\"] = meta_df[\"size_bin\"].value_counts().to_dict()\n",
        "    s[\"dom_region_top5\"] = meta_df[\"dom\"].value_counts().head(5).to_dict()\n",
        "    return s\n",
        "\n",
        "summary_all = summarize(meta, \"ALL\")\n",
        "print(\"\\n=== Global summary ===\")\n",
        "print(summary_all)\n",
        "\n",
        "per_client_summaries = []\n",
        "for cid, cl in enumerate(clients):\n",
        "    meta_c = client_metas[cid]\n",
        "    tr = meta_c[meta_c[\"pid\"].isin(cl[\"train\"])]\n",
        "    va = meta_c[meta_c[\"pid\"].isin(cl[\"val\"])]\n",
        "\n",
        "    s_client = {\n",
        "        \"client\": cid,\n",
        "        \"train\": summarize(tr, f\"client_{cid}_train\"),\n",
        "        \"val\": summarize(va, f\"client_{cid}_val\"),\n",
        "    }\n",
        "\n",
        "    per_client_summaries.append(s_client)\n",
        "    print(f\"\\n=== Client {cid} ===\")\n",
        "    print(s_client)\n",
        "\n",
        "# Save summaries\n",
        "with open(os.path.join(CLIENT_DIR, \"summary.json\"), \"w\") as f:\n",
        "    json.dump({\"global\": summary_all, \"per_client\": per_client_summaries}, f, indent=2)\n",
        "\n",
        "print(f\"\\nSaved {N_CLIENTS} non-IID client splits and summaries to: {CLIENT_DIR}\")"
      ],
      "metadata": {
        "id": "T1fCwCcXtNQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Distribution**"
      ],
      "metadata": {
        "id": "2hy9_4kXHHgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"global\": {\n",
        "    \"name\": \"ALL\",\n",
        "    \"n\": 202,\n",
        "    \"has_tumor_counts\": {\n",
        "      \"1\": 202\n",
        "    },\n",
        "    \"size_bin_counts\": {\n",
        "      \"0\": 68,\n",
        "      \"1\": 67,\n",
        "      \"2\": 67\n",
        "    },\n",
        "    \"dom_region_top5\": {\n",
        "      \"61\": 85,\n",
        "      \"50\": 81,\n",
        "      \"0\": 10,\n",
        "      \"10\": 4,\n",
        "      \"22\": 3\n",
        "    }\n",
        "  },\n",
        "  \"per_client\": [\n",
        "    {\n",
        "      \"client\": 0,\n",
        "      \"train\": {\n",
        "        \"name\": \"client_0_train\",\n",
        "        \"n\": 35,\n",
        "        \"has_tumor_counts\": {\n",
        "          \"1\": 35\n",
        "        },\n",
        "        \"size_bin_counts\": {\n",
        "          \"1\": 32,\n",
        "          \"0\": 2,\n",
        "          \"2\": 1\n",
        "        },\n",
        "        \"dom_region_top5\": {\n",
        "          \"50\": 18,\n",
        "          \"61\": 13,\n",
        "          \"7\": 1,\n",
        "          \"62\": 1,\n",
        "          \"10\": 1\n",
        "        }\n",
        "      },\n",
        "      \"val\": {\n",
        "        \"name\": \"client_0_val\",\n",
        "        \"n\": 9,\n",
        "        \"has_tumor_counts\": {\n",
        "          \"1\": 9\n",
        "        },\n",
        "        \"size_bin_counts\": {\n",
        "          \"1\": 8,\n",
        "          \"0\": 1\n",
        "        },\n",
        "        \"dom_region_top5\": {\n",
        "          \"50\": 4,\n",
        "          \"61\": 3,\n",
        "          \"10\": 1,\n",
        "          \"58\": 1\n",
        "        }\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"client\": 1,\n",
        "      \"train\": {\n",
        "        \"name\": \"client_1_train\",\n",
        "        \"n\": 96,\n",
        "        \"has_tumor_counts\": {\n",
        "          \"1\": 96\n",
        "        },\n",
        "        \"size_bin_counts\": {\n",
        "          \"2\": 49,\n",
        "          \"0\": 27,\n",
        "          \"1\": 20\n",
        "        },\n",
        "        \"dom_region_top5\": {\n",
        "          \"61\": 47,\n",
        "          \"50\": 34,\n",
        "          \"0\": 5,\n",
        "          \"1\": 1,\n",
        "          \"23\": 1\n",
        "        }\n",
        "      },\n",
        "      \"val\": {\n",
        "        \"name\": \"client_1_val\",\n",
        "        \"n\": 25,\n",
        "        \"has_tumor_counts\": {\n",
        "          \"1\": 25\n",
        "        },\n",
        "        \"size_bin_counts\": {\n",
        "          \"2\": 17,\n",
        "          \"0\": 4,\n",
        "          \"1\": 4\n",
        "        },\n",
        "        \"dom_region_top5\": {\n",
        "          \"61\": 13,\n",
        "          \"50\": 11,\n",
        "          \"0\": 1\n",
        "        }\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"client\": 2,\n",
        "      \"train\": {\n",
        "        \"name\": \"client_2_train\",\n",
        "        \"n\": 29,\n",
        "        \"has_tumor_counts\": {\n",
        "          \"1\": 29\n",
        "        },\n",
        "        \"size_bin_counts\": {\n",
        "          \"0\": 26,\n",
        "          \"1\": 3\n",
        "        },\n",
        "        \"dom_region_top5\": {\n",
        "          \"50\": 12,\n",
        "          \"61\": 8,\n",
        "          \"22\": 2,\n",
        "          \"0\": 2,\n",
        "          \"31\": 1\n",
        "        }\n",
        "      },\n",
        "      \"val\": {\n",
        "        \"name\": \"client_2_val\",\n",
        "        \"n\": 8,\n",
        "        \"has_tumor_counts\": {\n",
        "          \"1\": 8\n",
        "        },\n",
        "        \"size_bin_counts\": {\n",
        "          \"0\": 8\n",
        "        },\n",
        "        \"dom_region_top5\": {\n",
        "          \"50\": 2,\n",
        "          \"61\": 1,\n",
        "          \"10\": 1,\n",
        "          \"45\": 1,\n",
        "          \"22\": 1\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "id": "xm0V8RS9l96o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **M1 - Baseline Model**"
      ],
      "metadata": {
        "id": "JPU64UV3E4v9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Train a model for each client<h2>"
      ],
      "metadata": {
        "id": "gkngkfhZtbvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/client.zip -d /content/client"
      ],
      "metadata": {
        "id": "mF02IxEM22gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/Preprocessed-Data.zip -d /content/Preprocessed-Data"
      ],
      "metadata": {
        "id": "V6hGU2Q4FuQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, pickle, random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "from sklearn.metrics import jaccard_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "# Paths and I/O config\n",
        "DATA_ROOT = \"/content/Preprocessed-Data\"\n",
        "METADATA_DF_PATH = \"cleaned_df.pkl\"\n",
        "OUT_BASE = \"AITDM\"\n",
        "CLIENT_DIR = os.path.join(OUT_BASE, \"/content/client\")\n",
        "\n",
        "# Experiment config\n",
        "USE_ATLAS = True\n",
        "CLIENT_ID = 0\n",
        "BATCH_SIZE = 8\n",
        "LR = 1e-3\n",
        "EPOCHS = 50\n",
        "NUM_WORKERS = 2\n",
        "SEED = 42\n",
        "\n",
        "# Model / encoder config\n",
        "ENCODER_NAME = \"resnet34\"\n",
        "ENCODER_WEIGHTS = \"imagenet\"\n",
        "\n",
        "# Output directories\n",
        "OUT_MODELS_DIR = os.path.join(OUT_BASE, \"Models\", \"UNet_ImageOnly\")\n",
        "OUT_GRAPHS_DIR = os.path.join(OUT_BASE, \"Graphs\")\n",
        "os.makedirs(OUT_MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(OUT_GRAPHS_DIR, exist_ok=True)\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
        "\n",
        "def worker_init_fn(worker_id):\n",
        "    worker_seed = SEED + worker_id\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "set_seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class ImageOnlyGliomaDataset(Dataset):\n",
        "    \"\"\"Dataset that loads MRI, tumor mask, and optional atlas regions per patient.\"\"\"\n",
        "\n",
        "    def __init__(self, metadata_df_path, data_root, use_atlas=True, exclude_ids=None, transform=None):\n",
        "        import pickle, os, numpy as np, pandas as pd\n",
        "\n",
        "        # Load metadata dataframe\n",
        "        with open(metadata_df_path, \"rb\") as f:\n",
        "            df = pickle.load(f)\n",
        "\n",
        "        # Optionally exclude some patient IDs\n",
        "        if exclude_ids is None:\n",
        "            exclude_ids = [\"PatientID_0191\"]\n",
        "        self.df = df[~df[\"Patient_ID\"].isin(exclude_ids)].reset_index(drop=True)\n",
        "\n",
        "        self.data_root = data_root\n",
        "        self.use_atlas = use_atlas\n",
        "        self.transform = transform\n",
        "\n",
        "        # Collect patient IDs that have all required files\n",
        "        self.patient_ids = []\n",
        "        for pid in sorted(self.df[\"Patient_ID\"].tolist()):\n",
        "            mri_p = os.path.join(self.data_root, pid, f\"{pid}_mri.npy\")\n",
        "            tumor_p = os.path.join(self.data_root, pid, f\"{pid}_tumor.npy\")\n",
        "\n",
        "            if self.use_atlas:\n",
        "                reg_p = os.path.join(self.data_root, pid, f\"{pid}_regions.npy\")\n",
        "                if os.path.isfile(mri_p) and os.path.isfile(tumor_p) and os.path.isfile(reg_p):\n",
        "                    self.patient_ids.append(pid)\n",
        "            else:\n",
        "                if os.path.isfile(mri_p) and os.path.isfile(tumor_p):\n",
        "                    self.patient_ids.append(pid)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patient_ids)\n",
        "\n",
        "    @staticmethod\n",
        "    def _minmax(x):\n",
        "        \"\"\"Simple min-max normalization to [0, 1].\"\"\"\n",
        "        x = x.astype(np.float32)\n",
        "        mn, mx = np.min(x), np.max(x)\n",
        "        return (x - mn) / (mx - mn) if mx > mn else np.zeros_like(x, dtype=np.float32)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Load and return one sample (dict) for a patient.\"\"\"\n",
        "        import os, numpy as np\n",
        "\n",
        "        pid = self.patient_ids[idx]\n",
        "        base = os.path.join(self.data_root, pid)\n",
        "\n",
        "        # Load MRI and tumor mask\n",
        "        mri = np.load(os.path.join(base, f\"{pid}_mri.npy\")).astype(np.float32)\n",
        "        tumor = np.load(os.path.join(base, f\"{pid}_tumor.npy\")).astype(np.float32)\n",
        "\n",
        "        # Normalize MRI and binarize tumor mask\n",
        "        mri = self._minmax(mri)\n",
        "        tumor = (tumor > 0.5).astype(np.float32)\n",
        "\n",
        "        sample = {\"patient_id\": pid, \"mri\": mri, \"tumor\": tumor}\n",
        "\n",
        "        # Optionally load regions/atlas\n",
        "        if self.use_atlas:\n",
        "            regions = np.load(os.path.join(base, f\"{pid}_regions.npy\")).astype(np.float32)\n",
        "            regions = self._minmax(regions)\n",
        "            sample[\"regions\"] = regions\n",
        "\n",
        "        return sample\n",
        "\n",
        "def image_only_collate_fn(batch, use_atlas=True):\n",
        "    \"\"\"Custom collate: stack MRI (+ optional regions) and tumor into tensors.\"\"\"\n",
        "    mri = torch.stack([torch.tensor(it[\"mri\"]) for it in batch]).unsqueeze(1)\n",
        "    y = torch.stack([torch.tensor(it[\"tumor\"]) for it in batch]).unsqueeze(1)\n",
        "\n",
        "    if use_atlas:\n",
        "        regs = torch.stack([torch.tensor(it[\"regions\"]) for it in batch]).unsqueeze(1)\n",
        "        # Concatenate MRI and regions as channels\n",
        "        x = torch.cat([mri.float(), regs.float()], dim=1)\n",
        "    else:\n",
        "        x = mri.float()\n",
        "\n",
        "    return {\"x\": x, \"y\": y.float(), \"pid\": [it[\"patient_id\"] for it in batch]}\n",
        "\n",
        "class SubsetByPIDs(Dataset):\n",
        "    \"\"\"Wrap a dataset but keep only a subset of patient IDs.\"\"\"\n",
        "\n",
        "    def __init__(self, full_dataset: ImageOnlyGliomaDataset, pid_list: List[str]):\n",
        "        self.ds = full_dataset\n",
        "        # Map patient IDs to indices\n",
        "        pid_to_idx = {pid: i for i, pid in enumerate(self.ds.patient_ids)}\n",
        "        self.indices = [pid_to_idx[p] for p in pid_list if p in pid_to_idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.ds[self.indices[i]]\n",
        "\n",
        "# Load client-specific train/val patient IDs\n",
        "cdir = os.path.join(CLIENT_DIR, f\"client_{CLIENT_ID}\")\n",
        "with open(os.path.join(cdir, \"train_pids.json\"), \"r\") as f:\n",
        "    train_pids = json.load(f)\n",
        "with open(os.path.join(cdir, \"val_pids.json\"), \"r\") as f:\n",
        "    val_pids = json.load(f)\n",
        "\n",
        "# Build full dataset and then client subsets\n",
        "full_ds = ImageOnlyGliomaDataset(\n",
        "    METADATA_DF_PATH,\n",
        "    DATA_ROOT,\n",
        "    use_atlas=USE_ATLAS,\n",
        "    exclude_ids=[\"PatientID_0191\"],\n",
        ")\n",
        "train_dataset = SubsetByPIDs(full_ds, train_pids)\n",
        "val_dataset = SubsetByPIDs(full_ds, val_pids)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "    generator=torch.Generator().manual_seed(SEED),\n",
        "    worker_init_fn=worker_init_fn,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "    generator=torch.Generator().manual_seed(SEED),\n",
        "    worker_init_fn=worker_init_fn,\n",
        ")\n",
        "\n",
        "print(f\"Loaded client_{CLIENT_ID}: train patients={len(train_dataset)}, val patients={len(val_dataset)}\")\n",
        "\n",
        "# UNet model (from segmentation_models_pytorch)\n",
        "in_channels = 2 if USE_ATLAS else 1\n",
        "model = smp.Unet(\n",
        "    encoder_name=ENCODER_NAME,\n",
        "    encoder_weights=ENCODER_WEIGHTS,\n",
        "    in_channels=in_channels,\n",
        "    classes=1,\n",
        ").to(device)\n",
        "\n",
        "# Loss, optimizer, scheduler, and AMP scaler\n",
        "criterion = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
        "scheduler = optim.lr_scheduler.LinearLR(\n",
        "    optimizer, start_factor=1.0, end_factor=0.0, total_iters=EPOCHS\n",
        ")\n",
        "scaler = torch.amp.GradScaler(\"cuda\", enabled=(device.type == \"cuda\"))\n",
        "\n",
        "def calc_metrics(y_true, y_pred):\n",
        "    \"\"\"Compute Dice, IoU, and pixel accuracy.\"\"\"\n",
        "    y_true = y_true.astype(np.uint8).reshape(-1)\n",
        "    y_pred = y_pred.astype(np.uint8).reshape(-1)\n",
        "\n",
        "    inter = (y_true * y_pred).sum()\n",
        "    dice = (2.0 * inter) / (y_true.sum() + y_pred.sum() + 1e-8)\n",
        "\n",
        "    # IoU with sklearn, fallback if it fails\n",
        "    try:\n",
        "        iou = jaccard_score(y_true, y_pred, average=\"binary\")\n",
        "    except Exception:\n",
        "        union = y_true.sum() + y_pred.sum() - inter + 1e-8\n",
        "        iou = inter / union\n",
        "\n",
        "    acc = (y_true == y_pred).mean()\n",
        "    return float(dice), float(iou), float(acc)\n",
        "\n",
        "def plot_prediction(sample_imgs, save_path):\n",
        "    \"\"\"Plot MRI (+ optional regions), GT, and prediction for one batch.\"\"\"\n",
        "    if \"regions\" in sample_imgs:\n",
        "        fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
        "    else:\n",
        "        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # MRI\n",
        "    axs[0].imshow(sample_imgs[\"mri\"][0, 0], cmap=\"gray\")\n",
        "    axs[0].set_title(\"MRI\")\n",
        "    axs[0].axis(\"off\")\n",
        "\n",
        "    col = 1\n",
        "\n",
        "    # Regions (if available)\n",
        "    if \"regions\" in sample_imgs:\n",
        "        axs[col].imshow(sample_imgs[\"regions\"][0, 0], cmap=\"gray\")\n",
        "        axs[col].set_title(\"Regions\")\n",
        "        axs[col].axis(\"off\")\n",
        "        col += 1\n",
        "\n",
        "    # Ground truth\n",
        "    axs[col].imshow(sample_imgs[\"y_true\"][0, 0], cmap=\"gray\")\n",
        "    axs[col].set_title(\"Ground Truth\")\n",
        "    axs[col].axis(\"off\")\n",
        "    col += 1\n",
        "\n",
        "    # Prediction\n",
        "    axs[col].imshow(sample_imgs[\"y_pred\"][0, 0], cmap=\"gray\")\n",
        "    axs[col].set_title(\"Predicted\")\n",
        "    axs[col].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def plot_metrics(history, save_path):\n",
        "    \"\"\"Plot training and validation curves for loss and metrics.\"\"\"\n",
        "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
        "\n",
        "    plt.figure(figsize=(16, 12))\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\")\n",
        "    plt.plot(epochs, history[\"val_loss\"], label=\"Val Loss\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Loss\")\n",
        "\n",
        "    # Dice\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(epochs, history[\"train_dice\"], label=\"Train Dice\")\n",
        "    plt.plot(epochs, history[\"val_dice\"], label=\"Val Dice\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Dice\")\n",
        "\n",
        "    # IoU\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(epochs, history[\"train_iou\"], label=\"Train IoU\")\n",
        "    plt.plot(epochs, history[\"val_iou\"], label=\"Val IoU\")\n",
        "    plt.legend()\n",
        "    plt.title(\"IoU\")\n",
        "\n",
        "    # Accuracy\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(epochs, history[\"train_acc\"], label=\"Train Acc\")\n",
        "    plt.plot(epochs, history[\"val_acc\"], label=\"Val Acc\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Accuracy\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion, scaler):\n",
        "    \"\"\"One training epoch over the dataloader.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = total_dice = total_iou = total_acc = 0.0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        x = batch[\"x\"].to(device)\n",
        "        y = batch[\"y\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Mixed precision forward pass\n",
        "        with torch.amp.autocast(\"cuda\", enabled=(device.type == \"cuda\")):\n",
        "            preds = model(x)\n",
        "            loss = criterion(preds, y)\n",
        "\n",
        "        # Backward with gradient scaling\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "        # Convert predictions to binary and compute metrics\n",
        "        preds_prob = (torch.sigmoid(preds).detach().cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        y_np = (y.detach().cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        d, i, a = calc_metrics(y_np, preds_prob)\n",
        "\n",
        "        total_dice += d\n",
        "        total_iou += i\n",
        "        total_acc += a\n",
        "\n",
        "    n = len(dataloader)\n",
        "    return total_loss / n, total_dice / n, total_iou / n, total_acc / n\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_one_epoch(model, dataloader, criterion, keep_last_batch=True):\n",
        "    \"\"\"Evaluation loop over the validation dataloader.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = total_dice = total_iou = total_acc = 0.0\n",
        "    last_batch_imgs = None\n",
        "\n",
        "    for batch in dataloader:\n",
        "        x = batch[\"x\"].to(device)\n",
        "        y = batch[\"y\"].to(device)\n",
        "\n",
        "        preds = model(x)\n",
        "        loss = criterion(preds, y)\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "        preds_prob = (torch.sigmoid(preds).cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        y_np = (y.cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        d, i, a = calc_metrics(y_np, preds_prob)\n",
        "\n",
        "        total_dice += d\n",
        "        total_iou += i\n",
        "        total_acc += a\n",
        "\n",
        "        # Optionally keep last batch for visualization\n",
        "        if keep_last_batch:\n",
        "            ch0 = x[:, 0:1].cpu().numpy()\n",
        "            imgs = {\"mri\": ch0, \"y_true\": y.cpu().numpy(), \"y_pred\": preds_prob}\n",
        "            if x.shape[1] == 2:\n",
        "                ch1 = x[:, 1:2].cpu().numpy()\n",
        "                imgs[\"regions\"] = ch1\n",
        "            last_batch_imgs = imgs\n",
        "\n",
        "    n = len(dataloader)\n",
        "    return total_loss / n, total_dice / n, total_iou / n, total_acc / n, last_batch_imgs\n",
        "\n",
        "def infer_and_visualize_best(\n",
        "    model,\n",
        "    val_dataset,\n",
        "    use_atlas: bool,\n",
        "    out_dir: str,\n",
        "    client_id: int,\n",
        "    best_ckpt_path: str,\n",
        "    k_samples: int = 3,\n",
        "    threshold: float = 0.5,\n",
        "):\n",
        "    \"\"\"Load best checkpoint, run inference on a few validation samples, and save visualizations.\"\"\"\n",
        "    import random\n",
        "\n",
        "    # Make sampling deterministic\n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    if os.path.isfile(best_ckpt_path):\n",
        "        model.load_state_dict(torch.load(best_ckpt_path, map_location=device))\n",
        "        model.eval()\n",
        "        print(f\"[Best Model] Loaded: {best_ckpt_path}\\n\")\n",
        "    else:\n",
        "        print(f\"[Best Model] Missing checkpoint: {best_ckpt_path}\\n\")\n",
        "        return\n",
        "\n",
        "    k = min(k_samples, len(val_dataset))\n",
        "    if k == 0:\n",
        "        print(\"[Best Model] Empty val dataset.\\n\")\n",
        "        return\n",
        "\n",
        "    # Deterministic random subset of validation indices\n",
        "    idxs = random.sample(range(len(val_dataset)), k)\n",
        "\n",
        "    def _predict_one(sample):\n",
        "        \"\"\"Run model on one sample and return inputs and binarized prediction.\"\"\"\n",
        "        mri = torch.tensor(sample[\"mri\"]).unsqueeze(0).unsqueeze(0).float().to(device)\n",
        "\n",
        "        if use_atlas and (\"regions\" in sample):\n",
        "            regs = torch.tensor(sample[\"regions\"]).unsqueeze(0).unsqueeze(0).float().to(device)\n",
        "            x = torch.cat([mri, regs], dim=1)\n",
        "        else:\n",
        "            x = mri\n",
        "\n",
        "        with torch.no_grad():\n",
        "            prob = torch.sigmoid(model(x)).cpu().numpy()\n",
        "            pred_bin = (prob > threshold).astype(np.uint8)\n",
        "\n",
        "        return x.cpu().numpy(), pred_bin\n",
        "\n",
        "    saved_paths = []\n",
        "\n",
        "    # Save individual sample figures\n",
        "    for i, idx in enumerate(idxs, 1):\n",
        "        sample = val_dataset[idx]\n",
        "        pid = sample.get(\"patient_id\", f\"val_{idx}\")\n",
        "        x_np, pred_bin = _predict_one(sample)\n",
        "\n",
        "        imgs = {\n",
        "            \"mri\": x_np[:, 0:1],\n",
        "            \"y_true\": np.expand_dims(\n",
        "                np.expand_dims(sample[\"tumor\"], 0), 0\n",
        "            ).astype(np.float32),\n",
        "            \"y_pred\": pred_bin.astype(np.float32),\n",
        "        }\n",
        "\n",
        "        if use_atlas and (\"regions\" in sample):\n",
        "            imgs[\"regions\"] = x_np[:, 1:2]\n",
        "\n",
        "        out_path = os.path.join(\n",
        "            out_dir, f\"best_val_sample_{i}_client{client_id}_{pid}.png\"\n",
        "        )\n",
        "        plot_prediction(imgs, out_path)\n",
        "        saved_paths.append(out_path)\n",
        "\n",
        "    print(\"[Best Model] Saved individual figures:\")\n",
        "    for p in saved_paths:\n",
        "        print(\" -\", p)\n",
        "    print()\n",
        "\n",
        "    # Save grid figure\n",
        "    cols = 4 if use_atlas else 3\n",
        "    fig, axs = plt.subplots(k, cols, figsize=(5 * cols, 4 * k))\n",
        "\n",
        "    if k == 1:\n",
        "        axs = np.expand_dims(axs, 0)\n",
        "\n",
        "    for row, idx in enumerate(idxs):\n",
        "        sample = val_dataset[idx]\n",
        "        x_np, pred_bin = _predict_one(sample)\n",
        "\n",
        "        mri = x_np[0, 0]\n",
        "        gt = sample[\"tumor\"]\n",
        "        col = 0\n",
        "\n",
        "        axs[row, col].imshow(mri, cmap=\"gray\")\n",
        "        axs[row, col].set_title(\"MRI\")\n",
        "        axs[row, col].axis(\"off\")\n",
        "        col += 1\n",
        "\n",
        "        if use_atlas and (\"regions\" in sample):\n",
        "            regs = x_np[0, 1]\n",
        "            axs[row, col].imshow(regs, cmap=\"gray\")\n",
        "            axs[row, col].set_title(\"Regions\")\n",
        "            axs[row, col].axis(\"off\")\n",
        "            col += 1\n",
        "\n",
        "        axs[row, col].imshow(gt, cmap=\"gray\")\n",
        "        axs[row, col].set_title(\"Ground Truth\")\n",
        "        axs[row, col].axis(\"off\")\n",
        "        col += 1\n",
        "\n",
        "        axs[row, col].imshow(pred_bin[0, 0], cmap=\"gray\")\n",
        "        axs[row, col].set_title(\"Predicted (τ=0.5)\")\n",
        "        axs[row, col].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    grid_path = os.path.join(out_dir, f\"best_model_val_grid_client{client_id}.png\")\n",
        "    plt.savefig(grid_path, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    print(f\"[Best Model] Saved grid -> {grid_path}\\n\")\n",
        "\n",
        "# History containers for training curves\n",
        "history = {\n",
        "    \"train_loss\": [],\n",
        "    \"train_dice\": [],\n",
        "    \"train_iou\": [],\n",
        "    \"train_acc\": [],\n",
        "    \"val_loss\": [],\n",
        "    \"val_dice\": [],\n",
        "    \"val_iou\": [],\n",
        "    \"val_acc\": [],\n",
        "}\n",
        "\n",
        "# Tracking best validation metrics\n",
        "best_val_iou = 0.0\n",
        "best_val_dice = -float(\"inf\")\n",
        "best_val_loss = float(\"inf\")\n",
        "best_path = os.path.join(OUT_MODELS_DIR, f\"best_unet_client{CLIENT_ID}.pth\")\n",
        "\n",
        "log_rows = []\n",
        "\n",
        "# Main training loop\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    # Train one epoch\n",
        "    trL, trD, trI, trA = train_one_epoch(\n",
        "        model, train_loader, optimizer, criterion, scaler\n",
        "    )\n",
        "    # Validate\n",
        "    vaL, vaD, vaI, vaA, _ = eval_one_epoch(\n",
        "        model, val_loader, criterion, keep_last_batch=True\n",
        "    )\n",
        "\n",
        "    # Update scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    # Save metrics for plots\n",
        "    history[\"train_loss\"].append(trL)\n",
        "    history[\"train_dice\"].append(trD)\n",
        "    history[\"train_iou\"].append(trI)\n",
        "    history[\"train_acc\"].append(trA)\n",
        "\n",
        "    history[\"val_loss\"].append(vaL)\n",
        "    history[\"val_dice\"].append(vaD)\n",
        "    history[\"val_iou\"].append(vaI)\n",
        "    history[\"val_acc\"].append(vaA)\n",
        "\n",
        "    print(\n",
        "        f\"[Epoch {epoch:03d}/{EPOCHS}] \"\n",
        "        f\"Train — Loss {trL:.4f} | Dice {trD:.4f} | IoU {trI:.4f} | Accuracy {trA:.4f} || \"\n",
        "        f\"Val — Loss {vaL:.4f} | Dice {vaD:.4f} | IoU {vaI:.4f} | Accuracy {vaA:.4f}\\n\"\n",
        "    )\n",
        "\n",
        "    # Save checkpoint if Dice improves and loss decreases\n",
        "    saved_ckpt = False\n",
        "    if (vaD > best_val_dice) and (vaL < best_val_loss):\n",
        "        best_val_dice = vaD\n",
        "        best_val_loss = vaL\n",
        "        torch.save(model.state_dict(), best_path)\n",
        "        saved_ckpt = True\n",
        "        print(\n",
        "            f\"Saved best model (Val Dice↑ {best_val_dice:.4f} & Val Loss↓ {best_val_loss:.4f}) -> {best_path}\\n\"\n",
        "        )\n",
        "\n",
        "    # Log row for CSV\n",
        "    log_rows.append(\n",
        "        {\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": trL,\n",
        "            \"train_dice\": trD,\n",
        "            \"train_iou\": trI,\n",
        "            \"train_acc\": trA,\n",
        "            \"val_loss\": vaL,\n",
        "            \"val_dice\": vaD,\n",
        "            \"val_iou\": vaI,\n",
        "            \"val_acc\": vaA,\n",
        "            \"saved_ckpt\": saved_ckpt,\n",
        "            \"marker\": \"X\" if saved_ckpt else \"\",\n",
        "        }\n",
        "    )\n",
        "\n",
        "# Save metrics as CSV\n",
        "metrics_csv = os.path.join(OUT_GRAPHS_DIR, f\"metrics_client{CLIENT_ID}.csv\")\n",
        "pd.DataFrame(log_rows).to_csv(metrics_csv, index=False)\n",
        "print(f\"[Log] Wrote per-epoch metrics CSV -> {metrics_csv}\\n\")\n",
        "\n",
        "# Plot training curves\n",
        "plot_metrics(history, os.path.join(OUT_GRAPHS_DIR, f\"training_curves_client{CLIENT_ID}.png\"))\n",
        "\n",
        "# Run inference with best model and visualize a few validation samples\n",
        "infer_and_visualize_best(\n",
        "    model=model,\n",
        "    val_dataset=val_dataset,\n",
        "    use_atlas=USE_ATLAS,\n",
        "    out_dir=OUT_GRAPHS_DIR,\n",
        "    client_id=CLIENT_ID,\n",
        "    best_ckpt_path=best_path,\n",
        "    k_samples=3,\n",
        "    threshold=0.5,\n",
        ")"
      ],
      "metadata": {
        "id": "RKHibbXVvLNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **M1 - Federated Learning Setup**"
      ],
      "metadata": {
        "id": "zeATNHiwE86m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Federated Experiment<h2>\n",
        "\n",
        "**with Flower**"
      ],
      "metadata": {
        "id": "g0BCxaFfYUhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile seg_data.py\n",
        "import os, pickle, numpy as np, torch\n",
        "from typing import List\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Global paths and configuration\n",
        "DATA_ROOT = \"/content/Preprocessed-Data\"\n",
        "METADATA_DF_PATH = \"cleaned_df.pkl\"\n",
        "USE_ATLAS = True\n",
        "EXCLUDE_IDS = [\"PatientID_0191\"]\n",
        "\n",
        "# Dataset that loads MRI, tumor mask and optional atlas for each patient\n",
        "class ImageOnlyGliomaDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        metadata_df_path=METADATA_DF_PATH,\n",
        "        data_root=DATA_ROOT,\n",
        "        use_atlas=USE_ATLAS,\n",
        "        exclude_ids=None,\n",
        "        transform=None,\n",
        "    ):\n",
        "        with open(metadata_df_path, \"rb\") as f:\n",
        "            df = pickle.load(f)\n",
        "\n",
        "        # Optionally exclude specific patients\n",
        "        if exclude_ids is None:\n",
        "            exclude_ids = EXCLUDE_IDS\n",
        "\n",
        "        # Keep only non-excluded patient rows\n",
        "        self.df = df[~df[\"Patient_ID\"].isin(exclude_ids)].reset_index(drop=True)\n",
        "        self.data_root = data_root\n",
        "        self.use_atlas = use_atlas\n",
        "        self.transform = transform\n",
        "\n",
        "        # Collect patient IDs that have all required .npy files\n",
        "        self.patient_ids = []\n",
        "        for pid in sorted(self.df[\"Patient_ID\"].tolist()):\n",
        "            base = os.path.join(self.data_root, pid)\n",
        "            mri_p = os.path.join(base, f\"{pid}_mri.npy\")\n",
        "            tumor_p = os.path.join(base, f\"{pid}_tumor.npy\")\n",
        "            if self.use_atlas:\n",
        "                reg_p = os.path.join(base, f\"{pid}_regions.npy\")\n",
        "                ok = os.path.isfile(mri_p) and os.path.isfile(tumor_p) and os.path.isfile(reg_p)\n",
        "            else:\n",
        "                ok = os.path.isfile(mri_p) and os.path.isfile(tumor_p)\n",
        "            if ok:\n",
        "                self.patient_ids.append(pid)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patient_ids)\n",
        "\n",
        "    # Simple min–max normalization\n",
        "    @staticmethod\n",
        "    def _minmax(x):\n",
        "        x = x.astype(np.float32)\n",
        "        mn, mx = np.min(x), np.max(x)\n",
        "        return (x - mn) / (mx - mn) if mx > mn else np.zeros_like(x, dtype=np.float32)\n",
        "\n",
        "    # Load a single sample: MRI, tumor mask, and optional atlas\n",
        "    def __getitem__(self, idx):\n",
        "        pid = self.patient_ids[idx]\n",
        "        base = os.path.join(self.data_root, pid)\n",
        "\n",
        "        mri = np.load(os.path.join(base, f\"{pid}_mri.npy\")).astype(np.float32)\n",
        "        tumor = np.load(os.path.join(base, f\"{pid}_tumor.npy\")).astype(np.float32)\n",
        "\n",
        "        mri = self._minmax(mri)\n",
        "        tumor = (tumor > 0.5).astype(np.float32)\n",
        "\n",
        "        sample = {\"patient_id\": pid, \"mri\": mri, \"tumor\": tumor}\n",
        "\n",
        "        if self.use_atlas:\n",
        "            regions = np.load(os.path.join(base, f\"{pid}_regions.npy\")).astype(np.float32)\n",
        "            regions = self._minmax(regions)\n",
        "            sample[\"regions\"] = regions\n",
        "\n",
        "        return sample\n",
        "\n",
        "# Collate function to build batched tensors and patient ID list\n",
        "def image_only_collate_fn(batch, use_atlas=USE_ATLAS):\n",
        "    mri = torch.stack([torch.tensor(it[\"mri\"]) for it in batch]).unsqueeze(1)\n",
        "    y = torch.stack([torch.tensor(it[\"tumor\"]) for it in batch]).unsqueeze(1)\n",
        "\n",
        "    if use_atlas:\n",
        "        regs = torch.stack([torch.tensor(it[\"regions\"]) for it in batch]).unsqueeze(1)\n",
        "        x = torch.cat([mri.float(), regs.float()], dim=1)\n",
        "    else:\n",
        "        x = mri.float()\n",
        "\n",
        "    return {\"x\": x, \"y\": y.float(), \"pid\": [it[\"patient_id\"] for it in batch]}\n",
        "\n",
        "# Dataset wrapper that restricts to a subset of patient IDs\n",
        "class SubsetByPIDs(Dataset):\n",
        "    def __init__(self, full_dataset: ImageOnlyGliomaDataset, pid_list: List[str]):\n",
        "        self.ds = full_dataset\n",
        "        pid_to_idx = {pid: i for i, pid in enumerate(self.ds.patient_ids)}\n",
        "        self.indices = [pid_to_idx[p] for p in pid_list if p in pid_to_idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.ds[self.indices[i]]\n",
        "\n",
        "# Compute Dice, IoU and accuracy for binary masks\n",
        "def calc_metrics(y_true, y_pred):\n",
        "    y_true = y_true.astype(np.uint8).reshape(-1)\n",
        "    y_pred = y_pred.astype(np.uint8).reshape(-1)\n",
        "\n",
        "    inter = (y_true & y_pred).sum()\n",
        "    dice = (2.0 * inter) / (y_true.sum() + y_pred.sum() + 1e-8)\n",
        "    union = y_true.sum() + y_pred.sum() - inter + 1e-8\n",
        "    iou = inter / union\n",
        "    acc = (y_true == y_pred).mean()\n",
        "\n",
        "    return float(dice), float(iou), float(acc)"
      ],
      "metadata": {
        "id": "bU6gtvgwWuNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile fl_client.py\n",
        "import argparse, os, json, numpy as np, torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import flwr as fl\n",
        "import copy\n",
        "import segmentation_models_pytorch as smp\n",
        "from seg_data import (\n",
        "    ImageOnlyGliomaDataset,\n",
        "    SubsetByPIDs,\n",
        "    image_only_collate_fn,\n",
        "    calc_metrics,\n",
        "    DATA_ROOT,\n",
        "    METADATA_DF_PATH,\n",
        "    USE_ATLAS,\n",
        ")\n",
        "\n",
        "# Base directory for client data splits\n",
        "CLIENT_DIR = \"/content/client\"\n",
        "\n",
        "# Data/loading config\n",
        "BATCH_SIZE = 8\n",
        "NUM_WORKERS = 2\n",
        "SEED = 42\n",
        "\n",
        "# Device and model config\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "ENCODER_NAME = \"resnet34\"\n",
        "ENCODER_WEIGHTS = \"imagenet\"\n",
        "\n",
        "# Local checkpoint directory (per client best models)\n",
        "CKPT_DIR = os.path.join(\"AITDM\", \"checkpoints\")\n",
        "os.makedirs(CKPT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "def get_model():\n",
        "    \"\"\"Create and return a UNet segmentation model.\"\"\"\n",
        "    in_ch = 2 if USE_ATLAS else 1\n",
        "    model = smp.Unet(\n",
        "        encoder_name=ENCODER_NAME,\n",
        "        encoder_weights=ENCODER_WEIGHTS,\n",
        "        in_channels=in_ch,\n",
        "        classes=1,\n",
        "    )\n",
        "    return model.to(DEVICE)\n",
        "\n",
        "\n",
        "def get_loaders(cid: int):\n",
        "    \"\"\"Build train/val dataloaders for a given client ID.\"\"\"\n",
        "    full = ImageOnlyGliomaDataset(\n",
        "        METADATA_DF_PATH, DATA_ROOT, use_atlas=USE_ATLAS, exclude_ids=[\"PatientID_0191\"]\n",
        "    )\n",
        "\n",
        "    # Load client-specific patient IDs\n",
        "    with open(os.path.join(CLIENT_DIR, f\"client_{cid}\", \"train_pids.json\")) as f:\n",
        "        tr_p = json.load(f)\n",
        "    with open(os.path.join(CLIENT_DIR, f\"client_{cid}\", \"val_pids.json\")) as f:\n",
        "        va_p = json.load(f)\n",
        "\n",
        "    g = torch.Generator().manual_seed(SEED)\n",
        "\n",
        "    # Subset datasets\n",
        "    ds_tr = SubsetByPIDs(full, tr_p)\n",
        "    ds_va = SubsetByPIDs(full, va_p)\n",
        "\n",
        "    # Train loader\n",
        "    ld_tr = DataLoader(\n",
        "        ds_tr,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "        collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "        generator=g,\n",
        "    )\n",
        "\n",
        "    # Validation loader\n",
        "    ld_va = DataLoader(\n",
        "        ds_va,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "        collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "        generator=g,\n",
        "    )\n",
        "\n",
        "    return ld_tr, ld_va, len(ds_tr), len(ds_va)\n",
        "\n",
        "\n",
        "def get_parameters(model):\n",
        "    \"\"\"Convert model parameters to a list of NumPy arrays (for Flower).\"\"\"\n",
        "    return [p.detach().cpu().numpy() for _, p in model.state_dict().items()]\n",
        "\n",
        "\n",
        "def set_parameters(model, params):\n",
        "    \"\"\"Load model parameters from a list of NumPy arrays (from Flower).\"\"\"\n",
        "    sd = model.state_dict()\n",
        "    for k, v in zip(sd.keys(), params):\n",
        "        sd[k] = torch.tensor(v)\n",
        "    model.load_state_dict(sd, strict=True)\n",
        "\n",
        "\n",
        "# Loss functions used for combined criterion\n",
        "bce = nn.BCEWithLogitsLoss()\n",
        "dice_loss = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
        "\n",
        "\n",
        "def criterion(pred, y):\n",
        "    \"\"\"Hybrid loss: BCE + Dice.\"\"\"\n",
        "    return 0.5 * bce(pred, y) + 0.5 * dice_loss(pred, y)\n",
        "\n",
        "\n",
        "def maybe_save_best(cid, val_loss, val_dice, best_epoch, rnd, model):\n",
        "    \"\"\"Save best local model (per client) based on validation loss and Dice.\"\"\"\n",
        "    best_json = os.path.join(CKPT_DIR, f\"client_{cid}_best.json\")\n",
        "    best_pt = os.path.join(CKPT_DIR, f\"client_{cid}_best.pt\")\n",
        "\n",
        "    # Default previous best values\n",
        "    prev = {\"val_loss\": float(\"inf\"), \"val_dice\": -1.0}\n",
        "    if os.path.isfile(best_json):\n",
        "        try:\n",
        "            with open(best_json, \"r\") as f:\n",
        "                prev = json.load(f)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Check if current model improved both loss and Dice\n",
        "    improved = (val_loss < prev.get(\"val_loss\", float(\"inf\"))) and (\n",
        "        val_dice > prev.get(\"val_dice\", -1.0)\n",
        "    )\n",
        "\n",
        "    if improved:\n",
        "        # Save state dict and metadata\n",
        "        torch.save(model.state_dict(), best_pt)\n",
        "        with open(best_json, \"w\") as f:\n",
        "            json.dump(\n",
        "                {\n",
        "                    \"round\": int(rnd),\n",
        "                    \"epoch\": int(best_epoch),\n",
        "                    \"val_loss\": float(val_loss),\n",
        "                    \"val_dice\": float(val_dice),\n",
        "                },\n",
        "                f,\n",
        "            )\n",
        "\n",
        "\n",
        "class SegClient(fl.client.NumPyClient):\n",
        "    \"\"\"Flower NumPyClient for federated glioma segmentation.\"\"\"\n",
        "\n",
        "    def __init__(self, cid: int):\n",
        "        self.cid = cid\n",
        "        self.model = get_model()\n",
        "        self.train_loader, self.val_loader, self.ntr, self.nva = get_loaders(cid)\n",
        "\n",
        "    def get_parameters(self, config):\n",
        "        \"\"\"Return current local model parameters.\"\"\"\n",
        "        return get_parameters(self.model)\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        \"\"\"Local training for a number of epochs, then return updated parameters and metrics.\"\"\"\n",
        "        # Load global parameters\n",
        "        set_parameters(self.model, parameters)\n",
        "\n",
        "        # Read fit configuration from server\n",
        "        epochs = int(config.get(\"local_epochs\", 1))\n",
        "        lr = float(config.get(\"lr\", 1e-3))\n",
        "        rnd = int(config.get(\"round\", 0))\n",
        "\n",
        "        # Optimizer and AMP scaler\n",
        "        opt = optim.AdamW(self.model.parameters(), lr=lr)\n",
        "        scaler = torch.amp.GradScaler(\"cuda\", enabled=(DEVICE.type == \"cuda\"))\n",
        "\n",
        "        # Track best validation performance\n",
        "        best_state = None\n",
        "        best_val_loss = float(\"inf\")\n",
        "        best_val_dice = -1.0\n",
        "        best_epoch_idx = -1\n",
        "\n",
        "        # Track best train metrics for that epoch (optional, for debugging)\n",
        "        best_train_loss = float(\"inf\")\n",
        "        best_train_dice = -1.0\n",
        "        best_train_iou = 0.0\n",
        "        best_train_acc = 0.0\n",
        "\n",
        "        # Per-epoch logs (will be sent to server)\n",
        "        epoch_logs = []\n",
        "\n",
        "        for epoch_idx in range(1, epochs + 1):\n",
        "            # ----- Training phase -----\n",
        "            self.model.train()\n",
        "            tot_tr_loss = tot_tr_d = tot_tr_i = tot_tr_a = 0.0\n",
        "            nb_tr = 0\n",
        "\n",
        "            for batch in self.train_loader:\n",
        "                x = batch[\"x\"].to(DEVICE)\n",
        "                y = batch[\"y\"].to(DEVICE)\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "\n",
        "                # Forward pass with mixed precision\n",
        "                with torch.amp.autocast(\"cuda\", enabled=(DEVICE.type == \"cuda\")):\n",
        "                    pred = self.model(x)\n",
        "                    loss = criterion(pred, y)\n",
        "\n",
        "                # Backward pass and optimizer step\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(opt)\n",
        "                scaler.update()\n",
        "\n",
        "                # Compute train metrics on this batch\n",
        "                with torch.no_grad():\n",
        "                    y_hat = (torch.sigmoid(pred).detach().cpu().numpy() > 0.5).astype(\n",
        "                        np.uint8\n",
        "                    )\n",
        "                    y_np = (y.detach().cpu().numpy() > 0.5).astype(np.uint8)\n",
        "                    d, i, a = calc_metrics(y_np, y_hat)\n",
        "\n",
        "                tot_tr_loss += float(loss.item())\n",
        "                tot_tr_d += d\n",
        "                tot_tr_i += i\n",
        "                tot_tr_a += a\n",
        "                nb_tr += 1\n",
        "\n",
        "            nb_tr = max(nb_tr, 1)\n",
        "            epoch_tr_loss = tot_tr_loss / nb_tr\n",
        "            epoch_tr_dice = tot_tr_d / nb_tr\n",
        "            epoch_tr_iou = tot_tr_i / nb_tr\n",
        "            epoch_tr_acc = tot_tr_a / nb_tr\n",
        "\n",
        "            # ----- Validation phase -----\n",
        "            self.model.eval()\n",
        "            tot_val_loss = tot_val_d = tot_val_i = tot_val_a = 0.0\n",
        "            nb_val = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in self.val_loader:\n",
        "                    x = batch[\"x\"].to(DEVICE)\n",
        "                    y = batch[\"y\"].to(DEVICE)\n",
        "                    pred = self.model(x)\n",
        "\n",
        "                    v_loss = float(criterion(pred, y).item())\n",
        "                    y_hat = (torch.sigmoid(pred).cpu().numpy() > 0.5).astype(np.uint8)\n",
        "                    y_np = (y.cpu().numpy() > 0.5).astype(np.uint8)\n",
        "                    d, i, a = calc_metrics(y_np, y_hat)\n",
        "\n",
        "                    tot_val_loss += v_loss\n",
        "                    tot_val_d += d\n",
        "                    tot_val_i += i\n",
        "                    tot_val_a += a\n",
        "                    nb_val += 1\n",
        "\n",
        "            nb_val = max(nb_val, 1)\n",
        "            epoch_val_loss = tot_val_loss / nb_val\n",
        "            epoch_val_dice = tot_val_d / nb_val\n",
        "            epoch_val_iou = tot_val_i / nb_val\n",
        "            epoch_val_acc = tot_val_a / nb_val\n",
        "\n",
        "            # Log per-epoch metrics\n",
        "            epoch_logs.append(\n",
        "                {\n",
        "                    \"epoch\": int(epoch_idx),\n",
        "                    \"train_loss\": float(epoch_tr_loss),\n",
        "                    \"train_dice\": float(epoch_tr_dice),\n",
        "                    \"train_iou\": float(epoch_tr_iou),\n",
        "                    \"train_acc\": float(epoch_tr_acc),\n",
        "                    \"val_loss\": float(epoch_val_loss),\n",
        "                    \"val_dice\": float(epoch_val_dice),\n",
        "                    \"val_iou\": float(epoch_val_iou),\n",
        "                    \"val_acc\": float(epoch_val_acc),\n",
        "                }\n",
        "            )\n",
        "\n",
        "            # Update best model based on validation metrics\n",
        "            if (epoch_val_loss < best_val_loss) and (epoch_val_dice > best_val_dice):\n",
        "                best_val_loss = epoch_val_loss\n",
        "                best_val_dice = epoch_val_dice\n",
        "                best_state = copy.deepcopy(self.model.state_dict())\n",
        "                best_epoch_idx = epoch_idx\n",
        "\n",
        "                best_train_loss = epoch_tr_loss\n",
        "                best_train_dice = epoch_tr_dice\n",
        "                best_train_iou = epoch_tr_iou\n",
        "                best_train_acc = epoch_tr_acc\n",
        "\n",
        "        # Load best local model state if available\n",
        "        if best_state is not None:\n",
        "            self.model.load_state_dict(best_state)\n",
        "\n",
        "        # Mark best epoch inside logs\n",
        "        for ep in epoch_logs:\n",
        "            ep[\"best_epoch\"] = (ep[\"epoch\"] == best_epoch_idx)\n",
        "\n",
        "        # Metrics sent back to server\n",
        "        train_metrics = {\n",
        "            \"cid\": int(self.cid),\n",
        "            \"best_epoch\": int(best_epoch_idx),\n",
        "            \"best_val_loss\": float(best_val_loss),\n",
        "            \"best_val_dice\": float(best_val_dice),\n",
        "            \"per_epoch\": json.dumps(epoch_logs),\n",
        "        }\n",
        "\n",
        "        # Save best local checkpoint\n",
        "        maybe_save_best(self.cid, best_val_loss, best_val_dice, best_epoch_idx, rnd, self.model)\n",
        "\n",
        "        # Return updated parameters, number of train examples, and metrics\n",
        "        return get_parameters(self.model), self.ntr, train_metrics\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "        \"\"\"Evaluate global parameters on local validation set.\"\"\"\n",
        "        # Load global parameters\n",
        "        set_parameters(self.model, parameters)\n",
        "        self.model.eval()\n",
        "\n",
        "        tot_loss = tot_d = tot_i = tot_a = 0.0\n",
        "        nb = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in self.val_loader:\n",
        "                x = batch[\"x\"].to(DEVICE)\n",
        "                y = batch[\"y\"].to(DEVICE)\n",
        "                pred = self.model(x)\n",
        "\n",
        "                loss = float(criterion(pred, y).item())\n",
        "                y_hat = (torch.sigmoid(pred).cpu().numpy() > 0.5).astype(np.uint8)\n",
        "                y_np = (y.cpu().numpy() > 0.5).astype(np.uint8)\n",
        "                d, i, a = calc_metrics(y_np, y_hat)\n",
        "\n",
        "                tot_loss += loss\n",
        "                tot_d += d\n",
        "                tot_i += i\n",
        "                tot_a += a\n",
        "                nb += 1\n",
        "\n",
        "        nb = max(nb, 1)\n",
        "        metrics = {\n",
        "            \"loss\": tot_loss / nb,\n",
        "            \"dice\": tot_d / nb,\n",
        "            \"iou\": tot_i / nb,\n",
        "            \"acc\": tot_a / nb,\n",
        "            \"cid\": int(self.cid),\n",
        "        }\n",
        "\n",
        "        # Flower expects (loss, num_examples, metrics)\n",
        "        return metrics[\"loss\"], self.nva, metrics\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Standalone client entry point (for non-simulation setups)\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--cid\", type=int, required=True)\n",
        "    parser.add_argument(\"--server\", default=\"0.0.0.0:8080\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(f\"[SegClient {args.cid}] device={DEVICE}, cuda={torch.cuda.is_available()}\")\n",
        "\n",
        "    fl.client.start_numpy_client(\n",
        "        server_address=args.server,\n",
        "        client=SegClient(args.cid),\n",
        "    )"
      ],
      "metadata": {
        "id": "kFmBJsv8Fr23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile fl_sim_colab.py\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "import torch\n",
        "import flwr as fl\n",
        "from flwr.common import FitIns\n",
        "from fl_client import SegClient\n",
        "\n",
        "# Base directory for outputs\n",
        "BASE_DIR = \"AITDM\"\n",
        "# Directory where per-client metrics will be saved\n",
        "METRICS_DIR = os.path.join(BASE_DIR, \"metrics\")\n",
        "os.makedirs(METRICS_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "def client_fn(cid: str):\n",
        "    \"\"\"Create a Flower client for a given client ID.\"\"\"\n",
        "    return SegClient(int(cid)).to_client()\n",
        "\n",
        "\n",
        "def ensure_csv(path: str, header: list[str]):\n",
        "    \"\"\"Create a CSV file with header if it does not exist.\"\"\"\n",
        "    if not os.path.isfile(path):\n",
        "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "        with open(path, \"w\", newline=\"\") as f:\n",
        "            csv.writer(f).writerow(header)\n",
        "\n",
        "\n",
        "def append_row(path: str, row: list):\n",
        "    \"\"\"Append a single row to a CSV file.\"\"\"\n",
        "    with open(path, \"a\", newline=\"\") as f:\n",
        "        csv.writer(f).writerow(row)\n",
        "\n",
        "\n",
        "class PerClientLoggingFedAvg(fl.server.strategy.FedAvg):\n",
        "    \"\"\"Custom FedAvg strategy that logs per-client metrics to CSV.\"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # CSV header for logged metrics\n",
        "        self.header = [\n",
        "            \"round\",\n",
        "            \"epoch\",\n",
        "            \"train_loss\",\n",
        "            \"train_dice\",\n",
        "            \"train_iou\",\n",
        "            \"train_acc\",\n",
        "            \"val_loss\",\n",
        "            \"val_dice\",\n",
        "            \"val_iou\",\n",
        "            \"val_acc\",\n",
        "            \"best_epoch\",\n",
        "        ]\n",
        "\n",
        "    def configure_fit(self, server_round, parameters, client_manager):\n",
        "        \"\"\"Inject current round number into fit configuration for each client.\"\"\"\n",
        "        items = super().configure_fit(server_round, parameters, client_manager)\n",
        "        out = []\n",
        "        for it in items:\n",
        "            if isinstance(it, tuple):\n",
        "                client, fitins = it\n",
        "            else:\n",
        "                client, fitins = None, it\n",
        "\n",
        "            cfg = dict(fitins.config)\n",
        "            cfg[\"round\"] = server_round\n",
        "            new_fitins = FitIns(fitins.parameters, cfg)\n",
        "\n",
        "            out.append((client, new_fitins) if client is not None else new_fitins)\n",
        "        return out\n",
        "\n",
        "    def aggregate_fit(self, rnd, results, failures):\n",
        "        \"\"\"Aggregate fit results and log per-epoch metrics for each client.\"\"\"\n",
        "        agg = super().aggregate_fit(rnd, results, failures)\n",
        "\n",
        "        for client_proxy, fit_res in results:\n",
        "            m = fit_res.metrics or {}\n",
        "            cid = str(m.get(\"cid\", client_proxy.cid))\n",
        "\n",
        "            client_csv = os.path.join(METRICS_DIR, f\"metrics_client_{cid}.csv\")\n",
        "            ensure_csv(client_csv, self.header)\n",
        "\n",
        "            best_epoch = int(m.get(\"best_epoch\", -1))\n",
        "            per_epoch_raw = m.get(\"per_epoch\", \"[]\")\n",
        "\n",
        "            # Parse per-epoch metrics sent from the client\n",
        "            try:\n",
        "                per_epoch = json.loads(per_epoch_raw)\n",
        "            except Exception:\n",
        "                per_epoch = []\n",
        "\n",
        "            # Write one row per local epoch\n",
        "            for ep in per_epoch:\n",
        "                epoch = ep.get(\"epoch\", \"\")\n",
        "                row = [\n",
        "                    rnd,\n",
        "                    epoch,\n",
        "                    ep.get(\"train_loss\", \"\"),\n",
        "                    ep.get(\"train_dice\", \"\"),\n",
        "                    ep.get(\"train_iou\", \"\"),\n",
        "                    ep.get(\"train_acc\", \"\"),\n",
        "                    ep.get(\"val_loss\", \"\"),\n",
        "                    ep.get(\"val_dice\", \"\"),\n",
        "                    ep.get(\"val_iou\", \"\"),\n",
        "                    ep.get(\"val_acc\", \"\"),\n",
        "                    \"x\" if int(epoch) == best_epoch else \"\",\n",
        "                ]\n",
        "                append_row(client_csv, row)\n",
        "\n",
        "        return agg\n",
        "\n",
        "\n",
        "# Federated learning strategy configuration\n",
        "strategy = PerClientLoggingFedAvg(\n",
        "    fraction_fit=1.0,\n",
        "    fraction_evaluate=1.0,\n",
        "    min_fit_clients=3,\n",
        "    min_evaluate_clients=3,\n",
        "    min_available_clients=3,\n",
        "    on_fit_config_fn=lambda rnd: {\"local_epochs\": 5, \"lr\": 1e-3},\n",
        ")\n",
        "\n",
        "# Resource configuration (GPU if available)\n",
        "use_gpu = torch.cuda.is_available()\n",
        "client_resources = {\"num_cpus\": 1, \"num_gpus\": 1.0 if use_gpu else 0.0}\n",
        "\n",
        "# Start Flower simulation with 3 clients and 5 communication rounds\n",
        "fl.simulation.start_simulation(\n",
        "    client_fn=client_fn,\n",
        "    num_clients=3,\n",
        "    config=fl.server.ServerConfig(num_rounds=5),\n",
        "    strategy=strategy,\n",
        "    client_resources=client_resources,\n",
        "    ray_init_args={\"include_dashboard\": False},\n",
        ")"
      ],
      "metadata": {
        "id": "IZxs3HOEF4Wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python fl_sim_colab.py"
      ],
      "metadata": {
        "id": "H3dlt740MYbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **M2 - Clip Model**"
      ],
      "metadata": {
        "id": "WzRm-EHu8iCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We train UNet for tumor segmentation, train BioClinicalBERT for region prediction in reports, then train a multimodal CLIP-like model that aligns text–image embeddings and, simultaneously, preserves segmentation through a combined loss.**"
      ],
      "metadata": {
        "id": "oA8ci1ByLTbg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>UNet on images<h2>**"
      ],
      "metadata": {
        "id": "v9EatFoMKv7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, pickle, random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "from sklearn.metrics import jaccard_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "# Paths and I/O config\n",
        "DATA_ROOT = \"/content/Preprocessed-Data\"\n",
        "METADATA_DF_PATH = \"cleaned_df.pkl\"\n",
        "OUT_BASE = \"AITDM\"\n",
        "CLIENT_DIR = os.path.join(OUT_BASE, \"/content/client\")\n",
        "\n",
        "# Experiment config\n",
        "USE_ATLAS = True\n",
        "CLIENT_ID = 0\n",
        "BATCH_SIZE = 8\n",
        "LR = 1e-3\n",
        "EPOCHS = 50\n",
        "NUM_WORKERS = 2\n",
        "SEED = 42\n",
        "\n",
        "# Model / encoder config\n",
        "ENCODER_NAME = \"resnet34\"\n",
        "ENCODER_WEIGHTS = \"imagenet\"\n",
        "\n",
        "# Output directories\n",
        "OUT_MODELS_DIR = os.path.join(OUT_BASE, \"Models\", \"UNet_ImageOnly\")\n",
        "OUT_GRAPHS_DIR = os.path.join(OUT_BASE, \"Graphs\")\n",
        "os.makedirs(OUT_MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(OUT_GRAPHS_DIR, exist_ok=True)\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
        "\n",
        "def worker_init_fn(worker_id):\n",
        "    worker_seed = SEED + worker_id\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "set_seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class ImageOnlyGliomaDataset(Dataset):\n",
        "    \"\"\"Dataset that loads MRI, tumor mask, and optional atlas regions per patient.\"\"\"\n",
        "\n",
        "    def __init__(self, metadata_df_path, data_root, use_atlas=True, exclude_ids=None, transform=None):\n",
        "        import pickle, os, numpy as np, pandas as pd\n",
        "\n",
        "        # Load metadata dataframe\n",
        "        with open(metadata_df_path, \"rb\") as f:\n",
        "            df = pickle.load(f)\n",
        "\n",
        "        # Optionally exclude some patient IDs\n",
        "        if exclude_ids is None:\n",
        "            exclude_ids = [\"PatientID_0191\"]\n",
        "        self.df = df[~df[\"Patient_ID\"].isin(exclude_ids)].reset_index(drop=True)\n",
        "\n",
        "        self.data_root = data_root\n",
        "        self.use_atlas = use_atlas\n",
        "        self.transform = transform\n",
        "\n",
        "        # Collect patient IDs that have all required files\n",
        "        self.patient_ids = []\n",
        "        for pid in sorted(self.df[\"Patient_ID\"].tolist()):\n",
        "            mri_p = os.path.join(self.data_root, pid, f\"{pid}_mri.npy\")\n",
        "            tumor_p = os.path.join(self.data_root, pid, f\"{pid}_tumor.npy\")\n",
        "\n",
        "            if self.use_atlas:\n",
        "                reg_p = os.path.join(self.data_root, pid, f\"{pid}_regions.npy\")\n",
        "                if os.path.isfile(mri_p) and os.path.isfile(tumor_p) and os.path.isfile(reg_p):\n",
        "                    self.patient_ids.append(pid)\n",
        "            else:\n",
        "                if os.path.isfile(mri_p) and os.path.isfile(tumor_p):\n",
        "                    self.patient_ids.append(pid)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patient_ids)\n",
        "\n",
        "    @staticmethod\n",
        "    def _minmax(x):\n",
        "        \"\"\"Simple min-max normalization to [0, 1].\"\"\"\n",
        "        x = x.astype(np.float32)\n",
        "        mn, mx = np.min(x), np.max(x)\n",
        "        return (x - mn) / (mx - mn) if mx > mn else np.zeros_like(x, dtype=np.float32)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Load and return one sample (dict) for a patient.\"\"\"\n",
        "        import os, numpy as np\n",
        "\n",
        "        pid = self.patient_ids[idx]\n",
        "        base = os.path.join(self.data_root, pid)\n",
        "\n",
        "        # Load MRI and tumor mask\n",
        "        mri = np.load(os.path.join(base, f\"{pid}_mri.npy\")).astype(np.float32)\n",
        "        tumor = np.load(os.path.join(base, f\"{pid}_tumor.npy\")).astype(np.float32)\n",
        "\n",
        "        # Normalize MRI and binarize tumor mask\n",
        "        mri = self._minmax(mri)\n",
        "        tumor = (tumor > 0.5).astype(np.float32)\n",
        "\n",
        "        sample = {\"patient_id\": pid, \"mri\": mri, \"tumor\": tumor}\n",
        "\n",
        "        # Optionally load regions/atlas\n",
        "        if self.use_atlas:\n",
        "            regions = np.load(os.path.join(base, f\"{pid}_regions.npy\")).astype(np.float32)\n",
        "            regions = self._minmax(regions)\n",
        "            sample[\"regions\"] = regions\n",
        "\n",
        "        return sample\n",
        "\n",
        "def image_only_collate_fn(batch, use_atlas=True):\n",
        "    \"\"\"Custom collate: stack MRI (+ optional regions) and tumor into tensors.\"\"\"\n",
        "    mri = torch.stack([torch.tensor(it[\"mri\"]) for it in batch]).unsqueeze(1)\n",
        "    y = torch.stack([torch.tensor(it[\"tumor\"]) for it in batch]).unsqueeze(1)\n",
        "\n",
        "    if use_atlas:\n",
        "        regs = torch.stack([torch.tensor(it[\"regions\"]) for it in batch]).unsqueeze(1)\n",
        "        # Concatenate MRI and regions as channels\n",
        "        x = torch.cat([mri.float(), regs.float()], dim=1)\n",
        "    else:\n",
        "        x = mri.float()\n",
        "\n",
        "    return {\"x\": x, \"y\": y.float(), \"pid\": [it[\"patient_id\"] for it in batch]}\n",
        "\n",
        "class SubsetByPIDs(Dataset):\n",
        "    \"\"\"Wrap a dataset but keep only a subset of patient IDs.\"\"\"\n",
        "\n",
        "    def __init__(self, full_dataset: ImageOnlyGliomaDataset, pid_list: List[str]):\n",
        "        self.ds = full_dataset\n",
        "        # Map patient IDs to indices\n",
        "        pid_to_idx = {pid: i for i, pid in enumerate(self.ds.patient_ids)}\n",
        "        self.indices = [pid_to_idx[p] for p in pid_list if p in pid_to_idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.ds[self.indices[i]]\n",
        "\n",
        "# Load client-specific train/val patient IDs\n",
        "cdir = os.path.join(CLIENT_DIR, f\"client_{CLIENT_ID}\")\n",
        "with open(os.path.join(cdir, \"train_pids.json\"), \"r\") as f:\n",
        "    train_pids = json.load(f)\n",
        "with open(os.path.join(cdir, \"val_pids.json\"), \"r\") as f:\n",
        "    val_pids = json.load(f)\n",
        "\n",
        "# Build full dataset and then client subsets\n",
        "full_ds = ImageOnlyGliomaDataset(\n",
        "    METADATA_DF_PATH,\n",
        "    DATA_ROOT,\n",
        "    use_atlas=USE_ATLAS,\n",
        "    exclude_ids=[\"PatientID_0191\"],\n",
        ")\n",
        "train_dataset = SubsetByPIDs(full_ds, train_pids)\n",
        "val_dataset = SubsetByPIDs(full_ds, val_pids)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "    generator=torch.Generator().manual_seed(SEED),\n",
        "    worker_init_fn=worker_init_fn,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "    generator=torch.Generator().manual_seed(SEED),\n",
        "    worker_init_fn=worker_init_fn,\n",
        ")\n",
        "\n",
        "print(f\"Loaded client_{CLIENT_ID}: train patients={len(train_dataset)}, val patients={len(val_dataset)}\")\n",
        "\n",
        "# UNet model (from segmentation_models_pytorch)\n",
        "in_channels = 2 if USE_ATLAS else 1\n",
        "model = smp.Unet(\n",
        "    encoder_name=ENCODER_NAME,\n",
        "    encoder_weights=ENCODER_WEIGHTS,\n",
        "    in_channels=in_channels,\n",
        "    classes=1,\n",
        ").to(device)\n",
        "\n",
        "# Loss, optimizer, scheduler, and AMP scaler\n",
        "criterion = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
        "scheduler = optim.lr_scheduler.LinearLR(\n",
        "    optimizer, start_factor=1.0, end_factor=0.0, total_iters=EPOCHS\n",
        ")\n",
        "scaler = torch.amp.GradScaler(\"cuda\", enabled=(device.type == \"cuda\"))\n",
        "\n",
        "def calc_metrics(y_true, y_pred):\n",
        "    \"\"\"Compute Dice, IoU, and pixel accuracy.\"\"\"\n",
        "    y_true = y_true.astype(np.uint8).reshape(-1)\n",
        "    y_pred = y_pred.astype(np.uint8).reshape(-1)\n",
        "\n",
        "    inter = (y_true * y_pred).sum()\n",
        "    dice = (2.0 * inter) / (y_true.sum() + y_pred.sum() + 1e-8)\n",
        "\n",
        "    # IoU with sklearn, fallback if it fails\n",
        "    try:\n",
        "        iou = jaccard_score(y_true, y_pred, average=\"binary\")\n",
        "    except Exception:\n",
        "        union = y_true.sum() + y_pred.sum() - inter + 1e-8\n",
        "        iou = inter / union\n",
        "\n",
        "    acc = (y_true == y_pred).mean()\n",
        "    return float(dice), float(iou), float(acc)\n",
        "\n",
        "def plot_prediction(sample_imgs, save_path):\n",
        "    \"\"\"Plot MRI (+ optional regions), GT, and prediction for one batch.\"\"\"\n",
        "    if \"regions\" in sample_imgs:\n",
        "        fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
        "    else:\n",
        "        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # MRI\n",
        "    axs[0].imshow(sample_imgs[\"mri\"][0, 0], cmap=\"gray\")\n",
        "    axs[0].set_title(\"MRI\")\n",
        "    axs[0].axis(\"off\")\n",
        "\n",
        "    col = 1\n",
        "\n",
        "    # Regions (if available)\n",
        "    if \"regions\" in sample_imgs:\n",
        "        axs[col].imshow(sample_imgs[\"regions\"][0, 0], cmap=\"gray\")\n",
        "        axs[col].set_title(\"Regions\")\n",
        "        axs[col].axis(\"off\")\n",
        "        col += 1\n",
        "\n",
        "    # Ground truth\n",
        "    axs[col].imshow(sample_imgs[\"y_true\"][0, 0], cmap=\"gray\")\n",
        "    axs[col].set_title(\"Ground Truth\")\n",
        "    axs[col].axis(\"off\")\n",
        "    col += 1\n",
        "\n",
        "    # Prediction\n",
        "    axs[col].imshow(sample_imgs[\"y_pred\"][0, 0], cmap=\"gray\")\n",
        "    axs[col].set_title(\"Predicted\")\n",
        "    axs[col].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def plot_metrics(history, save_path):\n",
        "    \"\"\"Plot training and validation curves for loss and metrics.\"\"\"\n",
        "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
        "\n",
        "    plt.figure(figsize=(16, 12))\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\")\n",
        "    plt.plot(epochs, history[\"val_loss\"], label=\"Val Loss\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Loss\")\n",
        "\n",
        "    # Dice\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(epochs, history[\"train_dice\"], label=\"Train Dice\")\n",
        "    plt.plot(epochs, history[\"val_dice\"], label=\"Val Dice\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Dice\")\n",
        "\n",
        "    # IoU\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(epochs, history[\"train_iou\"], label=\"Train IoU\")\n",
        "    plt.plot(epochs, history[\"val_iou\"], label=\"Val IoU\")\n",
        "    plt.legend()\n",
        "    plt.title(\"IoU\")\n",
        "\n",
        "    # Accuracy\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(epochs, history[\"train_acc\"], label=\"Train Acc\")\n",
        "    plt.plot(epochs, history[\"val_acc\"], label=\"Val Acc\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Accuracy\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion, scaler):\n",
        "    \"\"\"One training epoch over the dataloader.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = total_dice = total_iou = total_acc = 0.0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        x = batch[\"x\"].to(device)\n",
        "        y = batch[\"y\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Mixed precision forward pass\n",
        "        with torch.amp.autocast(\"cuda\", enabled=(device.type == \"cuda\")):\n",
        "            preds = model(x)\n",
        "            loss = criterion(preds, y)\n",
        "\n",
        "        # Backward with gradient scaling\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "        # Convert predictions to binary and compute metrics\n",
        "        preds_prob = (torch.sigmoid(preds).detach().cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        y_np = (y.detach().cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        d, i, a = calc_metrics(y_np, preds_prob)\n",
        "\n",
        "        total_dice += d\n",
        "        total_iou += i\n",
        "        total_acc += a\n",
        "\n",
        "    n = len(dataloader)\n",
        "    return total_loss / n, total_dice / n, total_iou / n, total_acc / n\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_one_epoch(model, dataloader, criterion, keep_last_batch=True):\n",
        "    \"\"\"Evaluation loop over the validation dataloader.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = total_dice = total_iou = total_acc = 0.0\n",
        "    last_batch_imgs = None\n",
        "\n",
        "    for batch in dataloader:\n",
        "        x = batch[\"x\"].to(device)\n",
        "        y = batch[\"y\"].to(device)\n",
        "\n",
        "        preds = model(x)\n",
        "        loss = criterion(preds, y)\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "        preds_prob = (torch.sigmoid(preds).cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        y_np = (y.cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        d, i, a = calc_metrics(y_np, preds_prob)\n",
        "\n",
        "        total_dice += d\n",
        "        total_iou += i\n",
        "        total_acc += a\n",
        "\n",
        "        # Optionally keep last batch for visualization\n",
        "        if keep_last_batch:\n",
        "            ch0 = x[:, 0:1].cpu().numpy()\n",
        "            imgs = {\"mri\": ch0, \"y_true\": y.cpu().numpy(), \"y_pred\": preds_prob}\n",
        "            if x.shape[1] == 2:\n",
        "                ch1 = x[:, 1:2].cpu().numpy()\n",
        "                imgs[\"regions\"] = ch1\n",
        "            last_batch_imgs = imgs\n",
        "\n",
        "    n = len(dataloader)\n",
        "    return total_loss / n, total_dice / n, total_iou / n, total_acc / n, last_batch_imgs\n",
        "\n",
        "def infer_and_visualize_best(\n",
        "    model,\n",
        "    val_dataset,\n",
        "    use_atlas: bool,\n",
        "    out_dir: str,\n",
        "    client_id: int,\n",
        "    best_ckpt_path: str,\n",
        "    k_samples: int = 3,\n",
        "    threshold: float = 0.5,\n",
        "):\n",
        "    \"\"\"Load best checkpoint, run inference on a few validation samples, and save visualizations.\"\"\"\n",
        "    import random\n",
        "\n",
        "    # Make sampling deterministic\n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    if os.path.isfile(best_ckpt_path):\n",
        "        model.load_state_dict(torch.load(best_ckpt_path, map_location=device))\n",
        "        model.eval()\n",
        "        print(f\"[Best Model] Loaded: {best_ckpt_path}\\n\")\n",
        "    else:\n",
        "        print(f\"[Best Model] Missing checkpoint: {best_ckpt_path}\\n\")\n",
        "        return\n",
        "\n",
        "    k = min(k_samples, len(val_dataset))\n",
        "    if k == 0:\n",
        "        print(\"[Best Model] Empty val dataset.\\n\")\n",
        "        return\n",
        "\n",
        "    # Deterministic random subset of validation indices\n",
        "    idxs = random.sample(range(len(val_dataset)), k)\n",
        "\n",
        "    def _predict_one(sample):\n",
        "        \"\"\"Run model on one sample and return inputs and binarized prediction.\"\"\"\n",
        "        mri = torch.tensor(sample[\"mri\"]).unsqueeze(0).unsqueeze(0).float().to(device)\n",
        "\n",
        "        if use_atlas and (\"regions\" in sample):\n",
        "            regs = torch.tensor(sample[\"regions\"]).unsqueeze(0).unsqueeze(0).float().to(device)\n",
        "            x = torch.cat([mri, regs], dim=1)\n",
        "        else:\n",
        "            x = mri\n",
        "\n",
        "        with torch.no_grad():\n",
        "            prob = torch.sigmoid(model(x)).cpu().numpy()\n",
        "            pred_bin = (prob > threshold).astype(np.uint8)\n",
        "\n",
        "        return x.cpu().numpy(), pred_bin\n",
        "\n",
        "    saved_paths = []\n",
        "\n",
        "    # Save individual sample figures\n",
        "    for i, idx in enumerate(idxs, 1):\n",
        "        sample = val_dataset[idx]\n",
        "        pid = sample.get(\"patient_id\", f\"val_{idx}\")\n",
        "        x_np, pred_bin = _predict_one(sample)\n",
        "\n",
        "        imgs = {\n",
        "            \"mri\": x_np[:, 0:1],\n",
        "            \"y_true\": np.expand_dims(\n",
        "                np.expand_dims(sample[\"tumor\"], 0), 0\n",
        "            ).astype(np.float32),\n",
        "            \"y_pred\": pred_bin.astype(np.float32),\n",
        "        }\n",
        "\n",
        "        if use_atlas and (\"regions\" in sample):\n",
        "            imgs[\"regions\"] = x_np[:, 1:2]\n",
        "\n",
        "        out_path = os.path.join(\n",
        "            out_dir, f\"best_val_sample_{i}_client{client_id}_{pid}.png\"\n",
        "        )\n",
        "        plot_prediction(imgs, out_path)\n",
        "        saved_paths.append(out_path)\n",
        "\n",
        "    print(\"[Best Model] Saved individual figures:\")\n",
        "    for p in saved_paths:\n",
        "        print(\" -\", p)\n",
        "    print()\n",
        "\n",
        "    # Save grid figure\n",
        "    cols = 4 if use_atlas else 3\n",
        "    fig, axs = plt.subplots(k, cols, figsize=(5 * cols, 4 * k))\n",
        "\n",
        "    if k == 1:\n",
        "        axs = np.expand_dims(axs, 0)\n",
        "\n",
        "    for row, idx in enumerate(idxs):\n",
        "        sample = val_dataset[idx]\n",
        "        x_np, pred_bin = _predict_one(sample)\n",
        "\n",
        "        mri = x_np[0, 0]\n",
        "        gt = sample[\"tumor\"]\n",
        "        col = 0\n",
        "\n",
        "        axs[row, col].imshow(mri, cmap=\"gray\")\n",
        "        axs[row, col].set_title(\"MRI\")\n",
        "        axs[row, col].axis(\"off\")\n",
        "        col += 1\n",
        "\n",
        "        if use_atlas and (\"regions\" in sample):\n",
        "            regs = x_np[0, 1]\n",
        "            axs[row, col].imshow(regs, cmap=\"gray\")\n",
        "            axs[row, col].set_title(\"Regions\")\n",
        "            axs[row, col].axis(\"off\")\n",
        "            col += 1\n",
        "\n",
        "        axs[row, col].imshow(gt, cmap=\"gray\")\n",
        "        axs[row, col].set_title(\"Ground Truth\")\n",
        "        axs[row, col].axis(\"off\")\n",
        "        col += 1\n",
        "\n",
        "        axs[row, col].imshow(pred_bin[0, 0], cmap=\"gray\")\n",
        "        axs[row, col].set_title(\"Predicted (τ=0.5)\")\n",
        "        axs[row, col].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    grid_path = os.path.join(out_dir, f\"best_model_val_grid_client{client_id}.png\")\n",
        "    plt.savefig(grid_path, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    print(f\"[Best Model] Saved grid -> {grid_path}\\n\")\n",
        "\n",
        "# History containers for training curves\n",
        "history = {\n",
        "    \"train_loss\": [],\n",
        "    \"train_dice\": [],\n",
        "    \"train_iou\": [],\n",
        "    \"train_acc\": [],\n",
        "    \"val_loss\": [],\n",
        "    \"val_dice\": [],\n",
        "    \"val_iou\": [],\n",
        "    \"val_acc\": [],\n",
        "}\n",
        "\n",
        "# Tracking best validation metrics\n",
        "best_val_iou = 0.0\n",
        "best_val_dice = -float(\"inf\")\n",
        "best_val_loss = float(\"inf\")\n",
        "best_path = os.path.join(OUT_MODELS_DIR, f\"best_unet_client{CLIENT_ID}.pth\")\n",
        "\n",
        "log_rows = []\n",
        "\n",
        "# Main training loop\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    # Train one epoch\n",
        "    trL, trD, trI, trA = train_one_epoch(\n",
        "        model, train_loader, optimizer, criterion, scaler\n",
        "    )\n",
        "    # Validate\n",
        "    vaL, vaD, vaI, vaA, _ = eval_one_epoch(\n",
        "        model, val_loader, criterion, keep_last_batch=True\n",
        "    )\n",
        "\n",
        "    # Update scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    # Save metrics for plots\n",
        "    history[\"train_loss\"].append(trL)\n",
        "    history[\"train_dice\"].append(trD)\n",
        "    history[\"train_iou\"].append(trI)\n",
        "    history[\"train_acc\"].append(trA)\n",
        "\n",
        "    history[\"val_loss\"].append(vaL)\n",
        "    history[\"val_dice\"].append(vaD)\n",
        "    history[\"val_iou\"].append(vaI)\n",
        "    history[\"val_acc\"].append(vaA)\n",
        "\n",
        "    print(\n",
        "        f\"[Epoch {epoch:03d}/{EPOCHS}] \"\n",
        "        f\"Train — Loss {trL:.4f} | Dice {trD:.4f} | IoU {trI:.4f} | Accuracy {trA:.4f} || \"\n",
        "        f\"Val — Loss {vaL:.4f} | Dice {vaD:.4f} | IoU {vaI:.4f} | Accuracy {vaA:.4f}\\n\"\n",
        "    )\n",
        "\n",
        "    # Save checkpoint if Dice improves and loss decreases\n",
        "    saved_ckpt = False\n",
        "    if (vaD > best_val_dice) and (vaL < best_val_loss):\n",
        "        best_val_dice = vaD\n",
        "        best_val_loss = vaL\n",
        "        torch.save(model.state_dict(), best_path)\n",
        "        saved_ckpt = True\n",
        "        print(\n",
        "            f\"Saved best model (Val Dice↑ {best_val_dice:.4f} & Val Loss↓ {best_val_loss:.4f}) -> {best_path}\\n\"\n",
        "        )\n",
        "\n",
        "    # Log row for CSV\n",
        "    log_rows.append(\n",
        "        {\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": trL,\n",
        "            \"train_dice\": trD,\n",
        "            \"train_iou\": trI,\n",
        "            \"train_acc\": trA,\n",
        "            \"val_loss\": vaL,\n",
        "            \"val_dice\": vaD,\n",
        "            \"val_iou\": vaI,\n",
        "            \"val_acc\": vaA,\n",
        "            \"saved_ckpt\": saved_ckpt,\n",
        "            \"marker\": \"X\" if saved_ckpt else \"\",\n",
        "        }\n",
        "    )\n",
        "\n",
        "# Save metrics as CSV\n",
        "metrics_csv = os.path.join(OUT_GRAPHS_DIR, f\"metrics_client{CLIENT_ID}.csv\")\n",
        "pd.DataFrame(log_rows).to_csv(metrics_csv, index=False)\n",
        "print(f\"[Log] Wrote per-epoch metrics CSV -> {metrics_csv}\\n\")\n",
        "\n",
        "# Plot training curves\n",
        "plot_metrics(history, os.path.join(OUT_GRAPHS_DIR, f\"training_curves_client{CLIENT_ID}.png\"))\n",
        "\n",
        "# Run inference with best model and visualize a few validation samples\n",
        "infer_and_visualize_best(\n",
        "    model=model,\n",
        "    val_dataset=val_dataset,\n",
        "    use_atlas=USE_ATLAS,\n",
        "    out_dir=OUT_GRAPHS_DIR,\n",
        "    client_id=CLIENT_ID,\n",
        "    best_ckpt_path=best_path,\n",
        "    k_samples=3,\n",
        "    threshold=0.5,\n",
        ")"
      ],
      "metadata": {
        "id": "PwxW3U1PKUbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>BioClinicalBERT on reports<h2>**"
      ],
      "metadata": {
        "id": "QKYalDg9K4tg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import random\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "\n",
        "warnings.simplefilter(\"ignore\", category=UndefinedMetricWarning)\n",
        "\n",
        "# ---- Reproducibility ----\n",
        "SEED = 42\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
        "\n",
        "# ---- Paths / Config ----\n",
        "METADATA_DF_PATH = \"/content/cleaned_df.pkl\"\n",
        "LABELS_PATH = \"/content/labels_list.pkl\"\n",
        "OUT_BASE = \"/content/AITDM\"\n",
        "CLIENT_BASE_DIR = \"/content/client\"\n",
        "\n",
        "CLIENT_ID = 0\n",
        "BATCH_SIZE = 8\n",
        "LR = 2e-5\n",
        "EPOCHS = 20\n",
        "NUM_WORKERS = 2\n",
        "MAX_LEN = 512\n",
        "WARMUP_RATIO = 0.1\n",
        "\n",
        "BERT_MODEL_NAME = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "DROPOUT = 0.3\n",
        "\n",
        "TOPK_PRED = 5\n",
        "USE_POS_WEIGHT = True\n",
        "REMOVE_BACKGROUND_LABEL = True\n",
        "\n",
        "OUT_MODELS_DIR = os.path.join(OUT_BASE, \"Models\", \"BioClinicalBert\")\n",
        "OUT_GRAPHS_DIR = os.path.join(OUT_BASE, \"Graphs\")\n",
        "os.makedirs(OUT_MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(OUT_GRAPHS_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "def set_seed(seed: int = 42) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
        "\n",
        "\n",
        "def worker_init_fn(worker_id: int) -> None:\n",
        "    s = SEED + worker_id\n",
        "    random.seed(s)\n",
        "    np.random.seed(s)\n",
        "\n",
        "\n",
        "set_seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class ReportsGliomaDataset(Dataset):\n",
        "    \"\"\"Multi-label dataset: report text -> set of region labels.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        metadata_df_path: str,\n",
        "        labels_path: str,\n",
        "        exclude_ids: Optional[List[str]] = None,\n",
        "        remove_background: bool = False,\n",
        "    ):\n",
        "        with open(metadata_df_path, \"rb\") as f:\n",
        "            df = pickle.load(f)\n",
        "\n",
        "        with open(labels_path, \"rb\") as f:\n",
        "            labels = list(pickle.load(f))\n",
        "\n",
        "        if remove_background:\n",
        "            labels = [l for l in labels if str(l).strip().lower() != \"background\"]\n",
        "\n",
        "        self.labels: List[str] = labels\n",
        "        self.label_to_idx: Dict[str, int] = {lab: i for i, lab in enumerate(self.labels)}\n",
        "\n",
        "        if exclude_ids is None:\n",
        "            exclude_ids = [\"PatientID_0191\"]\n",
        "\n",
        "        df = df[~df[\"Patient_ID\"].isin(exclude_ids)].reset_index(drop=True)\n",
        "        df = df[df[\"Report\"].notna() & df[\"Top 5 Regions\"].notna()].reset_index(drop=True)\n",
        "\n",
        "        self.df = df\n",
        "        self.patient_ids = df[\"Patient_ID\"].tolist()\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.patient_ids)\n",
        "\n",
        "    def _make_multilabel_target(self, regions_list) -> np.ndarray:\n",
        "        y = np.zeros(len(self.labels), dtype=np.float32)\n",
        "        if not isinstance(regions_list, (list, tuple)):\n",
        "            return y\n",
        "        for reg in regions_list:\n",
        "            idx = self.label_to_idx.get(reg, None)\n",
        "            if idx is not None:\n",
        "                y[idx] = 1.0\n",
        "        return y\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, object]:\n",
        "        row = self.df.iloc[idx]\n",
        "        pid = row[\"Patient_ID\"]\n",
        "        report = str(row[\"Report\"])\n",
        "        top5 = row[\"Top 5 Regions\"]\n",
        "        target = self._make_multilabel_target(top5)\n",
        "        return {\"patient_id\": pid, \"report\": report, \"target\": target}\n",
        "\n",
        "\n",
        "class SubsetByPIDs(Dataset):\n",
        "    \"\"\"Filter a dataset by a provided list of patient IDs.\"\"\"\n",
        "\n",
        "    def __init__(self, full_dataset: ReportsGliomaDataset, pid_list: List[str]):\n",
        "        self.ds = full_dataset\n",
        "        pid_to_idx = {pid: i for i, pid in enumerate(self.ds.patient_ids)}\n",
        "        self.indices = [pid_to_idx[p] for p in pid_list if p in pid_to_idx]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, i: int) -> Dict[str, object]:\n",
        "        return self.ds[self.indices[i]]\n",
        "\n",
        "\n",
        "def reports_collate_fn(batch: List[Dict[str, object]]) -> Dict[str, object]:\n",
        "    reports = [b[\"report\"] for b in batch]\n",
        "    targets = torch.from_numpy(np.stack([b[\"target\"] for b in batch])).float()\n",
        "    pids = [b[\"patient_id\"] for b in batch]\n",
        "    return {\"report\": reports, \"target\": targets, \"pid\": pids}\n",
        "\n",
        "\n",
        "class BioBERTMultiLabelClassifier(nn.Module):\n",
        "    \"\"\"BioClinicalBERT + linear head for multi-label prediction.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str, num_labels: int, dropout: float = 0.3):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled = outputs.pooler_output\n",
        "        if pooled is None:\n",
        "            pooled = outputs.last_hidden_state[:, 0]  # CLS\n",
        "        x = self.dropout(pooled)\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
        "\n",
        "\n",
        "def tokenize_reports(reports: List[str], max_len: int = 512) -> Dict[str, torch.Tensor]:\n",
        "    return tokenizer(\n",
        "        reports,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_len,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "\n",
        "def compute_pos_weight(train_dataset: Dataset, num_labels: int) -> torch.Tensor:\n",
        "    ys = np.stack([train_dataset[i][\"target\"] for i in range(len(train_dataset))]).astype(np.float32)\n",
        "    pos = ys.sum(axis=0)\n",
        "    neg = ys.shape[0] - pos\n",
        "    pw = (neg + 1e-6) / (pos + 1e-6)\n",
        "    pw = np.clip(pw, 1.0, 100.0)\n",
        "    return torch.tensor(pw, dtype=torch.float32)\n",
        "\n",
        "\n",
        "def preds_topk(probs: np.ndarray, k: int) -> np.ndarray:\n",
        "    k = max(1, min(k, probs.shape[1]))\n",
        "    pred = np.zeros_like(probs, dtype=np.int32)\n",
        "    topk_idx = np.argsort(-probs, axis=1)[:, :k]\n",
        "    rows = np.arange(probs.shape[0])[:, None]\n",
        "    pred[rows, topk_idx] = 1\n",
        "    return pred\n",
        "\n",
        "\n",
        "def precision_recall_at_k(y_true: np.ndarray, probs: np.ndarray, k: int) -> Tuple[float, float]:\n",
        "    pred = preds_topk(probs, k)\n",
        "    tp = (pred * y_true).sum(axis=1)\n",
        "    prec = (tp / (k + 1e-8)).mean()\n",
        "    true_pos = y_true.sum(axis=1)\n",
        "    rec = (tp / (true_pos + 1e-8)).mean()\n",
        "    return float(prec), float(rec)\n",
        "\n",
        "\n",
        "def safe_auc_per_class(y_true: np.ndarray, y_score: np.ndarray) -> np.ndarray:\n",
        "    num_labels = y_true.shape[1]\n",
        "    out = np.full(num_labels, np.nan, dtype=np.float32)\n",
        "    for j in range(num_labels):\n",
        "        col = y_true[:, j]\n",
        "        if col.min() == col.max():\n",
        "            continue\n",
        "        try:\n",
        "            out[j] = float(roc_auc_score(col, y_score[:, j]))\n",
        "        except Exception:\n",
        "            out[j] = np.nan\n",
        "    return out\n",
        "\n",
        "\n",
        "def plot_history(history: Dict[str, List[float]], save_dir: str, client_id: int, topk: int) -> None:\n",
        "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
        "\n",
        "    def _plot(tr_key: str, va_key: str, title: str, fname: str, ylabel: str):\n",
        "        plt.figure()\n",
        "        plt.plot(epochs, history[tr_key], label=\"Train\")\n",
        "        plt.plot(epochs, history[va_key], label=\"Val\")\n",
        "        plt.title(title)\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(ylabel)\n",
        "        plt.legend()\n",
        "        plt.savefig(os.path.join(save_dir, fname), bbox_inches=\"tight\")\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "    _plot(\"train_loss\", \"val_loss\", \"BioClinicalBERT Loss\", f\"biobert_loss_client{client_id}.png\", \"Loss\")\n",
        "    _plot(\"train_f1_macro_topk\", \"val_f1_macro_topk\", f\"F1 Macro (Top-{topk})\", f\"biobert_f1macro_topk_client{client_id}.png\", \"F1\")\n",
        "    _plot(\"train_f1_micro_topk\", \"val_f1_micro_topk\", f\"F1 Micro (Top-{topk})\", f\"biobert_f1micro_topk_client{client_id}.png\", \"F1\")\n",
        "    _plot(\"train_exact_match_topk\", \"val_exact_match_topk\", f\"Exact Match (Top-{topk})\", f\"biobert_exactmatch_topk_client{client_id}.png\", \"Exact Match\")\n",
        "\n",
        "\n",
        "def train_eval_biobert(\n",
        "    model: nn.Module,\n",
        "    train_loader: DataLoader,\n",
        "    val_loader: DataLoader,\n",
        "    label_names: List[str],\n",
        "    train_dataset: Dataset,\n",
        "    epochs: int = 20,\n",
        "    lr: float = 2e-5,\n",
        "    warmup_ratio: float = 0.1,\n",
        "    max_len: int = 512,\n",
        "    client_id: int = 0,\n",
        "    topk: int = 5,\n",
        "    use_pos_weight: bool = True,\n",
        ") -> Tuple[Dict[str, List[float]], str]:\n",
        "    num_labels = len(label_names)\n",
        "\n",
        "    pos_weight = compute_pos_weight(train_dataset, num_labels).to(device) if use_pos_weight else None\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight) if pos_weight is not None else nn.BCEWithLogitsLoss()\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    total_steps = len(train_loader) * epochs\n",
        "    warmup_steps = int(warmup_ratio * total_steps)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=warmup_steps,\n",
        "        num_training_steps=total_steps,\n",
        "    )\n",
        "\n",
        "    scaler = torch.amp.GradScaler(\"cuda\", enabled=(device.type == \"cuda\"))\n",
        "    model.to(device)\n",
        "\n",
        "    best_val_f1 = -1.0\n",
        "    best_path = os.path.join(OUT_MODELS_DIR, f\"best_biobert_client{client_id}.pt\")\n",
        "\n",
        "    history: Dict[str, List[float]] = {\n",
        "        \"train_loss\": [],\n",
        "        \"val_loss\": [],\n",
        "        \"train_f1_macro_topk\": [],\n",
        "        \"val_f1_macro_topk\": [],\n",
        "        \"train_f1_micro_topk\": [],\n",
        "        \"val_f1_micro_topk\": [],\n",
        "        \"train_f1_samples_topk\": [],\n",
        "        \"val_f1_samples_topk\": [],\n",
        "        \"train_exact_match_topk\": [],\n",
        "        \"val_exact_match_topk\": [],\n",
        "        \"train_p_at_k\": [],\n",
        "        \"val_p_at_k\": [],\n",
        "        \"train_r_at_k\": [],\n",
        "        \"val_r_at_k\": [],\n",
        "        \"train_f1_macro_thr\": [],\n",
        "        \"val_f1_macro_thr\": [],\n",
        "    }\n",
        "\n",
        "    log_rows: List[Dict[str, object]] = []\n",
        "\n",
        "    def _metrics(y_true: np.ndarray, probs: np.ndarray) -> Dict[str, float]:\n",
        "        pred_topk = preds_topk(probs, topk)\n",
        "        f1_macro = f1_score(y_true, pred_topk, average=\"macro\", zero_division=0)\n",
        "        f1_micro = f1_score(y_true, pred_topk, average=\"micro\", zero_division=0)\n",
        "        f1_samples = f1_score(y_true, pred_topk, average=\"samples\", zero_division=0)\n",
        "        exact_match = float((y_true == pred_topk).all(axis=1).mean())\n",
        "        p_at_k, r_at_k = precision_recall_at_k(y_true, probs, topk)\n",
        "        pred_thr = (probs > 0.5).astype(np.int32)\n",
        "        f1_macro_thr = f1_score(y_true, pred_thr, average=\"macro\", zero_division=0)\n",
        "        return {\n",
        "            \"f1_macro_topk\": float(f1_macro),\n",
        "            \"f1_micro_topk\": float(f1_micro),\n",
        "            \"f1_samples_topk\": float(f1_samples),\n",
        "            \"exact_match_topk\": float(exact_match),\n",
        "            \"p_at_k\": float(p_at_k),\n",
        "            \"r_at_k\": float(r_at_k),\n",
        "            \"f1_macro_thr\": float(f1_macro_thr),\n",
        "        }\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        # ---- Train ----\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        y_true_train, y_prob_train = [], []\n",
        "\n",
        "        for batch in train_loader:\n",
        "            reports = batch[\"report\"]\n",
        "            labels = batch[\"target\"].to(device, non_blocking=True)\n",
        "\n",
        "            enc = tokenize_reports(reports, max_len=max_len)\n",
        "            input_ids = enc[\"input_ids\"].to(device, non_blocking=True)\n",
        "            attention_mask = enc[\"attention_mask\"].to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            with torch.amp.autocast(\"cuda\", enabled=(device.type == \"cuda\")):\n",
        "                logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += float(loss.item())\n",
        "            probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "            y_prob_train.append(probs)\n",
        "            y_true_train.append(labels.detach().cpu().numpy())\n",
        "\n",
        "        y_true_train = np.concatenate(y_true_train, axis=0).astype(np.int32)\n",
        "        y_prob_train = np.concatenate(y_prob_train, axis=0)\n",
        "        trm = _metrics(y_true_train, y_prob_train)\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss / max(1, len(train_loader)))\n",
        "        history[\"train_f1_macro_topk\"].append(trm[\"f1_macro_topk\"])\n",
        "        history[\"train_f1_micro_topk\"].append(trm[\"f1_micro_topk\"])\n",
        "        history[\"train_f1_samples_topk\"].append(trm[\"f1_samples_topk\"])\n",
        "        history[\"train_exact_match_topk\"].append(trm[\"exact_match_topk\"])\n",
        "        history[\"train_p_at_k\"].append(trm[\"p_at_k\"])\n",
        "        history[\"train_r_at_k\"].append(trm[\"r_at_k\"])\n",
        "        history[\"train_f1_macro_thr\"].append(trm[\"f1_macro_thr\"])\n",
        "\n",
        "        # ---- Validation ----\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        y_true_val, y_prob_val = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                reports = batch[\"report\"]\n",
        "                labels = batch[\"target\"].to(device, non_blocking=True)\n",
        "\n",
        "                enc = tokenize_reports(reports, max_len=max_len)\n",
        "                input_ids = enc[\"input_ids\"].to(device, non_blocking=True)\n",
        "                attention_mask = enc[\"attention_mask\"].to(device, non_blocking=True)\n",
        "\n",
        "                logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                loss = criterion(logits, labels)\n",
        "                val_loss += float(loss.item())\n",
        "\n",
        "                probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "                y_prob_val.append(probs)\n",
        "                y_true_val.append(labels.detach().cpu().numpy())\n",
        "\n",
        "        y_true_val = np.concatenate(y_true_val, axis=0).astype(np.int32)\n",
        "        y_prob_val = np.concatenate(y_prob_val, axis=0)\n",
        "        vam = _metrics(y_true_val, y_prob_val)\n",
        "\n",
        "        history[\"val_loss\"].append(val_loss / max(1, len(val_loader)))\n",
        "        history[\"val_f1_macro_topk\"].append(vam[\"f1_macro_topk\"])\n",
        "        history[\"val_f1_micro_topk\"].append(vam[\"f1_micro_topk\"])\n",
        "        history[\"val_f1_samples_topk\"].append(vam[\"f1_samples_topk\"])\n",
        "        history[\"val_exact_match_topk\"].append(vam[\"exact_match_topk\"])\n",
        "        history[\"val_p_at_k\"].append(vam[\"p_at_k\"])\n",
        "        history[\"val_r_at_k\"].append(vam[\"r_at_k\"])\n",
        "        history[\"val_f1_macro_thr\"].append(vam[\"f1_macro_thr\"])\n",
        "\n",
        "        pred_topk_val = preds_topk(y_prob_val, topk)\n",
        "        f1_per_class = f1_score(y_true_val, pred_topk_val, average=None, zero_division=0)\n",
        "        auc_per_class = safe_auc_per_class(y_true_val, y_prob_val)\n",
        "\n",
        "        print(f\"[Epoch {epoch:03d}/{epochs}]\")\n",
        "        print(\n",
        "            f\"Train — Loss {history['train_loss'][-1]:.4f} | \"\n",
        "            f\"F1macro@{topk} {trm['f1_macro_topk']:.4f} | F1micro@{topk} {trm['f1_micro_topk']:.4f} | \"\n",
        "            f\"P@{topk} {trm['p_at_k']:.4f} | R@{topk} {trm['r_at_k']:.4f} | \"\n",
        "            f\"Exact {trm['exact_match_topk']:.4f} | F1macro(thr0.5) {trm['f1_macro_thr']:.4f}\"\n",
        "        )\n",
        "        print(\n",
        "            f\"Val   — Loss {history['val_loss'][-1]:.4f} | \"\n",
        "            f\"F1macro@{topk} {vam['f1_macro_topk']:.4f} | F1micro@{topk} {vam['f1_micro_topk']:.4f} | \"\n",
        "            f\"P@{topk} {vam['p_at_k']:.4f} | R@{topk} {vam['r_at_k']:.4f} | \"\n",
        "            f\"Exact {vam['exact_match_topk']:.4f} | F1macro(thr0.5) {vam['f1_macro_thr']:.4f}\"\n",
        "        )\n",
        "\n",
        "        valid_auc_mask = ~np.isnan(auc_per_class)\n",
        "        valid_idx = np.where(valid_auc_mask)[0]\n",
        "        print(f\"AUC valid for {int(valid_auc_mask.sum())}/{len(auc_per_class)} classes in validation.\")\n",
        "        print(\"Val per-class F1/AUC (first 5 VALID):\")\n",
        "        for j in valid_idx[:5]:\n",
        "            print(f\"  {label_names[j]}: F1={float(f1_per_class[j]):.4f}, AUC={float(auc_per_class[j]):.4f}\")\n",
        "        if len(valid_idx) == 0:\n",
        "            print(\"  (No valid AUC classes in this validation split.)\")\n",
        "        print()\n",
        "\n",
        "        saved_ckpt = False\n",
        "        if vam[\"f1_macro_topk\"] > best_val_f1:\n",
        "            best_val_f1 = float(vam[\"f1_macro_topk\"])\n",
        "            torch.save(model.state_dict(), best_path)\n",
        "            saved_ckpt = True\n",
        "            print(f\"[Checkpoint] Saved best BioBERT (Val F1macro@{topk}={best_val_f1:.4f}) -> {best_path}\\n\")\n",
        "\n",
        "        log_rows.append(\n",
        "            {\n",
        "                \"epoch\": epoch,\n",
        "                \"train_loss\": history[\"train_loss\"][-1],\n",
        "                \"val_loss\": history[\"val_loss\"][-1],\n",
        "                \"train_f1_macro_topk\": history[\"train_f1_macro_topk\"][-1],\n",
        "                \"val_f1_macro_topk\": history[\"val_f1_macro_topk\"][-1],\n",
        "                \"train_f1_micro_topk\": history[\"train_f1_micro_topk\"][-1],\n",
        "                \"val_f1_micro_topk\": history[\"val_f1_micro_topk\"][-1],\n",
        "                \"train_f1_samples_topk\": history[\"train_f1_samples_topk\"][-1],\n",
        "                \"val_f1_samples_topk\": history[\"val_f1_samples_topk\"][-1],\n",
        "                \"train_exact_match_topk\": history[\"train_exact_match_topk\"][-1],\n",
        "                \"val_exact_match_topk\": history[\"val_exact_match_topk\"][-1],\n",
        "                \"train_p_at_k\": history[\"train_p_at_k\"][-1],\n",
        "                \"val_p_at_k\": history[\"val_p_at_k\"][-1],\n",
        "                \"train_r_at_k\": history[\"train_r_at_k\"][-1],\n",
        "                \"val_r_at_k\": history[\"val_r_at_k\"][-1],\n",
        "                \"train_f1_macro_thr\": history[\"train_f1_macro_thr\"][-1],\n",
        "                \"val_f1_macro_thr\": history[\"val_f1_macro_thr\"][-1],\n",
        "                \"saved_ckpt\": saved_ckpt,\n",
        "                \"marker\": \"X\" if saved_ckpt else \"\",\n",
        "            }\n",
        "        )\n",
        "\n",
        "    metrics_csv = os.path.join(OUT_GRAPHS_DIR, f\"biobert_metrics_client{client_id}.csv\")\n",
        "    pd.DataFrame(log_rows).to_csv(metrics_csv, index=False)\n",
        "    print(f\"[Log] Wrote BioBERT per-epoch metrics CSV -> {metrics_csv}\\n\")\n",
        "\n",
        "    plot_history(history, OUT_GRAPHS_DIR, client_id, topk)\n",
        "    return history, best_path\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load client split\n",
        "    cdir = os.path.join(CLIENT_BASE_DIR, f\"client_{CLIENT_ID}\")\n",
        "    with open(os.path.join(cdir, \"train_pids.json\"), \"r\") as f:\n",
        "        train_pids = json.load(f)\n",
        "    with open(os.path.join(cdir, \"val_pids.json\"), \"r\") as f:\n",
        "        val_pids = json.load(f)\n",
        "\n",
        "    # Build datasets\n",
        "    full_ds = ReportsGliomaDataset(\n",
        "        METADATA_DF_PATH,\n",
        "        LABELS_PATH,\n",
        "        exclude_ids=[\"PatientID_0191\"],\n",
        "        remove_background=REMOVE_BACKGROUND_LABEL,\n",
        "    )\n",
        "    label_names = full_ds.labels\n",
        "\n",
        "    train_dataset = SubsetByPIDs(full_ds, train_pids)\n",
        "    val_dataset = SubsetByPIDs(full_ds, val_pids)\n",
        "\n",
        "    # DataLoaders\n",
        "    g = torch.Generator().manual_seed(SEED)\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "        collate_fn=reports_collate_fn,\n",
        "        worker_init_fn=worker_init_fn,\n",
        "        generator=g,\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "        collate_fn=reports_collate_fn,\n",
        "        worker_init_fn=worker_init_fn,\n",
        "        generator=g,\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"Loaded client_{CLIENT_ID}: \"\n",
        "        f\"train patients={len(train_dataset)}, val patients={len(val_dataset)}, num_labels={len(label_names)}\"\n",
        "    )\n",
        "\n",
        "    # Model + training\n",
        "    model = BioBERTMultiLabelClassifier(\n",
        "        BERT_MODEL_NAME,\n",
        "        num_labels=len(label_names),\n",
        "        dropout=DROPOUT,\n",
        "    )\n",
        "\n",
        "    history, best_model_path = train_eval_biobert(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        label_names=label_names,\n",
        "        train_dataset=train_dataset,\n",
        "        epochs=EPOCHS,\n",
        "        lr=LR,\n",
        "        warmup_ratio=WARMUP_RATIO,\n",
        "        max_len=MAX_LEN,\n",
        "        client_id=CLIENT_ID,\n",
        "        topk=TOPK_PRED,\n",
        "        use_pos_weight=USE_POS_WEIGHT,\n",
        "    )\n",
        "\n",
        "    print(f\"[Done] Best BioBERT model saved at: {best_model_path}\")"
      ],
      "metadata": {
        "id": "LtOH3OE4UST0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>Clip-like model<h2>**"
      ],
      "metadata": {
        "id": "vGF1_XihLEBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import random\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "\n",
        "import segmentation_models_pytorch as smp\n",
        "from sklearn.metrics import jaccard_score\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# Global config\n",
        "# ------------------------\n",
        "SEED = 42\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# Reproducibility helpers\n",
        "# ------------------------\n",
        "def set_seed(seed: int = 42) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
        "\n",
        "\n",
        "def worker_init_fn(worker_id: int) -> None:\n",
        "    # Make each dataloader worker deterministic\n",
        "    s = SEED + worker_id\n",
        "    np.random.seed(s)\n",
        "    random.seed(s)\n",
        "\n",
        "\n",
        "set_seed(SEED)\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# Small utilities\n",
        "# ------------------------\n",
        "def minmax01(x: np.ndarray) -> np.ndarray:\n",
        "    # Min-max normalize to [0, 1]\n",
        "    x = x.astype(np.float32)\n",
        "    mn = float(np.min(x))\n",
        "    mx = float(np.max(x))\n",
        "    if mx <= mn:\n",
        "        return np.zeros_like(x, dtype=np.float32)\n",
        "    return (x - mn) / (mx - mn)\n",
        "\n",
        "\n",
        "def find_first_existing(paths: List[str]) -> str:\n",
        "    # Pick the first path that exists\n",
        "    for p in paths:\n",
        "        if p and os.path.exists(p):\n",
        "            return p\n",
        "    raise FileNotFoundError(\"None of these paths exist:\\n\" + \"\\n\".join(paths))\n",
        "\n",
        "\n",
        "def ensure_dir(p: str) -> None:\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# Dataset: loads MRI + atlas regions + tumor mask + report text\n",
        "# Also builds a multi-label target from \"Top 5 Regions\"\n",
        "# ------------------------\n",
        "class GliomaDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        metadata_df_path: str,\n",
        "        labels_path: str,\n",
        "        data_root: str,\n",
        "        exclude_ids: Optional[List[str]] = None,\n",
        "        remove_background: bool = False,\n",
        "    ):\n",
        "        # Load metadata dataframe (contains report + labels per patient)\n",
        "        with open(metadata_df_path, \"rb\") as f:\n",
        "            df = pickle.load(f)\n",
        "\n",
        "        if exclude_ids is None:\n",
        "            exclude_ids = [\"PatientID_0191\"]\n",
        "\n",
        "        # Filter invalid rows and excluded patients\n",
        "        df = df[~df[\"Patient_ID\"].isin(exclude_ids)].reset_index(drop=True)\n",
        "        df = df[df[\"Report\"].notna() & df[\"Top 5 Regions\"].notna()].reset_index(drop=True)\n",
        "\n",
        "        # Load label list\n",
        "        with open(labels_path, \"rb\") as f:\n",
        "            labels = list(pickle.load(f))\n",
        "\n",
        "        if remove_background:\n",
        "            labels = [l for l in labels if str(l).strip().lower() != \"background\"]\n",
        "\n",
        "        self.labels = labels\n",
        "        self.label_to_idx = {lab: i for i, lab in enumerate(self.labels)}\n",
        "\n",
        "        self.data_root = data_root\n",
        "        self.df = df\n",
        "\n",
        "        # Encode some categorical columns\n",
        "        self.categorical_cols = [\n",
        "            \"Sex at Birth\",\n",
        "            \"Race\",\n",
        "            \"Primary Diagnosis\",\n",
        "            \"Previous Brain Tumor\",\n",
        "            \"Type of previous brain tumor\",\n",
        "            \"Age Range\",\n",
        "        ]\n",
        "        self.code_maps: Dict[str, Dict[int, Any]] = {}\n",
        "        for col in self.categorical_cols:\n",
        "            cat = pd.Categorical(self.df[col])\n",
        "            self.df[col + \"_code\"] = cat.codes.astype(np.int64)\n",
        "            self.code_maps[col] = dict(enumerate(cat.categories))\n",
        "\n",
        "        # Keep only patients that have all required .npy files\n",
        "        self.patient_ids: List[str] = []\n",
        "        for pid in self.df[\"Patient_ID\"].tolist():\n",
        "            base = os.path.join(self.data_root, pid)\n",
        "            mri_p = os.path.join(base, f\"{pid}_mri.npy\")\n",
        "            reg_p = os.path.join(base, f\"{pid}_regions.npy\")\n",
        "            tumor_p = os.path.join(base, f\"{pid}_tumor.npy\")\n",
        "            if os.path.isfile(mri_p) and os.path.isfile(reg_p) and os.path.isfile(tumor_p):\n",
        "                self.patient_ids.append(pid)\n",
        "\n",
        "        self.df = self.df[self.df[\"Patient_ID\"].isin(self.patient_ids)].reset_index(drop=True)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.df)\n",
        "\n",
        "    def _make_target_regions(self, regions_list: Any) -> np.ndarray:\n",
        "        # Multi-label vector over all region labels\n",
        "        y = np.zeros(len(self.labels), dtype=np.float32)\n",
        "        if not isinstance(regions_list, (list, tuple)):\n",
        "            return y\n",
        "        for reg in regions_list:\n",
        "            idx = self.label_to_idx.get(reg, None)\n",
        "            if idx is not None:\n",
        "                y[idx] = 1.0\n",
        "        return y\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
        "        row = self.df.iloc[idx]\n",
        "        pid = row[\"Patient_ID\"]\n",
        "        base = os.path.join(self.data_root, pid)\n",
        "\n",
        "        # Load arrays\n",
        "        mri = np.load(os.path.join(base, f\"{pid}_mri.npy\")).astype(np.float32)\n",
        "        regions = np.load(os.path.join(base, f\"{pid}_regions.npy\")).astype(np.float32)\n",
        "        tumor = np.load(os.path.join(base, f\"{pid}_tumor.npy\")).astype(np.float32)\n",
        "\n",
        "        # Normalize inputs, binarize mask\n",
        "        mri = minmax01(mri)\n",
        "        regions = minmax01(regions)\n",
        "        tumor = (tumor > 0.5).astype(np.float32)\n",
        "\n",
        "        target_regions = self._make_target_regions(row[\"Top 5 Regions\"])\n",
        "\n",
        "        return {\n",
        "            \"patient_id\": pid,\n",
        "            \"mri\": mri,\n",
        "            \"regions\": regions,\n",
        "            \"tumor\": tumor,\n",
        "            \"report\": str(row[\"Report\"]),\n",
        "            \"target_regions\": target_regions,\n",
        "            # Extra metadata\n",
        "            \"sex\": int(row[\"Sex at Birth_code\"]),\n",
        "            \"race\": int(row[\"Race_code\"]),\n",
        "            \"age\": float(row[\"Age at diagnosis\"]),\n",
        "            \"primary_diagnosis\": int(row[\"Primary Diagnosis_code\"]),\n",
        "            \"h3_3a_mutation\": float(row[\"H3-3A mutation\"]),\n",
        "            \"pten_mutation\": float(row[\"PTEN mutation\"]),\n",
        "            \"CDKN2A_B_deletion\": float(row[\"CDKN2A/B deletion\"]),\n",
        "            \"TP53_alteration\": float(row[\"TP53 alteration\"]),\n",
        "            \"other_mutations_alterations\": row[\"Other mutations/alterations\"],\n",
        "            \"previous_brain_tumor\": int(row[\"Previous Brain Tumor_code\"]),\n",
        "            \"type_of_previous_brain_tumor\": int(row[\"Type of previous brain tumor_code\"]),\n",
        "            \"age_range\": int(row[\"Age Range_code\"]),\n",
        "        }\n",
        "\n",
        "\n",
        "def glioma_collate_fn(batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "    # Build tensors for MRI/regions/mask and keep report text as list\n",
        "    mri = torch.from_numpy(np.stack([b[\"mri\"] for b in batch])).float().unsqueeze(1)\n",
        "    regions = torch.from_numpy(np.stack([b[\"regions\"] for b in batch])).float().unsqueeze(1)\n",
        "    tumor = torch.from_numpy(np.stack([b[\"tumor\"] for b in batch])).float().unsqueeze(1)\n",
        "    target_regions = torch.from_numpy(np.stack([b[\"target_regions\"] for b in batch])).float()\n",
        "\n",
        "    return {\n",
        "        \"patient_id\": [b[\"patient_id\"] for b in batch],\n",
        "        \"report\": [b[\"report\"] for b in batch],\n",
        "        \"mri\": mri,\n",
        "        \"regions\": regions,\n",
        "        \"tumor\": tumor,\n",
        "        \"target_regions\": target_regions,\n",
        "    }\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# Text encoder: BioClinicalBERT + linear head (head not used later)\n",
        "# ------------------------\n",
        "class BioBERTMultiLabelClassifier(nn.Module):\n",
        "    def __init__(self, model_name: str, num_labels: int, dropout: float = 0.3):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
        "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled = out.pooler_output\n",
        "        if pooled is None:\n",
        "            pooled = out.last_hidden_state[:, 0]  # CLS\n",
        "        x = self.dropout(pooled)\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "def tokenize_reports(tokenizer: AutoTokenizer, reports: List[str], max_len: int = 512) -> Dict[str, torch.Tensor]:\n",
        "    # Tokenize a list of report texts for BERT\n",
        "    return tokenizer(\n",
        "        reports,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_len,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "\n",
        "def dice_iou_acc(y_true: np.ndarray, y_pred: np.ndarray) -> Tuple[float, float, float]:\n",
        "    # Compute Dice, IoU and pixel accuracy for segmentation masks\n",
        "    y_true = y_true.astype(np.uint8).reshape(-1)\n",
        "    y_pred = y_pred.astype(np.uint8).reshape(-1)\n",
        "    inter = int((y_true * y_pred).sum())\n",
        "    dice = (2.0 * inter) / (float(y_true.sum() + y_pred.sum()) + 1e-8)\n",
        "    try:\n",
        "        iou = float(jaccard_score(y_true, y_pred, average=\"binary\"))\n",
        "    except Exception:\n",
        "        union = float(y_true.sum() + y_pred.sum() - inter) + 1e-8\n",
        "        iou = float(inter / union)\n",
        "    acc = float((y_true == y_pred).mean())\n",
        "    return float(dice), float(iou), float(acc)\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# CLIP-like model: aligns text embedding with image embedding\n",
        "# and also predicts segmentation mask with UNet\n",
        "# ------------------------\n",
        "class ClipModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        bert_backbone: AutoModel,\n",
        "        tokenizer: AutoTokenizer,\n",
        "        unet_model: nn.Module,\n",
        "        embed_dim: int = 512,\n",
        "        max_len: int = 512,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.bert_backbone = bert_backbone\n",
        "        self.tokenizer = tokenizer\n",
        "        self.unet_model = unet_model\n",
        "        self.max_len = max_len\n",
        "\n",
        "        # Projection layers to a shared embedding space\n",
        "        self.text_projection = nn.Linear(self.bert_backbone.config.hidden_size, embed_dim)\n",
        "        self.image_projection = nn.Linear(512, embed_dim)\n",
        "\n",
        "        # CLIP temperature parameter and uncertainty weights for multitask loss\n",
        "        self.logit_scale = nn.Parameter(torch.tensor(np.log(1 / 0.07), dtype=torch.float32))\n",
        "        self.log_sigma_clip = nn.Parameter(torch.tensor(0.0))\n",
        "        self.log_sigma_seg = nn.Parameter(torch.tensor(0.0))\n",
        "\n",
        "    def encode_text(self, texts: List[str]) -> torch.Tensor:\n",
        "        # Encode reports -> normalized text embeddings\n",
        "        enc = tokenize_reports(self.tokenizer, texts, max_len=self.max_len)\n",
        "        input_ids = enc[\"input_ids\"].to(DEVICE, non_blocking=True)\n",
        "        attention_mask = enc[\"attention_mask\"].to(DEVICE, non_blocking=True)\n",
        "\n",
        "        out = self.bert_backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled = out.pooler_output\n",
        "        if pooled is None:\n",
        "            pooled = out.last_hidden_state[:, 0]\n",
        "\n",
        "        t = self.text_projection(pooled)\n",
        "        return F.normalize(t, dim=-1)\n",
        "\n",
        "    def encode_image(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # Encode image -> normalized image embeddings\n",
        "        feats = self.unet_model.encoder(x)[-1]\n",
        "        v = feats.mean(dim=(2, 3))\n",
        "        v = self.image_projection(v)\n",
        "        return F.normalize(v, dim=-1)\n",
        "\n",
        "    def forward(\n",
        "        self, texts: List[str], mri: torch.Tensor, regions: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        # Build 2-channel input (MRI + atlas regions)\n",
        "        x = torch.cat([mri, regions], dim=1).to(DEVICE, non_blocking=True)\n",
        "\n",
        "        # CLIP embeddings + segmentation logits\n",
        "        t = self.encode_text(texts)\n",
        "        v = self.encode_image(x)\n",
        "        seg_logits = self.unet_model(x)\n",
        "        return t, v, seg_logits\n",
        "\n",
        "    def clip_contrastive_loss(self, t: torch.Tensor, v: torch.Tensor) -> torch.Tensor:\n",
        "        # Standard CLIP-style symmetric cross-entropy loss over batch similarities\n",
        "        logit_scale = self.logit_scale.exp().clamp(1e-3, 100.0)\n",
        "        logits = (t @ v.t()) * logit_scale\n",
        "        labels = torch.arange(t.size(0), device=logits.device)\n",
        "        return 0.5 * (F.cross_entropy(logits, labels) + F.cross_entropy(logits.t(), labels))\n",
        "\n",
        "    def combined_loss(\n",
        "        self,\n",
        "        t: torch.Tensor,\n",
        "        v: torch.Tensor,\n",
        "        seg_logits: torch.Tensor,\n",
        "        seg_target: torch.Tensor,\n",
        "        seg_criterion: nn.Module,\n",
        "    ) -> torch.Tensor:\n",
        "        # Multi-task loss with learned uncertainty weighting\n",
        "        clip_loss = self.clip_contrastive_loss(t, v)\n",
        "        seg_loss = seg_criterion(seg_logits, seg_target.to(seg_logits.device, non_blocking=True).float())\n",
        "        return (\n",
        "            (1.0 / (2.0 * torch.exp(self.log_sigma_clip) ** 2)) * clip_loss\n",
        "            + (1.0 / (2.0 * torch.exp(self.log_sigma_seg) ** 2)) * seg_loss\n",
        "            + self.log_sigma_clip\n",
        "            + self.log_sigma_seg\n",
        "        )\n",
        "\n",
        "\n",
        "def load_biobert_backbone_only(bert_wrapper: BioBERTMultiLabelClassifier, ckpt_path: str) -> None:\n",
        "    # Load only BERT weights (skip classifier head)\n",
        "    sd = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "    sd = {k: v for k, v in sd.items() if not k.startswith(\"classifier.\")}\n",
        "    bert_wrapper.load_state_dict(sd, strict=False)\n",
        "\n",
        "\n",
        "def build_unet(in_channels: int = 2, encoder_name: str = \"resnet34\", encoder_weights: Optional[str] = None) -> nn.Module:\n",
        "    # UNet used both for segmentation and as image feature encoder\n",
        "    return smp.Unet(\n",
        "        encoder_name=encoder_name,\n",
        "        encoder_weights=encoder_weights,\n",
        "        in_channels=in_channels,\n",
        "        classes=1,\n",
        "        activation=None,\n",
        "    )\n",
        "\n",
        "\n",
        "def train_one_epoch_clip(\n",
        "    model: ClipModel,\n",
        "    loader: DataLoader,\n",
        "    optimizer: optim.Optimizer,\n",
        "    seg_criterion: nn.Module,\n",
        "    scaler: torch.amp.GradScaler,\n",
        ") -> Dict[str, float]:\n",
        "    # One epoch of training (contrastive + segmentation)\n",
        "    model.train()\n",
        "    tot_loss = tot_dice = tot_iou = tot_acc = 0.0\n",
        "    n = 0\n",
        "\n",
        "    for batch in loader:\n",
        "        texts = batch[\"report\"]\n",
        "        mri = batch[\"mri\"].to(DEVICE, non_blocking=True)\n",
        "        regions = batch[\"regions\"].to(DEVICE, non_blocking=True)\n",
        "        tumor = batch[\"tumor\"].to(DEVICE, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with torch.amp.autocast(\"cuda\", enabled=(DEVICE.type == \"cuda\")):\n",
        "            t, v, seg_logits = model(texts, mri, regions)\n",
        "            loss = model.combined_loss(t, v, seg_logits, tumor, seg_criterion)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            preds = (torch.sigmoid(seg_logits).detach().cpu().numpy() > 0.5).astype(np.uint8)\n",
        "            y_np = (tumor.detach().cpu().numpy() > 0.5).astype(np.uint8)\n",
        "            d, i, a = dice_iou_acc(y_np, preds)\n",
        "\n",
        "        tot_loss += float(loss.item())\n",
        "        tot_dice += d\n",
        "        tot_iou += i\n",
        "        tot_acc += a\n",
        "        n += 1\n",
        "\n",
        "    return {\n",
        "        \"loss\": tot_loss / max(1, n),\n",
        "        \"dice\": tot_dice / max(1, n),\n",
        "        \"iou\": tot_iou / max(1, n),\n",
        "        \"acc\": tot_acc / max(1, n),\n",
        "    }\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_one_epoch_clip(model: ClipModel, loader: DataLoader, seg_criterion: nn.Module) -> Dict[str, float]:\n",
        "    # Validation epoch\n",
        "    model.eval()\n",
        "    tot_loss = tot_dice = tot_iou = tot_acc = 0.0\n",
        "    n = 0\n",
        "\n",
        "    for batch in loader:\n",
        "        texts = batch[\"report\"]\n",
        "        mri = batch[\"mri\"].to(DEVICE, non_blocking=True)\n",
        "        regions = batch[\"regions\"].to(DEVICE, non_blocking=True)\n",
        "        tumor = batch[\"tumor\"].to(DEVICE, non_blocking=True)\n",
        "\n",
        "        t, v, seg_logits = model(texts, mri, regions)\n",
        "        loss = model.combined_loss(t, v, seg_logits, tumor, seg_criterion)\n",
        "\n",
        "        preds = (torch.sigmoid(seg_logits).detach().cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        y_np = (tumor.detach().cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        d, i, a = dice_iou_acc(y_np, preds)\n",
        "\n",
        "        tot_loss += float(loss.item())\n",
        "        tot_dice += d\n",
        "        tot_iou += i\n",
        "        tot_acc += a\n",
        "        n += 1\n",
        "\n",
        "    return {\n",
        "        \"loss\": tot_loss / max(1, n),\n",
        "        \"dice\": tot_dice / max(1, n),\n",
        "        \"iou\": tot_iou / max(1, n),\n",
        "        \"acc\": tot_acc / max(1, n),\n",
        "    }\n",
        "\n",
        "\n",
        "def main():\n",
        "    candidates_metadata = [\n",
        "        \"/content/cleaned_df.pkl\",\n",
        "        \"/content/drive/MyDrive/PKG - MU-Glioma-Post/cleaned_df.pkl\",\n",
        "        \"/content/cleaned_df.pkl\",\n",
        "    ]\n",
        "    candidates_labels = [\n",
        "        \"/content/labels_list.pkl\",\n",
        "        \"/content/drive/MyDrive/PKG - MU-Glioma-Post/labels_list.pkl\",\n",
        "        \"/content/labels_list.pkl\",\n",
        "    ]\n",
        "    candidates_data_root = [\n",
        "        \"/content/Preprocessed-Data\",\n",
        "        \"/content/drive/MyDrive/PKG - MU-Glioma-Post/Preprocessed-Data\",\n",
        "        \"/content/Preprocessed-Data\",\n",
        "    ]\n",
        "\n",
        "    METADATA_DF_PATH = find_first_existing(candidates_metadata)\n",
        "    LABELS_PATH = find_first_existing(candidates_labels)\n",
        "    DATA_ROOT = find_first_existing(candidates_data_root)\n",
        "\n",
        "    BERT_MODEL_NAME = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "\n",
        "    # Optional pretrained checkpoints\n",
        "    BIOBERT_BEST_CAND = [\n",
        "        \"/content/AITDM/Models/BioClinicalBert/best_biobert_client0.pt\",\n",
        "        \"/content/drive/MyDrive/PKG - MU-Glioma-Post/Models/BioClinicalBert/best.pt\",\n",
        "        \"/content/AITDM/Models/BioClinicalBert/best_biobert_client0.pt\",\n",
        "    ]\n",
        "    UNET_BEST_CAND = [\n",
        "        \"/content/AITDM/Models/UNet_ImageOnly/best_unet_client0.pth\",\n",
        "        \"/content/AITDM/Models/UNet_ImageOnly/best_unet_client0.pth\",\n",
        "        \"/content/drive/MyDrive/PKG - MU-Glioma-Post/Models/UNet/best_unet_model.pth\",\n",
        "        \"/content/AITDM/Models/UNet_ImageOnly/best_unet_client0.pth\",\n",
        "        \"/content/AITDM/Models/UNet_ImageOnly/best_unet_client0.pth\",\n",
        "    ]\n",
        "\n",
        "    BIOBERT_BEST = next((p for p in BIOBERT_BEST_CAND if os.path.isfile(p)), \"\")\n",
        "    UNET_BEST = next((p for p in UNET_BEST_CAND if os.path.isfile(p)), \"\")\n",
        "\n",
        "    # Output checkpoint for the CLIP-like model\n",
        "    CLIP_SAVE = \"/content/drive/MyDrive/PKG - MU-Glioma-Post/Models/CLIP_Based/best_clip_model.pth\"\n",
        "    ensure_dir(os.path.dirname(CLIP_SAVE))\n",
        "\n",
        "    # Training hyperparams\n",
        "    BATCH_SIZE = 4\n",
        "    EPOCHS = 50\n",
        "    LR = 1e-4\n",
        "    MAX_LEN = 512\n",
        "\n",
        "    print(\"Resolved paths:\")\n",
        "    print(\"  metadata:\", METADATA_DF_PATH)\n",
        "    print(\"  labels  :\", LABELS_PATH)\n",
        "    print(\"  data_root:\", DATA_ROOT)\n",
        "    print(\"  biobert ckpt:\", BIOBERT_BEST if BIOBERT_BEST else \"(not found, will use base)\")\n",
        "    print(\"  unet ckpt  :\", UNET_BEST if UNET_BEST else \"(not found, will use random init)\")\n",
        "\n",
        "    # Build dataset and split train/test\n",
        "    dataset = GliomaDataset(\n",
        "        metadata_df_path=METADATA_DF_PATH,\n",
        "        labels_path=LABELS_PATH,\n",
        "        data_root=DATA_ROOT,\n",
        "        exclude_ids=[\"PatientID_0191\"],\n",
        "        remove_background=False,\n",
        "    )\n",
        "\n",
        "    test_size = int(0.2 * len(dataset))\n",
        "    train_size = len(dataset) - test_size\n",
        "    train_dataset, test_dataset = random_split(\n",
        "        dataset, [train_size, test_size], generator=torch.Generator().manual_seed(SEED)\n",
        "    )\n",
        "\n",
        "    # Dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        collate_fn=glioma_collate_fn,\n",
        "        worker_init_fn=worker_init_fn,\n",
        "        drop_last=True,\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        collate_fn=glioma_collate_fn,\n",
        "        worker_init_fn=worker_init_fn,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    # Tokenizer + BERT backbone\n",
        "    tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
        "    bert_wrapper = BioBERTMultiLabelClassifier(BERT_MODEL_NAME, num_labels=len(dataset.labels), dropout=0.3)\n",
        "    if BIOBERT_BEST:\n",
        "        try:\n",
        "            bert_wrapper.load_state_dict(torch.load(BIOBERT_BEST, map_location=\"cpu\"))\n",
        "        except RuntimeError:\n",
        "            load_biobert_backbone_only(bert_wrapper, BIOBERT_BEST)\n",
        "    bert_backbone = bert_wrapper.bert.to(DEVICE)\n",
        "\n",
        "    # UNet\n",
        "    unet_model = build_unet(in_channels=2, encoder_name=\"resnet34\", encoder_weights=None)\n",
        "    if UNET_BEST:\n",
        "        unet_model.load_state_dict(torch.load(UNET_BEST, map_location=\"cpu\"))\n",
        "    unet_model = unet_model.to(DEVICE)\n",
        "\n",
        "    # Build CLIP-like multimodal model\n",
        "    clip_model = ClipModel(\n",
        "        bert_backbone=bert_backbone,\n",
        "        tokenizer=tokenizer,\n",
        "        unet_model=unet_model,\n",
        "        embed_dim=512,\n",
        "        max_len=MAX_LEN,\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    # Loss/optimizer/scheduler\n",
        "    seg_criterion = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
        "    optimizer = optim.AdamW(clip_model.parameters(), lr=LR)\n",
        "\n",
        "    total_steps = len(train_loader) * EPOCHS\n",
        "    warmup_steps = int(0.1 * total_steps)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    scaler = torch.amp.GradScaler(\"cuda\", enabled=(DEVICE.type == \"cuda\"))\n",
        "\n",
        "    # Track best checkpoint by (Dice up) and (loss down)\n",
        "    best_val_dice = -1.0\n",
        "    best_val_loss = float(\"inf\")\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        tr = train_one_epoch_clip(clip_model, train_loader, optimizer, seg_criterion, scaler)\n",
        "        va = eval_one_epoch_clip(clip_model, test_loader, seg_criterion)\n",
        "\n",
        "        # Step LR scheduler once per training step (done here in a loop)\n",
        "        for _ in range(len(train_loader)):\n",
        "            scheduler.step()\n",
        "\n",
        "        print(\n",
        "            f\"[Epoch {epoch:03d}/{EPOCHS}] \"\n",
        "            f\"Train loss {tr['loss']:.4f} dice {tr['dice']:.4f} iou {tr['iou']:.4f} acc {tr['acc']:.4f} || \"\n",
        "            f\"Val loss {va['loss']:.4f} dice {va['dice']:.4f} iou {va['iou']:.4f} acc {va['acc']:.4f}\"\n",
        "        )\n",
        "\n",
        "        # Save best model\n",
        "        if (va[\"dice\"] > best_val_dice) and (va[\"loss\"] < best_val_loss):\n",
        "            best_val_dice = va[\"dice\"]\n",
        "            best_val_loss = va[\"loss\"]\n",
        "            torch.save(clip_model.state_dict(), CLIP_SAVE)\n",
        "            print(f\"Saved best model -> {CLIP_SAVE} (val dice {best_val_dice:.4f}, val loss {best_val_loss:.4f})\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "e4SPJvVHVzde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **M2 - Ensemble on Images only**"
      ],
      "metadata": {
        "id": "tXoXxVZ4Lig_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import segmentation_models_pytorch as smp\n",
        "print(list(smp.encoders.get_encoder_names()))"
      ],
      "metadata": {
        "id": "YgL9BfPx8seu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>UNet - \"resnet50\"<h2>**"
      ],
      "metadata": {
        "id": "gtwsveySU7ft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, pickle, random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "from sklearn.metrics import jaccard_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "# Paths and I/O config\n",
        "DATA_ROOT = \"/content/Preprocessed-Data\"\n",
        "METADATA_DF_PATH = \"cleaned_df.pkl\"\n",
        "OUT_BASE = \"AITDM\"\n",
        "CLIENT_DIR = os.path.join(OUT_BASE, \"/content/client\")\n",
        "\n",
        "# Experiment config\n",
        "USE_ATLAS = True\n",
        "CLIENT_ID = 0\n",
        "BATCH_SIZE = 8\n",
        "LR = 1e-3\n",
        "EPOCHS = 50\n",
        "NUM_WORKERS = 2\n",
        "SEED = 42\n",
        "\n",
        "# Model / encoder config\n",
        "ENCODER_NAME = \"resnet50\"\n",
        "ENCODER_WEIGHTS = \"imagenet\"\n",
        "\n",
        "# Output directories\n",
        "OUT_MODELS_DIR = os.path.join(OUT_BASE, \"Models\", \"UNet_ImageOnly\")\n",
        "OUT_GRAPHS_DIR = os.path.join(OUT_BASE, \"Graphs\")\n",
        "os.makedirs(OUT_MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(OUT_GRAPHS_DIR, exist_ok=True)\n",
        "\n",
        "def sanitize(s: str) -> str:\n",
        "    return str(s).replace(\"/\", \"-\").replace(\" \", \"_\")\n",
        "\n",
        "encoder_tag = sanitize(ENCODER_NAME)\n",
        "weights_tag = sanitize(ENCODER_WEIGHTS) if ENCODER_WEIGHTS is not None else \"none\"\n",
        "atlas_tag = \"atlas\" if USE_ATLAS else \"img\"\n",
        "\n",
        "run_tag = f\"unet_{encoder_tag}_{weights_tag}_{atlas_tag}\"\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
        "\n",
        "def worker_init_fn(worker_id):\n",
        "    worker_seed = SEED + worker_id\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "set_seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class ImageOnlyGliomaDataset(Dataset):\n",
        "    \"\"\"Dataset that loads MRI, tumor mask, and optional atlas regions per patient.\"\"\"\n",
        "\n",
        "    def __init__(self, metadata_df_path, data_root, use_atlas=True, exclude_ids=None, transform=None):\n",
        "        import pickle, os, numpy as np, pandas as pd\n",
        "\n",
        "        # Load metadata dataframe\n",
        "        with open(metadata_df_path, \"rb\") as f:\n",
        "            df = pickle.load(f)\n",
        "\n",
        "        # Optionally exclude some patient IDs\n",
        "        if exclude_ids is None:\n",
        "            exclude_ids = [\"PatientID_0191\"]\n",
        "        self.df = df[~df[\"Patient_ID\"].isin(exclude_ids)].reset_index(drop=True)\n",
        "\n",
        "        self.data_root = data_root\n",
        "        self.use_atlas = use_atlas\n",
        "        self.transform = transform\n",
        "\n",
        "        # Collect patient IDs that have all required files\n",
        "        self.patient_ids = []\n",
        "        for pid in sorted(self.df[\"Patient_ID\"].tolist()):\n",
        "            mri_p = os.path.join(self.data_root, pid, f\"{pid}_mri.npy\")\n",
        "            tumor_p = os.path.join(self.data_root, pid, f\"{pid}_tumor.npy\")\n",
        "\n",
        "            if self.use_atlas:\n",
        "                reg_p = os.path.join(self.data_root, pid, f\"{pid}_regions.npy\")\n",
        "                if os.path.isfile(mri_p) and os.path.isfile(tumor_p) and os.path.isfile(reg_p):\n",
        "                    self.patient_ids.append(pid)\n",
        "            else:\n",
        "                if os.path.isfile(mri_p) and os.path.isfile(tumor_p):\n",
        "                    self.patient_ids.append(pid)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patient_ids)\n",
        "\n",
        "    @staticmethod\n",
        "    def _minmax(x):\n",
        "        \"\"\"Simple min-max normalization to [0, 1].\"\"\"\n",
        "        x = x.astype(np.float32)\n",
        "        mn, mx = np.min(x), np.max(x)\n",
        "        return (x - mn) / (mx - mn) if mx > mn else np.zeros_like(x, dtype=np.float32)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Load and return one sample (dict) for a patient.\"\"\"\n",
        "        import os, numpy as np\n",
        "\n",
        "        pid = self.patient_ids[idx]\n",
        "        base = os.path.join(self.data_root, pid)\n",
        "\n",
        "        # Load MRI and tumor mask\n",
        "        mri = np.load(os.path.join(base, f\"{pid}_mri.npy\")).astype(np.float32)\n",
        "        tumor = np.load(os.path.join(base, f\"{pid}_tumor.npy\")).astype(np.float32)\n",
        "\n",
        "        # Normalize MRI and binarize tumor mask\n",
        "        mri = self._minmax(mri)\n",
        "        tumor = (tumor > 0.5).astype(np.float32)\n",
        "\n",
        "        sample = {\"patient_id\": pid, \"mri\": mri, \"tumor\": tumor}\n",
        "\n",
        "        # Optionally load regions/atlas\n",
        "        if self.use_atlas:\n",
        "            regions = np.load(os.path.join(base, f\"{pid}_regions.npy\")).astype(np.float32)\n",
        "            regions = self._minmax(regions)\n",
        "            sample[\"regions\"] = regions\n",
        "\n",
        "        return sample\n",
        "\n",
        "def image_only_collate_fn(batch, use_atlas=True):\n",
        "    \"\"\"Custom collate: stack MRI (+ optional regions) and tumor into tensors.\"\"\"\n",
        "    mri = torch.stack([torch.tensor(it[\"mri\"]) for it in batch]).unsqueeze(1)\n",
        "    y = torch.stack([torch.tensor(it[\"tumor\"]) for it in batch]).unsqueeze(1)\n",
        "\n",
        "    if use_atlas:\n",
        "        regs = torch.stack([torch.tensor(it[\"regions\"]) for it in batch]).unsqueeze(1)\n",
        "        # Concatenate MRI and regions as channels\n",
        "        x = torch.cat([mri.float(), regs.float()], dim=1)\n",
        "    else:\n",
        "        x = mri.float()\n",
        "\n",
        "    return {\"x\": x, \"y\": y.float(), \"pid\": [it[\"patient_id\"] for it in batch]}\n",
        "\n",
        "class SubsetByPIDs(Dataset):\n",
        "    \"\"\"Wrap a dataset but keep only a subset of patient IDs.\"\"\"\n",
        "\n",
        "    def __init__(self, full_dataset: ImageOnlyGliomaDataset, pid_list: List[str]):\n",
        "        self.ds = full_dataset\n",
        "        # Map patient IDs to indices\n",
        "        pid_to_idx = {pid: i for i, pid in enumerate(self.ds.patient_ids)}\n",
        "        self.indices = [pid_to_idx[p] for p in pid_list if p in pid_to_idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.ds[self.indices[i]]\n",
        "\n",
        "# Load client-specific train/val patient IDs\n",
        "cdir = os.path.join(CLIENT_DIR, f\"client_{CLIENT_ID}\")\n",
        "with open(os.path.join(cdir, \"train_pids.json\"), \"r\") as f:\n",
        "    train_pids = json.load(f)\n",
        "with open(os.path.join(cdir, \"val_pids.json\"), \"r\") as f:\n",
        "    val_pids = json.load(f)\n",
        "\n",
        "# Build full dataset and then client subsets\n",
        "full_ds = ImageOnlyGliomaDataset(\n",
        "    METADATA_DF_PATH,\n",
        "    DATA_ROOT,\n",
        "    use_atlas=USE_ATLAS,\n",
        "    exclude_ids=[\"PatientID_0191\"],\n",
        ")\n",
        "train_dataset = SubsetByPIDs(full_ds, train_pids)\n",
        "val_dataset = SubsetByPIDs(full_ds, val_pids)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "    generator=torch.Generator().manual_seed(SEED),\n",
        "    worker_init_fn=worker_init_fn,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "    generator=torch.Generator().manual_seed(SEED),\n",
        "    worker_init_fn=worker_init_fn,\n",
        ")\n",
        "\n",
        "print(f\"Loaded client_{CLIENT_ID}: train patients={len(train_dataset)}, val patients={len(val_dataset)}\")\n",
        "\n",
        "# UNet model (from segmentation_models_pytorch)\n",
        "in_channels = 2 if USE_ATLAS else 1\n",
        "model = smp.Unet(\n",
        "    encoder_name=ENCODER_NAME,\n",
        "    encoder_weights=ENCODER_WEIGHTS,\n",
        "    in_channels=in_channels,\n",
        "    classes=1,\n",
        ").to(device)\n",
        "\n",
        "# Loss, optimizer, scheduler, and AMP scaler\n",
        "criterion = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
        "scheduler = optim.lr_scheduler.LinearLR(\n",
        "    optimizer, start_factor=1.0, end_factor=0.0, total_iters=EPOCHS\n",
        ")\n",
        "scaler = torch.amp.GradScaler(\"cuda\", enabled=(device.type == \"cuda\"))\n",
        "\n",
        "def calc_metrics(y_true, y_pred):\n",
        "    \"\"\"Compute Dice, IoU, and pixel accuracy.\"\"\"\n",
        "    y_true = y_true.astype(np.uint8).reshape(-1)\n",
        "    y_pred = y_pred.astype(np.uint8).reshape(-1)\n",
        "\n",
        "    inter = (y_true * y_pred).sum()\n",
        "    dice = (2.0 * inter) / (y_true.sum() + y_pred.sum() + 1e-8)\n",
        "\n",
        "    # IoU with sklearn, fallback if it fails\n",
        "    try:\n",
        "        iou = jaccard_score(y_true, y_pred, average=\"binary\")\n",
        "    except Exception:\n",
        "        union = y_true.sum() + y_pred.sum() - inter + 1e-8\n",
        "        iou = inter / union\n",
        "\n",
        "    acc = (y_true == y_pred).mean()\n",
        "    return float(dice), float(iou), float(acc)\n",
        "\n",
        "def plot_prediction(sample_imgs, save_path):\n",
        "    \"\"\"Plot MRI (+ optional regions), GT, and prediction for one batch.\"\"\"\n",
        "    if \"regions\" in sample_imgs:\n",
        "        fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
        "    else:\n",
        "        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # MRI\n",
        "    axs[0].imshow(sample_imgs[\"mri\"][0, 0], cmap=\"gray\")\n",
        "    axs[0].set_title(\"MRI\")\n",
        "    axs[0].axis(\"off\")\n",
        "\n",
        "    col = 1\n",
        "\n",
        "    # Regions (if available)\n",
        "    if \"regions\" in sample_imgs:\n",
        "        axs[col].imshow(sample_imgs[\"regions\"][0, 0], cmap=\"gray\")\n",
        "        axs[col].set_title(\"Regions\")\n",
        "        axs[col].axis(\"off\")\n",
        "        col += 1\n",
        "\n",
        "    # Ground truth\n",
        "    axs[col].imshow(sample_imgs[\"y_true\"][0, 0], cmap=\"gray\")\n",
        "    axs[col].set_title(\"Ground Truth\")\n",
        "    axs[col].axis(\"off\")\n",
        "    col += 1\n",
        "\n",
        "    # Prediction\n",
        "    axs[col].imshow(sample_imgs[\"y_pred\"][0, 0], cmap=\"gray\")\n",
        "    axs[col].set_title(\"Predicted\")\n",
        "    axs[col].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def plot_metrics(history, save_path):\n",
        "    \"\"\"Plot training and validation curves for loss and metrics.\"\"\"\n",
        "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
        "\n",
        "    plt.figure(figsize=(16, 12))\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\")\n",
        "    plt.plot(epochs, history[\"val_loss\"], label=\"Val Loss\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Loss\")\n",
        "\n",
        "    # Dice\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(epochs, history[\"train_dice\"], label=\"Train Dice\")\n",
        "    plt.plot(epochs, history[\"val_dice\"], label=\"Val Dice\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Dice\")\n",
        "\n",
        "    # IoU\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(epochs, history[\"train_iou\"], label=\"Train IoU\")\n",
        "    plt.plot(epochs, history[\"val_iou\"], label=\"Val IoU\")\n",
        "    plt.legend()\n",
        "    plt.title(\"IoU\")\n",
        "\n",
        "    # Accuracy\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(epochs, history[\"train_acc\"], label=\"Train Acc\")\n",
        "    plt.plot(epochs, history[\"val_acc\"], label=\"Val Acc\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Accuracy\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion, scaler):\n",
        "    \"\"\"One training epoch over the dataloader.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = total_dice = total_iou = total_acc = 0.0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        x = batch[\"x\"].to(device)\n",
        "        y = batch[\"y\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Mixed precision forward pass\n",
        "        with torch.amp.autocast(\"cuda\", enabled=(device.type == \"cuda\")):\n",
        "            preds = model(x)\n",
        "            loss = criterion(preds, y)\n",
        "\n",
        "        # Backward with gradient scaling\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "        # Convert predictions to binary and compute metrics\n",
        "        preds_prob = (torch.sigmoid(preds).detach().cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        y_np = (y.detach().cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        d, i, a = calc_metrics(y_np, preds_prob)\n",
        "\n",
        "        total_dice += d\n",
        "        total_iou += i\n",
        "        total_acc += a\n",
        "\n",
        "    n = len(dataloader)\n",
        "    return total_loss / n, total_dice / n, total_iou / n, total_acc / n\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_one_epoch(model, dataloader, criterion, keep_last_batch=True):\n",
        "    \"\"\"Evaluation loop over the validation dataloader.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = total_dice = total_iou = total_acc = 0.0\n",
        "    last_batch_imgs = None\n",
        "\n",
        "    for batch in dataloader:\n",
        "        x = batch[\"x\"].to(device)\n",
        "        y = batch[\"y\"].to(device)\n",
        "\n",
        "        preds = model(x)\n",
        "        loss = criterion(preds, y)\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "        preds_prob = (torch.sigmoid(preds).cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        y_np = (y.cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        d, i, a = calc_metrics(y_np, preds_prob)\n",
        "\n",
        "        total_dice += d\n",
        "        total_iou += i\n",
        "        total_acc += a\n",
        "\n",
        "        # Optionally keep last batch for visualization\n",
        "        if keep_last_batch:\n",
        "            ch0 = x[:, 0:1].cpu().numpy()\n",
        "            imgs = {\"mri\": ch0, \"y_true\": y.cpu().numpy(), \"y_pred\": preds_prob}\n",
        "            if x.shape[1] == 2:\n",
        "                ch1 = x[:, 1:2].cpu().numpy()\n",
        "                imgs[\"regions\"] = ch1\n",
        "            last_batch_imgs = imgs\n",
        "\n",
        "    n = len(dataloader)\n",
        "    return total_loss / n, total_dice / n, total_iou / n, total_acc / n, last_batch_imgs\n",
        "\n",
        "def infer_and_visualize_best(\n",
        "    model,\n",
        "    val_dataset,\n",
        "    use_atlas: bool,\n",
        "    out_dir: str,\n",
        "    client_id: int,\n",
        "    best_ckpt_path: str,\n",
        "    k_samples: int = 3,\n",
        "    threshold: float = 0.5,\n",
        "):\n",
        "    \"\"\"Load best checkpoint, run inference on a few validation samples, and save visualizations.\"\"\"\n",
        "    import random\n",
        "\n",
        "    # Make sampling deterministic\n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    if os.path.isfile(best_ckpt_path):\n",
        "        model.load_state_dict(torch.load(best_ckpt_path, map_location=device))\n",
        "        model.eval()\n",
        "        print(f\"[Best Model] Loaded: {best_ckpt_path}\\n\")\n",
        "    else:\n",
        "        print(f\"[Best Model] Missing checkpoint: {best_ckpt_path}\\n\")\n",
        "        return\n",
        "\n",
        "    k = min(k_samples, len(val_dataset))\n",
        "    if k == 0:\n",
        "        print(\"[Best Model] Empty val dataset.\\n\")\n",
        "        return\n",
        "\n",
        "    # Deterministic random subset of validation indices\n",
        "    idxs = random.sample(range(len(val_dataset)), k)\n",
        "\n",
        "    def _predict_one(sample):\n",
        "        \"\"\"Run model on one sample and return inputs and binarized prediction.\"\"\"\n",
        "        mri = torch.tensor(sample[\"mri\"]).unsqueeze(0).unsqueeze(0).float().to(device)\n",
        "\n",
        "        if use_atlas and (\"regions\" in sample):\n",
        "            regs = torch.tensor(sample[\"regions\"]).unsqueeze(0).unsqueeze(0).float().to(device)\n",
        "            x = torch.cat([mri, regs], dim=1)\n",
        "        else:\n",
        "            x = mri\n",
        "\n",
        "        with torch.no_grad():\n",
        "            prob = torch.sigmoid(model(x)).cpu().numpy()\n",
        "            pred_bin = (prob > threshold).astype(np.uint8)\n",
        "\n",
        "        return x.cpu().numpy(), pred_bin\n",
        "\n",
        "    saved_paths = []\n",
        "\n",
        "    # Save individual sample figures\n",
        "    for i, idx in enumerate(idxs, 1):\n",
        "        sample = val_dataset[idx]\n",
        "        pid = sample.get(\"patient_id\", f\"val_{idx}\")\n",
        "        x_np, pred_bin = _predict_one(sample)\n",
        "\n",
        "        imgs = {\n",
        "            \"mri\": x_np[:, 0:1],\n",
        "            \"y_true\": np.expand_dims(\n",
        "                np.expand_dims(sample[\"tumor\"], 0), 0\n",
        "            ).astype(np.float32),\n",
        "            \"y_pred\": pred_bin.astype(np.float32),\n",
        "        }\n",
        "\n",
        "        if use_atlas and (\"regions\" in sample):\n",
        "            imgs[\"regions\"] = x_np[:, 1:2]\n",
        "\n",
        "        out_path = os.path.join(\n",
        "            out_dir, f\"best_val_sample_{i}_client{client_id}_{pid}.png\"\n",
        "        )\n",
        "        plot_prediction(imgs, out_path)\n",
        "        saved_paths.append(out_path)\n",
        "\n",
        "    print(\"[Best Model] Saved individual figures:\")\n",
        "    for p in saved_paths:\n",
        "        print(\" -\", p)\n",
        "    print()\n",
        "\n",
        "    # Save grid figure\n",
        "    cols = 4 if use_atlas else 3\n",
        "    fig, axs = plt.subplots(k, cols, figsize=(5 * cols, 4 * k))\n",
        "\n",
        "    if k == 1:\n",
        "        axs = np.expand_dims(axs, 0)\n",
        "\n",
        "    for row, idx in enumerate(idxs):\n",
        "        sample = val_dataset[idx]\n",
        "        x_np, pred_bin = _predict_one(sample)\n",
        "\n",
        "        mri = x_np[0, 0]\n",
        "        gt = sample[\"tumor\"]\n",
        "        col = 0\n",
        "\n",
        "        axs[row, col].imshow(mri, cmap=\"gray\")\n",
        "        axs[row, col].set_title(\"MRI\")\n",
        "        axs[row, col].axis(\"off\")\n",
        "        col += 1\n",
        "\n",
        "        if use_atlas and (\"regions\" in sample):\n",
        "            regs = x_np[0, 1]\n",
        "            axs[row, col].imshow(regs, cmap=\"gray\")\n",
        "            axs[row, col].set_title(\"Regions\")\n",
        "            axs[row, col].axis(\"off\")\n",
        "            col += 1\n",
        "\n",
        "        axs[row, col].imshow(gt, cmap=\"gray\")\n",
        "        axs[row, col].set_title(\"Ground Truth\")\n",
        "        axs[row, col].axis(\"off\")\n",
        "        col += 1\n",
        "\n",
        "        axs[row, col].imshow(pred_bin[0, 0], cmap=\"gray\")\n",
        "        axs[row, col].set_title(\"Predicted (τ=0.5)\")\n",
        "        axs[row, col].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    grid_path = os.path.join(out_dir, f\"best_model_val_grid_{run_tag}_client{client_id}.png\")\n",
        "    plt.savefig(grid_path, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    print(f\"[Best Model] Saved grid -> {grid_path}\\n\")\n",
        "\n",
        "# History containers for training curves\n",
        "history = {\n",
        "    \"train_loss\": [],\n",
        "    \"train_dice\": [],\n",
        "    \"train_iou\": [],\n",
        "    \"train_acc\": [],\n",
        "    \"val_loss\": [],\n",
        "    \"val_dice\": [],\n",
        "    \"val_iou\": [],\n",
        "    \"val_acc\": [],\n",
        "}\n",
        "\n",
        "# Tracking best validation metrics\n",
        "best_val_iou = 0.0\n",
        "best_val_dice = -float(\"inf\")\n",
        "best_val_loss = float(\"inf\")\n",
        "best_path = os.path.join(OUT_MODELS_DIR, f\"best_{run_tag}_client{CLIENT_ID}.pth\")\n",
        "\n",
        "log_rows = []\n",
        "\n",
        "# Main training loop\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    # Train one epoch\n",
        "    trL, trD, trI, trA = train_one_epoch(\n",
        "        model, train_loader, optimizer, criterion, scaler\n",
        "    )\n",
        "    # Validate\n",
        "    vaL, vaD, vaI, vaA, _ = eval_one_epoch(\n",
        "        model, val_loader, criterion, keep_last_batch=True\n",
        "    )\n",
        "\n",
        "    # Update scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    # Save metrics for plots\n",
        "    history[\"train_loss\"].append(trL)\n",
        "    history[\"train_dice\"].append(trD)\n",
        "    history[\"train_iou\"].append(trI)\n",
        "    history[\"train_acc\"].append(trA)\n",
        "\n",
        "    history[\"val_loss\"].append(vaL)\n",
        "    history[\"val_dice\"].append(vaD)\n",
        "    history[\"val_iou\"].append(vaI)\n",
        "    history[\"val_acc\"].append(vaA)\n",
        "\n",
        "    print(\n",
        "        f\"[Epoch {epoch:03d}/{EPOCHS}] \"\n",
        "        f\"Train — Loss {trL:.4f} | Dice {trD:.4f} | IoU {trI:.4f} | Accuracy {trA:.4f} || \"\n",
        "        f\"Val — Loss {vaL:.4f} | Dice {vaD:.4f} | IoU {vaI:.4f} | Accuracy {vaA:.4f}\\n\"\n",
        "    )\n",
        "\n",
        "    # Save checkpoint if Dice improves and loss decreases\n",
        "    saved_ckpt = False\n",
        "    if (vaD > best_val_dice) and (vaL < best_val_loss):\n",
        "        best_val_dice = vaD\n",
        "        best_val_loss = vaL\n",
        "        torch.save(model.state_dict(), best_path)\n",
        "        saved_ckpt = True\n",
        "        print(\n",
        "            f\"Saved best model (Val Dice↑ {best_val_dice:.4f} & Val Loss↓ {best_val_loss:.4f}) -> {best_path}\\n\"\n",
        "        )\n",
        "\n",
        "    # Log row for CSV\n",
        "    log_rows.append(\n",
        "        {\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": trL,\n",
        "            \"train_dice\": trD,\n",
        "            \"train_iou\": trI,\n",
        "            \"train_acc\": trA,\n",
        "            \"val_loss\": vaL,\n",
        "            \"val_dice\": vaD,\n",
        "            \"val_iou\": vaI,\n",
        "            \"val_acc\": vaA,\n",
        "            \"saved_ckpt\": saved_ckpt,\n",
        "            \"marker\": \"X\" if saved_ckpt else \"\",\n",
        "        }\n",
        "    )\n",
        "\n",
        "# Save metrics as CSV\n",
        "metrics_csv = os.path.join(OUT_GRAPHS_DIR, f\"metrics_{run_tag}_client{CLIENT_ID}.csv\")\n",
        "pd.DataFrame(log_rows).to_csv(metrics_csv, index=False)\n",
        "print(f\"[Log] Wrote per-epoch metrics CSV -> {metrics_csv}\\n\")\n",
        "\n",
        "# Plot training curves (tagged)\n",
        "curves_path = os.path.join(OUT_GRAPHS_DIR, f\"training_curves_{run_tag}_client{CLIENT_ID}.png\")\n",
        "plot_metrics(history, curves_path)\n",
        "\n",
        "# Run inference with best model and visualize a few validation samples\n",
        "infer_and_visualize_best(\n",
        "    model=model,\n",
        "    val_dataset=val_dataset,\n",
        "    use_atlas=USE_ATLAS,\n",
        "    out_dir=OUT_GRAPHS_DIR,\n",
        "    client_id=CLIENT_ID,\n",
        "    best_ckpt_path=best_path,\n",
        "    k_samples=3,\n",
        "    threshold=0.5,\n",
        ")"
      ],
      "metadata": {
        "id": "BSXYT79OULsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>UNet - \"mit_b3\"<h2>**"
      ],
      "metadata": {
        "id": "xMKUalT3VBME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, pickle, random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "from sklearn.metrics import jaccard_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "# Paths and I/O config\n",
        "DATA_ROOT = \"/content/Preprocessed-Data\"\n",
        "METADATA_DF_PATH = \"cleaned_df.pkl\"\n",
        "OUT_BASE = \"AITDM\"\n",
        "CLIENT_DIR = os.path.join(OUT_BASE, \"/content/client\")\n",
        "\n",
        "# Experiment config\n",
        "USE_ATLAS = True\n",
        "CLIENT_ID = 0\n",
        "BATCH_SIZE = 8\n",
        "LR = 1e-3\n",
        "EPOCHS = 50\n",
        "NUM_WORKERS = 2\n",
        "SEED = 42\n",
        "\n",
        "# Model / encoder config\n",
        "ENCODER_NAME = \"mit_b3\"\n",
        "ENCODER_WEIGHTS = \"imagenet\"\n",
        "\n",
        "# Output directories\n",
        "OUT_MODELS_DIR = os.path.join(OUT_BASE, \"Models\", \"UNet_ImageOnly\")\n",
        "OUT_GRAPHS_DIR = os.path.join(OUT_BASE, \"Graphs\")\n",
        "os.makedirs(OUT_MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(OUT_GRAPHS_DIR, exist_ok=True)\n",
        "\n",
        "def sanitize(s: str) -> str:\n",
        "    return str(s).replace(\"/\", \"-\").replace(\" \", \"_\")\n",
        "\n",
        "encoder_tag = sanitize(ENCODER_NAME)\n",
        "weights_tag = sanitize(ENCODER_WEIGHTS) if ENCODER_WEIGHTS is not None else \"none\"\n",
        "atlas_tag = \"atlas\" if USE_ATLAS else \"img\"\n",
        "\n",
        "run_tag = f\"unet_{encoder_tag}_{weights_tag}_{atlas_tag}\"\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.use_deterministic_algorithms(False)\n",
        "\n",
        "def worker_init_fn(worker_id):\n",
        "    worker_seed = SEED + worker_id\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "set_seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class ImageOnlyGliomaDataset(Dataset):\n",
        "    \"\"\"Dataset that loads MRI, tumor mask, and optional atlas regions per patient.\"\"\"\n",
        "\n",
        "    def __init__(self, metadata_df_path, data_root, use_atlas=True, exclude_ids=None, transform=None):\n",
        "        import pickle, os, numpy as np, pandas as pd\n",
        "\n",
        "        # Load metadata dataframe\n",
        "        with open(metadata_df_path, \"rb\") as f:\n",
        "            df = pickle.load(f)\n",
        "\n",
        "        # Optionally exclude some patient IDs\n",
        "        if exclude_ids is None:\n",
        "            exclude_ids = [\"PatientID_0191\"]\n",
        "        self.df = df[~df[\"Patient_ID\"].isin(exclude_ids)].reset_index(drop=True)\n",
        "\n",
        "        self.data_root = data_root\n",
        "        self.use_atlas = use_atlas\n",
        "        self.transform = transform\n",
        "\n",
        "        # Collect patient IDs that have all required files\n",
        "        self.patient_ids = []\n",
        "        for pid in sorted(self.df[\"Patient_ID\"].tolist()):\n",
        "            mri_p = os.path.join(self.data_root, pid, f\"{pid}_mri.npy\")\n",
        "            tumor_p = os.path.join(self.data_root, pid, f\"{pid}_tumor.npy\")\n",
        "\n",
        "            if self.use_atlas:\n",
        "                reg_p = os.path.join(self.data_root, pid, f\"{pid}_regions.npy\")\n",
        "                if os.path.isfile(mri_p) and os.path.isfile(tumor_p) and os.path.isfile(reg_p):\n",
        "                    self.patient_ids.append(pid)\n",
        "            else:\n",
        "                if os.path.isfile(mri_p) and os.path.isfile(tumor_p):\n",
        "                    self.patient_ids.append(pid)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patient_ids)\n",
        "\n",
        "    @staticmethod\n",
        "    def _minmax(x):\n",
        "        \"\"\"Simple min-max normalization to [0, 1].\"\"\"\n",
        "        x = x.astype(np.float32)\n",
        "        mn, mx = np.min(x), np.max(x)\n",
        "        return (x - mn) / (mx - mn) if mx > mn else np.zeros_like(x, dtype=np.float32)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Load and return one sample (dict) for a patient.\"\"\"\n",
        "        import os, numpy as np\n",
        "\n",
        "        pid = self.patient_ids[idx]\n",
        "        base = os.path.join(self.data_root, pid)\n",
        "\n",
        "        # Load MRI and tumor mask\n",
        "        mri = np.load(os.path.join(base, f\"{pid}_mri.npy\")).astype(np.float32)\n",
        "        tumor = np.load(os.path.join(base, f\"{pid}_tumor.npy\")).astype(np.float32)\n",
        "\n",
        "        # Normalize MRI and binarize tumor mask\n",
        "        mri = self._minmax(mri)\n",
        "        tumor = (tumor > 0.5).astype(np.float32)\n",
        "\n",
        "        sample = {\"patient_id\": pid, \"mri\": mri, \"tumor\": tumor}\n",
        "\n",
        "        # Optionally load regions/atlas\n",
        "        if self.use_atlas:\n",
        "            regions = np.load(os.path.join(base, f\"{pid}_regions.npy\")).astype(np.float32)\n",
        "            regions = self._minmax(regions)\n",
        "            sample[\"regions\"] = regions\n",
        "\n",
        "        return sample\n",
        "\n",
        "def image_only_collate_fn(batch, use_atlas=True):\n",
        "    \"\"\"Custom collate: stack MRI (+ optional regions) and tumor into tensors.\"\"\"\n",
        "    mri = torch.stack([torch.tensor(it[\"mri\"]) for it in batch]).unsqueeze(1)\n",
        "    y = torch.stack([torch.tensor(it[\"tumor\"]) for it in batch]).unsqueeze(1)\n",
        "\n",
        "    if use_atlas:\n",
        "        regs = torch.stack([torch.tensor(it[\"regions\"]) for it in batch]).unsqueeze(1)\n",
        "        # Concatenate MRI and regions as channels\n",
        "        x = torch.cat([mri.float(), regs.float()], dim=1)\n",
        "    else:\n",
        "        x = mri.float()\n",
        "\n",
        "    return {\"x\": x, \"y\": y.float(), \"pid\": [it[\"patient_id\"] for it in batch]}\n",
        "\n",
        "class SubsetByPIDs(Dataset):\n",
        "    \"\"\"Wrap a dataset but keep only a subset of patient IDs.\"\"\"\n",
        "\n",
        "    def __init__(self, full_dataset: ImageOnlyGliomaDataset, pid_list: List[str]):\n",
        "        self.ds = full_dataset\n",
        "        # Map patient IDs to indices\n",
        "        pid_to_idx = {pid: i for i, pid in enumerate(self.ds.patient_ids)}\n",
        "        self.indices = [pid_to_idx[p] for p in pid_list if p in pid_to_idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.ds[self.indices[i]]\n",
        "\n",
        "# Load client-specific train/val patient IDs\n",
        "cdir = os.path.join(CLIENT_DIR, f\"client_{CLIENT_ID}\")\n",
        "with open(os.path.join(cdir, \"train_pids.json\"), \"r\") as f:\n",
        "    train_pids = json.load(f)\n",
        "with open(os.path.join(cdir, \"val_pids.json\"), \"r\") as f:\n",
        "    val_pids = json.load(f)\n",
        "\n",
        "# Build full dataset and then client subsets\n",
        "full_ds = ImageOnlyGliomaDataset(\n",
        "    METADATA_DF_PATH,\n",
        "    DATA_ROOT,\n",
        "    use_atlas=USE_ATLAS,\n",
        "    exclude_ids=[\"PatientID_0191\"],\n",
        ")\n",
        "train_dataset = SubsetByPIDs(full_ds, train_pids)\n",
        "val_dataset = SubsetByPIDs(full_ds, val_pids)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "    generator=torch.Generator().manual_seed(SEED),\n",
        "    worker_init_fn=worker_init_fn,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "    generator=torch.Generator().manual_seed(SEED),\n",
        "    worker_init_fn=worker_init_fn,\n",
        ")\n",
        "\n",
        "print(f\"Loaded client_{CLIENT_ID}: train patients={len(train_dataset)}, val patients={len(val_dataset)}\")\n",
        "\n",
        "# UNet model (from segmentation_models_pytorch)\n",
        "in_channels = 2 if USE_ATLAS else 1\n",
        "model = smp.Unet(\n",
        "    encoder_name=ENCODER_NAME,\n",
        "    encoder_weights=ENCODER_WEIGHTS,\n",
        "    in_channels=in_channels,\n",
        "    classes=1,\n",
        ").to(device)\n",
        "\n",
        "# Loss, optimizer, scheduler, and AMP scaler\n",
        "criterion = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
        "scheduler = optim.lr_scheduler.LinearLR(\n",
        "    optimizer, start_factor=1.0, end_factor=0.0, total_iters=EPOCHS\n",
        ")\n",
        "scaler = torch.amp.GradScaler(\"cuda\", enabled=(device.type == \"cuda\"))\n",
        "\n",
        "def calc_metrics(y_true, y_pred):\n",
        "    \"\"\"Compute Dice, IoU, and pixel accuracy.\"\"\"\n",
        "    y_true = y_true.astype(np.uint8).reshape(-1)\n",
        "    y_pred = y_pred.astype(np.uint8).reshape(-1)\n",
        "\n",
        "    inter = (y_true * y_pred).sum()\n",
        "    dice = (2.0 * inter) / (y_true.sum() + y_pred.sum() + 1e-8)\n",
        "\n",
        "    # IoU with sklearn, fallback if it fails\n",
        "    try:\n",
        "        iou = jaccard_score(y_true, y_pred, average=\"binary\")\n",
        "    except Exception:\n",
        "        union = y_true.sum() + y_pred.sum() - inter + 1e-8\n",
        "        iou = inter / union\n",
        "\n",
        "    acc = (y_true == y_pred).mean()\n",
        "    return float(dice), float(iou), float(acc)\n",
        "\n",
        "def plot_prediction(sample_imgs, save_path):\n",
        "    \"\"\"Plot MRI (+ optional regions), GT, and prediction for one batch.\"\"\"\n",
        "    if \"regions\" in sample_imgs:\n",
        "        fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
        "    else:\n",
        "        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # MRI\n",
        "    axs[0].imshow(sample_imgs[\"mri\"][0, 0], cmap=\"gray\")\n",
        "    axs[0].set_title(\"MRI\")\n",
        "    axs[0].axis(\"off\")\n",
        "\n",
        "    col = 1\n",
        "\n",
        "    # Regions (if available)\n",
        "    if \"regions\" in sample_imgs:\n",
        "        axs[col].imshow(sample_imgs[\"regions\"][0, 0], cmap=\"gray\")\n",
        "        axs[col].set_title(\"Regions\")\n",
        "        axs[col].axis(\"off\")\n",
        "        col += 1\n",
        "\n",
        "    # Ground truth\n",
        "    axs[col].imshow(sample_imgs[\"y_true\"][0, 0], cmap=\"gray\")\n",
        "    axs[col].set_title(\"Ground Truth\")\n",
        "    axs[col].axis(\"off\")\n",
        "    col += 1\n",
        "\n",
        "    # Prediction\n",
        "    axs[col].imshow(sample_imgs[\"y_pred\"][0, 0], cmap=\"gray\")\n",
        "    axs[col].set_title(\"Predicted\")\n",
        "    axs[col].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def plot_metrics(history, save_path):\n",
        "    \"\"\"Plot training and validation curves for loss and metrics.\"\"\"\n",
        "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
        "\n",
        "    plt.figure(figsize=(16, 12))\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\")\n",
        "    plt.plot(epochs, history[\"val_loss\"], label=\"Val Loss\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Loss\")\n",
        "\n",
        "    # Dice\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(epochs, history[\"train_dice\"], label=\"Train Dice\")\n",
        "    plt.plot(epochs, history[\"val_dice\"], label=\"Val Dice\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Dice\")\n",
        "\n",
        "    # IoU\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(epochs, history[\"train_iou\"], label=\"Train IoU\")\n",
        "    plt.plot(epochs, history[\"val_iou\"], label=\"Val IoU\")\n",
        "    plt.legend()\n",
        "    plt.title(\"IoU\")\n",
        "\n",
        "    # Accuracy\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(epochs, history[\"train_acc\"], label=\"Train Acc\")\n",
        "    plt.plot(epochs, history[\"val_acc\"], label=\"Val Acc\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Accuracy\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion, scaler):\n",
        "    \"\"\"One training epoch over the dataloader.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = total_dice = total_iou = total_acc = 0.0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        x = batch[\"x\"].to(device)\n",
        "        y = batch[\"y\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Mixed precision forward pass\n",
        "        with torch.amp.autocast(\"cuda\", enabled=(device.type == \"cuda\")):\n",
        "            preds = model(x)\n",
        "            loss = criterion(preds, y)\n",
        "\n",
        "        # Backward with gradient scaling\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "        # Convert predictions to binary and compute metrics\n",
        "        preds_prob = (torch.sigmoid(preds).detach().cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        y_np = (y.detach().cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        d, i, a = calc_metrics(y_np, preds_prob)\n",
        "\n",
        "        total_dice += d\n",
        "        total_iou += i\n",
        "        total_acc += a\n",
        "\n",
        "    n = len(dataloader)\n",
        "    return total_loss / n, total_dice / n, total_iou / n, total_acc / n\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_one_epoch(model, dataloader, criterion, keep_last_batch=True):\n",
        "    \"\"\"Evaluation loop over the validation dataloader.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = total_dice = total_iou = total_acc = 0.0\n",
        "    last_batch_imgs = None\n",
        "\n",
        "    for batch in dataloader:\n",
        "        x = batch[\"x\"].to(device)\n",
        "        y = batch[\"y\"].to(device)\n",
        "\n",
        "        preds = model(x)\n",
        "        loss = criterion(preds, y)\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "        preds_prob = (torch.sigmoid(preds).cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        y_np = (y.cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        d, i, a = calc_metrics(y_np, preds_prob)\n",
        "\n",
        "        total_dice += d\n",
        "        total_iou += i\n",
        "        total_acc += a\n",
        "\n",
        "        # Optionally keep last batch for visualization\n",
        "        if keep_last_batch:\n",
        "            ch0 = x[:, 0:1].cpu().numpy()\n",
        "            imgs = {\"mri\": ch0, \"y_true\": y.cpu().numpy(), \"y_pred\": preds_prob}\n",
        "            if x.shape[1] == 2:\n",
        "                ch1 = x[:, 1:2].cpu().numpy()\n",
        "                imgs[\"regions\"] = ch1\n",
        "            last_batch_imgs = imgs\n",
        "\n",
        "    n = len(dataloader)\n",
        "    return total_loss / n, total_dice / n, total_iou / n, total_acc / n, last_batch_imgs\n",
        "\n",
        "def infer_and_visualize_best(\n",
        "    model,\n",
        "    val_dataset,\n",
        "    use_atlas: bool,\n",
        "    out_dir: str,\n",
        "    client_id: int,\n",
        "    best_ckpt_path: str,\n",
        "    k_samples: int = 3,\n",
        "    threshold: float = 0.5,\n",
        "):\n",
        "    \"\"\"Load best checkpoint, run inference on a few validation samples, and save visualizations.\"\"\"\n",
        "    import random\n",
        "\n",
        "    # Make sampling deterministic\n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    if os.path.isfile(best_ckpt_path):\n",
        "        model.load_state_dict(torch.load(best_ckpt_path, map_location=device))\n",
        "        model.eval()\n",
        "        print(f\"[Best Model] Loaded: {best_ckpt_path}\\n\")\n",
        "    else:\n",
        "        print(f\"[Best Model] Missing checkpoint: {best_ckpt_path}\\n\")\n",
        "        return\n",
        "\n",
        "    k = min(k_samples, len(val_dataset))\n",
        "    if k == 0:\n",
        "        print(\"[Best Model] Empty val dataset.\\n\")\n",
        "        return\n",
        "\n",
        "    # Deterministic random subset of validation indices\n",
        "    idxs = random.sample(range(len(val_dataset)), k)\n",
        "\n",
        "    def _predict_one(sample):\n",
        "        \"\"\"Run model on one sample and return inputs and binarized prediction.\"\"\"\n",
        "        mri = torch.tensor(sample[\"mri\"]).unsqueeze(0).unsqueeze(0).float().to(device)\n",
        "\n",
        "        if use_atlas and (\"regions\" in sample):\n",
        "            regs = torch.tensor(sample[\"regions\"]).unsqueeze(0).unsqueeze(0).float().to(device)\n",
        "            x = torch.cat([mri, regs], dim=1)\n",
        "        else:\n",
        "            x = mri\n",
        "\n",
        "        with torch.no_grad():\n",
        "            prob = torch.sigmoid(model(x)).cpu().numpy()\n",
        "            pred_bin = (prob > threshold).astype(np.uint8)\n",
        "\n",
        "        return x.cpu().numpy(), pred_bin\n",
        "\n",
        "    saved_paths = []\n",
        "\n",
        "    # Save individual sample figures\n",
        "    for i, idx in enumerate(idxs, 1):\n",
        "        sample = val_dataset[idx]\n",
        "        pid = sample.get(\"patient_id\", f\"val_{idx}\")\n",
        "        x_np, pred_bin = _predict_one(sample)\n",
        "\n",
        "        imgs = {\n",
        "            \"mri\": x_np[:, 0:1],\n",
        "            \"y_true\": np.expand_dims(\n",
        "                np.expand_dims(sample[\"tumor\"], 0), 0\n",
        "            ).astype(np.float32),\n",
        "            \"y_pred\": pred_bin.astype(np.float32),\n",
        "        }\n",
        "\n",
        "        if use_atlas and (\"regions\" in sample):\n",
        "            imgs[\"regions\"] = x_np[:, 1:2]\n",
        "\n",
        "        out_path = os.path.join(\n",
        "            out_dir, f\"best_val_sample_{i}_client{client_id}_{pid}.png\"\n",
        "        )\n",
        "        plot_prediction(imgs, out_path)\n",
        "        saved_paths.append(out_path)\n",
        "\n",
        "    print(\"[Best Model] Saved individual figures:\")\n",
        "    for p in saved_paths:\n",
        "        print(\" -\", p)\n",
        "    print()\n",
        "\n",
        "    # Save grid figure\n",
        "    cols = 4 if use_atlas else 3\n",
        "    fig, axs = plt.subplots(k, cols, figsize=(5 * cols, 4 * k))\n",
        "\n",
        "    if k == 1:\n",
        "        axs = np.expand_dims(axs, 0)\n",
        "\n",
        "    for row, idx in enumerate(idxs):\n",
        "        sample = val_dataset[idx]\n",
        "        x_np, pred_bin = _predict_one(sample)\n",
        "\n",
        "        mri = x_np[0, 0]\n",
        "        gt = sample[\"tumor\"]\n",
        "        col = 0\n",
        "\n",
        "        axs[row, col].imshow(mri, cmap=\"gray\")\n",
        "        axs[row, col].set_title(\"MRI\")\n",
        "        axs[row, col].axis(\"off\")\n",
        "        col += 1\n",
        "\n",
        "        if use_atlas and (\"regions\" in sample):\n",
        "            regs = x_np[0, 1]\n",
        "            axs[row, col].imshow(regs, cmap=\"gray\")\n",
        "            axs[row, col].set_title(\"Regions\")\n",
        "            axs[row, col].axis(\"off\")\n",
        "            col += 1\n",
        "\n",
        "        axs[row, col].imshow(gt, cmap=\"gray\")\n",
        "        axs[row, col].set_title(\"Ground Truth\")\n",
        "        axs[row, col].axis(\"off\")\n",
        "        col += 1\n",
        "\n",
        "        axs[row, col].imshow(pred_bin[0, 0], cmap=\"gray\")\n",
        "        axs[row, col].set_title(\"Predicted (τ=0.5)\")\n",
        "        axs[row, col].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    grid_path = os.path.join(out_dir, f\"best_model_val_grid_{run_tag}_client{client_id}.png\")\n",
        "    plt.savefig(grid_path, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    print(f\"[Best Model] Saved grid -> {grid_path}\\n\")\n",
        "\n",
        "# History containers for training curves\n",
        "history = {\n",
        "    \"train_loss\": [],\n",
        "    \"train_dice\": [],\n",
        "    \"train_iou\": [],\n",
        "    \"train_acc\": [],\n",
        "    \"val_loss\": [],\n",
        "    \"val_dice\": [],\n",
        "    \"val_iou\": [],\n",
        "    \"val_acc\": [],\n",
        "}\n",
        "\n",
        "# Tracking best validation metrics\n",
        "best_val_iou = 0.0\n",
        "best_val_dice = -float(\"inf\")\n",
        "best_val_loss = float(\"inf\")\n",
        "best_path = os.path.join(OUT_MODELS_DIR, f\"best_{run_tag}_client{CLIENT_ID}.pth\")\n",
        "\n",
        "log_rows = []\n",
        "\n",
        "# Main training loop\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    # Train one epoch\n",
        "    trL, trD, trI, trA = train_one_epoch(\n",
        "        model, train_loader, optimizer, criterion, scaler\n",
        "    )\n",
        "    # Validate\n",
        "    vaL, vaD, vaI, vaA, _ = eval_one_epoch(\n",
        "        model, val_loader, criterion, keep_last_batch=True\n",
        "    )\n",
        "\n",
        "    # Update scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    # Save metrics for plots\n",
        "    history[\"train_loss\"].append(trL)\n",
        "    history[\"train_dice\"].append(trD)\n",
        "    history[\"train_iou\"].append(trI)\n",
        "    history[\"train_acc\"].append(trA)\n",
        "\n",
        "    history[\"val_loss\"].append(vaL)\n",
        "    history[\"val_dice\"].append(vaD)\n",
        "    history[\"val_iou\"].append(vaI)\n",
        "    history[\"val_acc\"].append(vaA)\n",
        "\n",
        "    print(\n",
        "        f\"[Epoch {epoch:03d}/{EPOCHS}] \"\n",
        "        f\"Train — Loss {trL:.4f} | Dice {trD:.4f} | IoU {trI:.4f} | Accuracy {trA:.4f} || \"\n",
        "        f\"Val — Loss {vaL:.4f} | Dice {vaD:.4f} | IoU {vaI:.4f} | Accuracy {vaA:.4f}\\n\"\n",
        "    )\n",
        "\n",
        "    # Save checkpoint if Dice improves and loss decreases\n",
        "    saved_ckpt = False\n",
        "    if (vaD > best_val_dice) and (vaL < best_val_loss):\n",
        "        best_val_dice = vaD\n",
        "        best_val_loss = vaL\n",
        "        torch.save(model.state_dict(), best_path)\n",
        "        saved_ckpt = True\n",
        "        print(\n",
        "            f\"Saved best model (Val Dice↑ {best_val_dice:.4f} & Val Loss↓ {best_val_loss:.4f}) -> {best_path}\\n\"\n",
        "        )\n",
        "\n",
        "    # Log row for CSV\n",
        "    log_rows.append(\n",
        "        {\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": trL,\n",
        "            \"train_dice\": trD,\n",
        "            \"train_iou\": trI,\n",
        "            \"train_acc\": trA,\n",
        "            \"val_loss\": vaL,\n",
        "            \"val_dice\": vaD,\n",
        "            \"val_iou\": vaI,\n",
        "            \"val_acc\": vaA,\n",
        "            \"saved_ckpt\": saved_ckpt,\n",
        "            \"marker\": \"X\" if saved_ckpt else \"\",\n",
        "        }\n",
        "    )\n",
        "\n",
        "# Save metrics as CSV\n",
        "metrics_csv = os.path.join(OUT_GRAPHS_DIR, f\"metrics_{run_tag}_client{CLIENT_ID}.csv\")\n",
        "pd.DataFrame(log_rows).to_csv(metrics_csv, index=False)\n",
        "print(f\"[Log] Wrote per-epoch metrics CSV -> {metrics_csv}\\n\")\n",
        "\n",
        "# Plot training curves (tagged)\n",
        "curves_path = os.path.join(OUT_GRAPHS_DIR, f\"training_curves_{run_tag}_client{CLIENT_ID}.png\")\n",
        "plot_metrics(history, curves_path)\n",
        "\n",
        "# Run inference with best model and visualize a few validation samples\n",
        "infer_and_visualize_best(\n",
        "    model=model,\n",
        "    val_dataset=val_dataset,\n",
        "    use_atlas=USE_ATLAS,\n",
        "    out_dir=OUT_GRAPHS_DIR,\n",
        "    client_id=CLIENT_ID,\n",
        "    best_ckpt_path=best_path,\n",
        "    k_samples=3,\n",
        "    threshold=0.5,\n",
        ")"
      ],
      "metadata": {
        "id": "LQFigtpPHpoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>DeepLabV3Plus - \"timm-mobilenetv3_small_100\"<h2>**"
      ],
      "metadata": {
        "id": "4ZjGJX8yVG0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, pickle, random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "from sklearn.metrics import jaccard_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "# Paths and I/O config\n",
        "DATA_ROOT = \"/content/Preprocessed-Data\"\n",
        "METADATA_DF_PATH = \"cleaned_df.pkl\"\n",
        "OUT_BASE = \"AITDM\"\n",
        "CLIENT_DIR = os.path.join(OUT_BASE, \"/content/client\")\n",
        "\n",
        "# Experiment config\n",
        "USE_ATLAS = True\n",
        "CLIENT_ID = 0\n",
        "BATCH_SIZE = 8\n",
        "LR = 1e-3\n",
        "EPOCHS = 50\n",
        "NUM_WORKERS = 2\n",
        "SEED = 42\n",
        "\n",
        "# Model / encoder config\n",
        "ENCODER_NAME = \"timm-mobilenetv3_small_100\"\n",
        "ENCODER_WEIGHTS = \"imagenet\"\n",
        "\n",
        "# Output directories\n",
        "OUT_MODELS_DIR = os.path.join(OUT_BASE, \"Models\", \"DeepLabV3Plus_ImageOnly\")\n",
        "OUT_GRAPHS_DIR = os.path.join(OUT_BASE, \"Graphs\")\n",
        "os.makedirs(OUT_MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(OUT_GRAPHS_DIR, exist_ok=True)\n",
        "\n",
        "def sanitize(s: str) -> str:\n",
        "    return str(s).replace(\"/\", \"-\").replace(\" \", \"_\")\n",
        "\n",
        "encoder_tag = sanitize(ENCODER_NAME)\n",
        "weights_tag = sanitize(ENCODER_WEIGHTS) if ENCODER_WEIGHTS is not None else \"none\"\n",
        "atlas_tag = \"atlas\" if USE_ATLAS else \"img\"\n",
        "\n",
        "run_tag = f\"deeplabv3plus_{encoder_tag}_{weights_tag}_{atlas_tag}\"\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.use_deterministic_algorithms(False)\n",
        "\n",
        "def worker_init_fn(worker_id):\n",
        "    worker_seed = SEED + worker_id\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "set_seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class ImageOnlyGliomaDataset(Dataset):\n",
        "    \"\"\"Dataset that loads MRI, tumor mask, and optional atlas regions per patient.\"\"\"\n",
        "\n",
        "    def __init__(self, metadata_df_path, data_root, use_atlas=True, exclude_ids=None, transform=None):\n",
        "        import pickle, os, numpy as np, pandas as pd\n",
        "\n",
        "        # Load metadata dataframe\n",
        "        with open(metadata_df_path, \"rb\") as f:\n",
        "            df = pickle.load(f)\n",
        "\n",
        "        # Optionally exclude some patient IDs\n",
        "        if exclude_ids is None:\n",
        "            exclude_ids = [\"PatientID_0191\"]\n",
        "        self.df = df[~df[\"Patient_ID\"].isin(exclude_ids)].reset_index(drop=True)\n",
        "\n",
        "        self.data_root = data_root\n",
        "        self.use_atlas = use_atlas\n",
        "        self.transform = transform\n",
        "\n",
        "        # Collect patient IDs that have all required files\n",
        "        self.patient_ids = []\n",
        "        for pid in sorted(self.df[\"Patient_ID\"].tolist()):\n",
        "            mri_p = os.path.join(self.data_root, pid, f\"{pid}_mri.npy\")\n",
        "            tumor_p = os.path.join(self.data_root, pid, f\"{pid}_tumor.npy\")\n",
        "\n",
        "            if self.use_atlas:\n",
        "                reg_p = os.path.join(self.data_root, pid, f\"{pid}_regions.npy\")\n",
        "                if os.path.isfile(mri_p) and os.path.isfile(tumor_p) and os.path.isfile(reg_p):\n",
        "                    self.patient_ids.append(pid)\n",
        "            else:\n",
        "                if os.path.isfile(mri_p) and os.path.isfile(tumor_p):\n",
        "                    self.patient_ids.append(pid)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patient_ids)\n",
        "\n",
        "    @staticmethod\n",
        "    def _minmax(x):\n",
        "        \"\"\"Simple min-max normalization to [0, 1].\"\"\"\n",
        "        x = x.astype(np.float32)\n",
        "        mn, mx = np.min(x), np.max(x)\n",
        "        return (x - mn) / (mx - mn) if mx > mn else np.zeros_like(x, dtype=np.float32)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Load and return one sample (dict) for a patient.\"\"\"\n",
        "        import os, numpy as np\n",
        "\n",
        "        pid = self.patient_ids[idx]\n",
        "        base = os.path.join(self.data_root, pid)\n",
        "\n",
        "        # Load MRI and tumor mask\n",
        "        mri = np.load(os.path.join(base, f\"{pid}_mri.npy\")).astype(np.float32)\n",
        "        tumor = np.load(os.path.join(base, f\"{pid}_tumor.npy\")).astype(np.float32)\n",
        "\n",
        "        # Normalize MRI and binarize tumor mask\n",
        "        mri = self._minmax(mri)\n",
        "        tumor = (tumor > 0.5).astype(np.float32)\n",
        "\n",
        "        sample = {\"patient_id\": pid, \"mri\": mri, \"tumor\": tumor}\n",
        "\n",
        "        # Optionally load regions/atlas\n",
        "        if self.use_atlas:\n",
        "            regions = np.load(os.path.join(base, f\"{pid}_regions.npy\")).astype(np.float32)\n",
        "            regions = self._minmax(regions)\n",
        "            sample[\"regions\"] = regions\n",
        "\n",
        "        return sample\n",
        "\n",
        "def image_only_collate_fn(batch, use_atlas=True):\n",
        "    \"\"\"Custom collate: stack MRI (+ optional regions) and tumor into tensors.\"\"\"\n",
        "    mri = torch.stack([torch.tensor(it[\"mri\"]) for it in batch]).unsqueeze(1)\n",
        "    y = torch.stack([torch.tensor(it[\"tumor\"]) for it in batch]).unsqueeze(1)\n",
        "\n",
        "    if use_atlas:\n",
        "        regs = torch.stack([torch.tensor(it[\"regions\"]) for it in batch]).unsqueeze(1)\n",
        "        # Concatenate MRI and regions as channels\n",
        "        x = torch.cat([mri.float(), regs.float()], dim=1)\n",
        "    else:\n",
        "        x = mri.float()\n",
        "\n",
        "    return {\"x\": x, \"y\": y.float(), \"pid\": [it[\"patient_id\"] for it in batch]}\n",
        "\n",
        "class SubsetByPIDs(Dataset):\n",
        "    \"\"\"Wrap a dataset but keep only a subset of patient IDs.\"\"\"\n",
        "\n",
        "    def __init__(self, full_dataset: ImageOnlyGliomaDataset, pid_list: List[str]):\n",
        "        self.ds = full_dataset\n",
        "        # Map patient IDs to indices\n",
        "        pid_to_idx = {pid: i for i, pid in enumerate(self.ds.patient_ids)}\n",
        "        self.indices = [pid_to_idx[p] for p in pid_list if p in pid_to_idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.ds[self.indices[i]]\n",
        "\n",
        "# Load client-specific train/val patient IDs\n",
        "cdir = os.path.join(CLIENT_DIR, f\"client_{CLIENT_ID}\")\n",
        "with open(os.path.join(cdir, \"train_pids.json\"), \"r\") as f:\n",
        "    train_pids = json.load(f)\n",
        "with open(os.path.join(cdir, \"val_pids.json\"), \"r\") as f:\n",
        "    val_pids = json.load(f)\n",
        "\n",
        "# Build full dataset and then client subsets\n",
        "full_ds = ImageOnlyGliomaDataset(\n",
        "    METADATA_DF_PATH,\n",
        "    DATA_ROOT,\n",
        "    use_atlas=USE_ATLAS,\n",
        "    exclude_ids=[\"PatientID_0191\"],\n",
        ")\n",
        "train_dataset = SubsetByPIDs(full_ds, train_pids)\n",
        "val_dataset = SubsetByPIDs(full_ds, val_pids)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "    generator=torch.Generator().manual_seed(SEED),\n",
        "    worker_init_fn=worker_init_fn,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "    generator=torch.Generator().manual_seed(SEED),\n",
        "    worker_init_fn=worker_init_fn,\n",
        ")\n",
        "\n",
        "print(f\"Loaded client_{CLIENT_ID}: train patients={len(train_dataset)}, val patients={len(val_dataset)}\")\n",
        "\n",
        "# UNet model (from segmentation_models_pytorch)\n",
        "in_channels = 2 if USE_ATLAS else 1\n",
        "model = smp.DeepLabV3Plus(\n",
        "    encoder_name=ENCODER_NAME,\n",
        "    encoder_weights=ENCODER_WEIGHTS,\n",
        "    in_channels=in_channels,\n",
        "    classes=1,\n",
        ").to(device)\n",
        "\n",
        "# Loss, optimizer, scheduler, and AMP scaler\n",
        "criterion = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
        "scheduler = optim.lr_scheduler.LinearLR(\n",
        "    optimizer, start_factor=1.0, end_factor=0.0, total_iters=EPOCHS\n",
        ")\n",
        "scaler = torch.amp.GradScaler(\"cuda\", enabled=(device.type == \"cuda\"))\n",
        "\n",
        "def calc_metrics(y_true, y_pred):\n",
        "    \"\"\"Compute Dice, IoU, and pixel accuracy.\"\"\"\n",
        "    y_true = y_true.astype(np.uint8).reshape(-1)\n",
        "    y_pred = y_pred.astype(np.uint8).reshape(-1)\n",
        "\n",
        "    inter = (y_true * y_pred).sum()\n",
        "    dice = (2.0 * inter) / (y_true.sum() + y_pred.sum() + 1e-8)\n",
        "\n",
        "    # IoU with sklearn, fallback if it fails\n",
        "    try:\n",
        "        iou = jaccard_score(y_true, y_pred, average=\"binary\")\n",
        "    except Exception:\n",
        "        union = y_true.sum() + y_pred.sum() - inter + 1e-8\n",
        "        iou = inter / union\n",
        "\n",
        "    acc = (y_true == y_pred).mean()\n",
        "    return float(dice), float(iou), float(acc)\n",
        "\n",
        "def plot_prediction(sample_imgs, save_path):\n",
        "    \"\"\"Plot MRI (+ optional regions), GT, and prediction for one batch.\"\"\"\n",
        "    if \"regions\" in sample_imgs:\n",
        "        fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
        "    else:\n",
        "        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # MRI\n",
        "    axs[0].imshow(sample_imgs[\"mri\"][0, 0], cmap=\"gray\")\n",
        "    axs[0].set_title(\"MRI\")\n",
        "    axs[0].axis(\"off\")\n",
        "\n",
        "    col = 1\n",
        "\n",
        "    # Regions (if available)\n",
        "    if \"regions\" in sample_imgs:\n",
        "        axs[col].imshow(sample_imgs[\"regions\"][0, 0], cmap=\"gray\")\n",
        "        axs[col].set_title(\"Regions\")\n",
        "        axs[col].axis(\"off\")\n",
        "        col += 1\n",
        "\n",
        "    # Ground truth\n",
        "    axs[col].imshow(sample_imgs[\"y_true\"][0, 0], cmap=\"gray\")\n",
        "    axs[col].set_title(\"Ground Truth\")\n",
        "    axs[col].axis(\"off\")\n",
        "    col += 1\n",
        "\n",
        "    # Prediction\n",
        "    axs[col].imshow(sample_imgs[\"y_pred\"][0, 0], cmap=\"gray\")\n",
        "    axs[col].set_title(\"Predicted\")\n",
        "    axs[col].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def plot_metrics(history, save_path):\n",
        "    \"\"\"Plot training and validation curves for loss and metrics.\"\"\"\n",
        "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
        "\n",
        "    plt.figure(figsize=(16, 12))\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\")\n",
        "    plt.plot(epochs, history[\"val_loss\"], label=\"Val Loss\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Loss\")\n",
        "\n",
        "    # Dice\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(epochs, history[\"train_dice\"], label=\"Train Dice\")\n",
        "    plt.plot(epochs, history[\"val_dice\"], label=\"Val Dice\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Dice\")\n",
        "\n",
        "    # IoU\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(epochs, history[\"train_iou\"], label=\"Train IoU\")\n",
        "    plt.plot(epochs, history[\"val_iou\"], label=\"Val IoU\")\n",
        "    plt.legend()\n",
        "    plt.title(\"IoU\")\n",
        "\n",
        "    # Accuracy\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(epochs, history[\"train_acc\"], label=\"Train Acc\")\n",
        "    plt.plot(epochs, history[\"val_acc\"], label=\"Val Acc\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Accuracy\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion, scaler):\n",
        "    \"\"\"One training epoch over the dataloader.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = total_dice = total_iou = total_acc = 0.0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        x = batch[\"x\"].to(device)\n",
        "        y = batch[\"y\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Mixed precision forward pass\n",
        "        with torch.amp.autocast(\"cuda\", enabled=(device.type == \"cuda\")):\n",
        "            preds = model(x)\n",
        "            loss = criterion(preds, y)\n",
        "\n",
        "        # Backward with gradient scaling\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "        # Convert predictions to binary and compute metrics\n",
        "        preds_prob = (torch.sigmoid(preds).detach().cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        y_np = (y.detach().cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        d, i, a = calc_metrics(y_np, preds_prob)\n",
        "\n",
        "        total_dice += d\n",
        "        total_iou += i\n",
        "        total_acc += a\n",
        "\n",
        "    n = len(dataloader)\n",
        "    return total_loss / n, total_dice / n, total_iou / n, total_acc / n\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_one_epoch(model, dataloader, criterion, keep_last_batch=True):\n",
        "    \"\"\"Evaluation loop over the validation dataloader.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = total_dice = total_iou = total_acc = 0.0\n",
        "    last_batch_imgs = None\n",
        "\n",
        "    for batch in dataloader:\n",
        "        x = batch[\"x\"].to(device)\n",
        "        y = batch[\"y\"].to(device)\n",
        "\n",
        "        preds = model(x)\n",
        "        loss = criterion(preds, y)\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "        preds_prob = (torch.sigmoid(preds).cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        y_np = (y.cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        d, i, a = calc_metrics(y_np, preds_prob)\n",
        "\n",
        "        total_dice += d\n",
        "        total_iou += i\n",
        "        total_acc += a\n",
        "\n",
        "        # Optionally keep last batch for visualization\n",
        "        if keep_last_batch:\n",
        "            ch0 = x[:, 0:1].cpu().numpy()\n",
        "            imgs = {\"mri\": ch0, \"y_true\": y.cpu().numpy(), \"y_pred\": preds_prob}\n",
        "            if x.shape[1] == 2:\n",
        "                ch1 = x[:, 1:2].cpu().numpy()\n",
        "                imgs[\"regions\"] = ch1\n",
        "            last_batch_imgs = imgs\n",
        "\n",
        "    n = len(dataloader)\n",
        "    return total_loss / n, total_dice / n, total_iou / n, total_acc / n, last_batch_imgs\n",
        "\n",
        "def infer_and_visualize_best(\n",
        "    model,\n",
        "    val_dataset,\n",
        "    use_atlas: bool,\n",
        "    out_dir: str,\n",
        "    client_id: int,\n",
        "    best_ckpt_path: str,\n",
        "    k_samples: int = 3,\n",
        "    threshold: float = 0.5,\n",
        "):\n",
        "    \"\"\"Load best checkpoint, run inference on a few validation samples, and save visualizations.\"\"\"\n",
        "    import random\n",
        "\n",
        "    # Make sampling deterministic\n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    if os.path.isfile(best_ckpt_path):\n",
        "        model.load_state_dict(torch.load(best_ckpt_path, map_location=device))\n",
        "        model.eval()\n",
        "        print(f\"[Best Model] Loaded: {best_ckpt_path}\\n\")\n",
        "    else:\n",
        "        print(f\"[Best Model] Missing checkpoint: {best_ckpt_path}\\n\")\n",
        "        return\n",
        "\n",
        "    k = min(k_samples, len(val_dataset))\n",
        "    if k == 0:\n",
        "        print(\"[Best Model] Empty val dataset.\\n\")\n",
        "        return\n",
        "\n",
        "    # Deterministic random subset of validation indices\n",
        "    idxs = random.sample(range(len(val_dataset)), k)\n",
        "\n",
        "    def _predict_one(sample):\n",
        "        \"\"\"Run model on one sample and return inputs and binarized prediction.\"\"\"\n",
        "        mri = torch.tensor(sample[\"mri\"]).unsqueeze(0).unsqueeze(0).float().to(device)\n",
        "\n",
        "        if use_atlas and (\"regions\" in sample):\n",
        "            regs = torch.tensor(sample[\"regions\"]).unsqueeze(0).unsqueeze(0).float().to(device)\n",
        "            x = torch.cat([mri, regs], dim=1)\n",
        "        else:\n",
        "            x = mri\n",
        "\n",
        "        with torch.no_grad():\n",
        "            prob = torch.sigmoid(model(x)).cpu().numpy()\n",
        "            pred_bin = (prob > threshold).astype(np.uint8)\n",
        "\n",
        "        return x.cpu().numpy(), pred_bin\n",
        "\n",
        "    saved_paths = []\n",
        "\n",
        "    # Save individual sample figures\n",
        "    for i, idx in enumerate(idxs, 1):\n",
        "        sample = val_dataset[idx]\n",
        "        pid = sample.get(\"patient_id\", f\"val_{idx}\")\n",
        "        x_np, pred_bin = _predict_one(sample)\n",
        "\n",
        "        imgs = {\n",
        "            \"mri\": x_np[:, 0:1],\n",
        "            \"y_true\": np.expand_dims(\n",
        "                np.expand_dims(sample[\"tumor\"], 0), 0\n",
        "            ).astype(np.float32),\n",
        "            \"y_pred\": pred_bin.astype(np.float32),\n",
        "        }\n",
        "\n",
        "        if use_atlas and (\"regions\" in sample):\n",
        "            imgs[\"regions\"] = x_np[:, 1:2]\n",
        "\n",
        "        out_path = os.path.join(\n",
        "            out_dir, f\"best_val_sample_{i}_client{client_id}_{pid}.png\"\n",
        "        )\n",
        "        plot_prediction(imgs, out_path)\n",
        "        saved_paths.append(out_path)\n",
        "\n",
        "    print(\"[Best Model] Saved individual figures:\")\n",
        "    for p in saved_paths:\n",
        "        print(\" -\", p)\n",
        "    print()\n",
        "\n",
        "    # Save grid figure\n",
        "    cols = 4 if use_atlas else 3\n",
        "    fig, axs = plt.subplots(k, cols, figsize=(5 * cols, 4 * k))\n",
        "\n",
        "    if k == 1:\n",
        "        axs = np.expand_dims(axs, 0)\n",
        "\n",
        "    for row, idx in enumerate(idxs):\n",
        "        sample = val_dataset[idx]\n",
        "        x_np, pred_bin = _predict_one(sample)\n",
        "\n",
        "        mri = x_np[0, 0]\n",
        "        gt = sample[\"tumor\"]\n",
        "        col = 0\n",
        "\n",
        "        axs[row, col].imshow(mri, cmap=\"gray\")\n",
        "        axs[row, col].set_title(\"MRI\")\n",
        "        axs[row, col].axis(\"off\")\n",
        "        col += 1\n",
        "\n",
        "        if use_atlas and (\"regions\" in sample):\n",
        "            regs = x_np[0, 1]\n",
        "            axs[row, col].imshow(regs, cmap=\"gray\")\n",
        "            axs[row, col].set_title(\"Regions\")\n",
        "            axs[row, col].axis(\"off\")\n",
        "            col += 1\n",
        "\n",
        "        axs[row, col].imshow(gt, cmap=\"gray\")\n",
        "        axs[row, col].set_title(\"Ground Truth\")\n",
        "        axs[row, col].axis(\"off\")\n",
        "        col += 1\n",
        "\n",
        "        axs[row, col].imshow(pred_bin[0, 0], cmap=\"gray\")\n",
        "        axs[row, col].set_title(\"Predicted (τ=0.5)\")\n",
        "        axs[row, col].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    grid_path = os.path.join(out_dir, f\"best_model_val_grid_{run_tag}_client{client_id}.png\")\n",
        "    plt.savefig(grid_path, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    print(f\"[Best Model] Saved grid -> {grid_path}\\n\")\n",
        "\n",
        "# History containers for training curves\n",
        "history = {\n",
        "    \"train_loss\": [],\n",
        "    \"train_dice\": [],\n",
        "    \"train_iou\": [],\n",
        "    \"train_acc\": [],\n",
        "    \"val_loss\": [],\n",
        "    \"val_dice\": [],\n",
        "    \"val_iou\": [],\n",
        "    \"val_acc\": [],\n",
        "}\n",
        "\n",
        "# Tracking best validation metrics\n",
        "best_val_iou = 0.0\n",
        "best_val_dice = -float(\"inf\")\n",
        "best_val_loss = float(\"inf\")\n",
        "best_path = os.path.join(OUT_MODELS_DIR, f\"best_{run_tag}_client{CLIENT_ID}.pth\")\n",
        "\n",
        "log_rows = []\n",
        "\n",
        "# Main training loop\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    # Train one epoch\n",
        "    trL, trD, trI, trA = train_one_epoch(\n",
        "        model, train_loader, optimizer, criterion, scaler\n",
        "    )\n",
        "    # Validate\n",
        "    vaL, vaD, vaI, vaA, _ = eval_one_epoch(\n",
        "        model, val_loader, criterion, keep_last_batch=True\n",
        "    )\n",
        "\n",
        "    # Update scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    # Save metrics for plots\n",
        "    history[\"train_loss\"].append(trL)\n",
        "    history[\"train_dice\"].append(trD)\n",
        "    history[\"train_iou\"].append(trI)\n",
        "    history[\"train_acc\"].append(trA)\n",
        "\n",
        "    history[\"val_loss\"].append(vaL)\n",
        "    history[\"val_dice\"].append(vaD)\n",
        "    history[\"val_iou\"].append(vaI)\n",
        "    history[\"val_acc\"].append(vaA)\n",
        "\n",
        "    print(\n",
        "        f\"[Epoch {epoch:03d}/{EPOCHS}] \"\n",
        "        f\"Train — Loss {trL:.4f} | Dice {trD:.4f} | IoU {trI:.4f} | Accuracy {trA:.4f} || \"\n",
        "        f\"Val — Loss {vaL:.4f} | Dice {vaD:.4f} | IoU {vaI:.4f} | Accuracy {vaA:.4f}\\n\"\n",
        "    )\n",
        "\n",
        "    # Save checkpoint if Dice improves and loss decreases\n",
        "    saved_ckpt = False\n",
        "    if (vaD > best_val_dice) and (vaL < best_val_loss):\n",
        "        best_val_dice = vaD\n",
        "        best_val_loss = vaL\n",
        "        torch.save(model.state_dict(), best_path)\n",
        "        saved_ckpt = True\n",
        "        print(\n",
        "            f\"Saved best model (Val Dice↑ {best_val_dice:.4f} & Val Loss↓ {best_val_loss:.4f}) -> {best_path}\\n\"\n",
        "        )\n",
        "\n",
        "    # Log row for CSV\n",
        "    log_rows.append(\n",
        "        {\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": trL,\n",
        "            \"train_dice\": trD,\n",
        "            \"train_iou\": trI,\n",
        "            \"train_acc\": trA,\n",
        "            \"val_loss\": vaL,\n",
        "            \"val_dice\": vaD,\n",
        "            \"val_iou\": vaI,\n",
        "            \"val_acc\": vaA,\n",
        "            \"saved_ckpt\": saved_ckpt,\n",
        "            \"marker\": \"X\" if saved_ckpt else \"\",\n",
        "        }\n",
        "    )\n",
        "\n",
        "# Save metrics as CSV\n",
        "metrics_csv = os.path.join(OUT_GRAPHS_DIR, f\"metrics_{run_tag}_client{CLIENT_ID}.csv\")\n",
        "pd.DataFrame(log_rows).to_csv(metrics_csv, index=False)\n",
        "print(f\"[Log] Wrote per-epoch metrics CSV -> {metrics_csv}\\n\")\n",
        "\n",
        "# Plot training curves (tagged)\n",
        "curves_path = os.path.join(OUT_GRAPHS_DIR, f\"training_curves_{run_tag}_client{CLIENT_ID}.png\")\n",
        "plot_metrics(history, curves_path)\n",
        "\n",
        "# Run inference with best model and visualize a few validation samples\n",
        "infer_and_visualize_best(\n",
        "    model=model,\n",
        "    val_dataset=val_dataset,\n",
        "    use_atlas=USE_ATLAS,\n",
        "    out_dir=OUT_GRAPHS_DIR,\n",
        "    client_id=CLIENT_ID,\n",
        "    best_ckpt_path=best_path,\n",
        "    k_samples=3,\n",
        "    threshold=0.5,\n",
        ")"
      ],
      "metadata": {
        "id": "VvaGPwgQPnIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>Ensemble<h2>**"
      ],
      "metadata": {
        "id": "xdquMfwqMDg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import segmentation_models_pytorch as smp\n",
        "from sklearn.metrics import jaccard_score\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SEED = 42\n",
        "USE_ATLAS = True\n",
        "THRESHOLD = 0.5\n",
        "K_SAMPLES = 3\n",
        "\n",
        "OUT_BASE = \"AITDM\"\n",
        "OUT_GRAPHS_DIR = os.path.join(OUT_BASE, \"Graphs\")\n",
        "os.makedirs(OUT_GRAPHS_DIR, exist_ok=True)\n",
        "\n",
        "OUT_MODELS_UNET_DIR = os.path.join(OUT_BASE, \"Models\", \"UNet_ImageOnly\")\n",
        "CKPT_RESNET50 = os.path.join(OUT_MODELS_UNET_DIR, \"best_unet_resnet50_imagenet_atlas_client0.pth\")\n",
        "CKPT_MITB3 = os.path.join(OUT_MODELS_UNET_DIR, \"best_unet_mit_b3_imagenet_atlas_client0.pth\")\n",
        "\n",
        "OUT_MODELS_DLV3P_DIR = os.path.join(OUT_BASE, \"Models\", \"DeepLabV3Plus_ImageOnly\")\n",
        "CKPT_DLV3P = os.path.join(\n",
        "    OUT_MODELS_DLV3P_DIR,\n",
        "    \"best_deeplabv3plus_timm-mobilenetv3_small_100_imagenet_atlas_client0.pth\",\n",
        ")\n",
        "\n",
        "ENS_WEIGHTS = [2.5 / 10, 2.5 / 10, 5 / 10]\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "set_seed(SEED)\n",
        "\n",
        "\n",
        "def calc_metrics(y_true, y_pred):\n",
        "    y_true = y_true.astype(np.uint8).reshape(-1)\n",
        "    y_pred = y_pred.astype(np.uint8).reshape(-1)\n",
        "\n",
        "    inter = (y_true * y_pred).sum()\n",
        "    dice = (2.0 * inter) / (y_true.sum() + y_pred.sum() + 1e-8)\n",
        "\n",
        "    try:\n",
        "        iou = jaccard_score(y_true, y_pred, average=\"binary\")\n",
        "    except Exception:\n",
        "        union = y_true.sum() + y_pred.sum() - inter + 1e-8\n",
        "        iou = inter / union\n",
        "\n",
        "    acc = (y_true == y_pred).mean()\n",
        "    return float(dice), float(iou), float(acc)\n",
        "\n",
        "\n",
        "def plot_prediction(imgs, save_path, title=None):\n",
        "    has_regions = \"regions\" in imgs\n",
        "    cols = 4 if has_regions else 3\n",
        "    fig, axs = plt.subplots(1, cols, figsize=(5 * cols, 5))\n",
        "\n",
        "    axs[0].imshow(imgs[\"mri\"][0, 0], cmap=\"gray\")\n",
        "    axs[0].set_title(\"MRI\")\n",
        "    axs[0].axis(\"off\")\n",
        "\n",
        "    c = 1\n",
        "    if has_regions:\n",
        "        axs[c].imshow(imgs[\"regions\"][0, 0], cmap=\"gray\")\n",
        "        axs[c].set_title(\"Regions\")\n",
        "        axs[c].axis(\"off\")\n",
        "        c += 1\n",
        "\n",
        "    axs[c].imshow(imgs[\"y_true\"][0, 0], cmap=\"gray\")\n",
        "    axs[c].set_title(\"Ground Truth\")\n",
        "    axs[c].axis(\"off\")\n",
        "    c += 1\n",
        "\n",
        "    axs[c].imshow(imgs[\"y_pred\"][0, 0], cmap=\"gray\")\n",
        "    axs[c].set_title(f\"Ensemble (τ={THRESHOLD})\")\n",
        "    axs[c].axis(\"off\")\n",
        "\n",
        "    if title:\n",
        "        fig.suptitle(title, y=1.02)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def build_unet(encoder_name, encoder_weights=\"imagenet\", in_channels=2, classes=1):\n",
        "    return smp.Unet(\n",
        "        encoder_name=encoder_name,\n",
        "        encoder_weights=encoder_weights,\n",
        "        in_channels=in_channels,\n",
        "        classes=classes,\n",
        "    )\n",
        "\n",
        "\n",
        "def build_deeplabv3p(encoder_name, encoder_weights=\"imagenet\", in_channels=2, classes=1):\n",
        "    return smp.DeepLabV3Plus(\n",
        "        encoder_name=encoder_name,\n",
        "        encoder_weights=encoder_weights,\n",
        "        in_channels=in_channels,\n",
        "        classes=classes,\n",
        "    )\n",
        "\n",
        "\n",
        "def load_model_unet(encoder_name, ckpt_path, in_channels):\n",
        "    assert os.path.isfile(ckpt_path), f\"Missing checkpoint: {ckpt_path}\"\n",
        "    m = build_unet(encoder_name, \"imagenet\", in_channels=in_channels, classes=1).to(DEVICE)\n",
        "    m.load_state_dict(torch.load(ckpt_path, map_location=DEVICE))\n",
        "    m.eval()\n",
        "    return m\n",
        "\n",
        "\n",
        "def load_model_dlv3p(encoder_name, ckpt_path, in_channels):\n",
        "    assert os.path.isfile(ckpt_path), f\"Missing checkpoint: {ckpt_path}\"\n",
        "    m = build_deeplabv3p(encoder_name, \"imagenet\", in_channels=in_channels, classes=1).to(DEVICE)\n",
        "    m.load_state_dict(torch.load(ckpt_path, map_location=DEVICE))\n",
        "    m.eval()\n",
        "    return m\n",
        "\n",
        "\n",
        "in_channels = 2 if USE_ATLAS else 1\n",
        "model_r50 = load_model_unet(\"resnet50\", CKPT_RESNET50, in_channels=in_channels)\n",
        "model_mit3 = load_model_unet(\"mit_b3\", CKPT_MITB3, in_channels=in_channels)\n",
        "model_dlv3p = load_model_dlv3p(\"timm-mobilenetv3_small_100\", CKPT_DLV3P, in_channels=in_channels)\n",
        "\n",
        "MODELS = [model_r50, model_mit3, model_dlv3p]\n",
        "criterion = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def ensemble_forward_logits_multi(x, models, weights, target_hw=None):\n",
        "    # Weighted sum of logits; resize to target_hw if needed\n",
        "    w = np.array(weights, dtype=np.float32)\n",
        "    w = w / (w.sum() + 1e-8)\n",
        "\n",
        "    logits_sum = None\n",
        "    for mi, wi in zip(models, w):\n",
        "        li = mi(x)\n",
        "        if target_hw is not None and li.shape[-2:] != target_hw:\n",
        "            li = F.interpolate(li, size=target_hw, mode=\"bilinear\", align_corners=False)\n",
        "        logits_sum = li * float(wi) if logits_sum is None else logits_sum + li * float(wi)\n",
        "\n",
        "    return logits_sum\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_one_epoch_ensemble(dataloader, models, weights, threshold=0.5, criterion=None, keep_last_batch=True):\n",
        "    # Metrics are computed per-batch, then averaged across batches\n",
        "    total_loss = total_dice = total_iou = total_acc = 0.0\n",
        "    last_batch_imgs = None\n",
        "    n = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        x = batch[\"x\"].to(DEVICE)\n",
        "        y = batch[\"y\"].to(DEVICE)\n",
        "\n",
        "        logits = ensemble_forward_logits_multi(x, models=models, weights=weights, target_hw=y.shape[-2:])\n",
        "\n",
        "        if criterion is not None:\n",
        "            loss = criterion(logits, y)\n",
        "            total_loss += float(loss.item())\n",
        "\n",
        "        preds_bin = (torch.sigmoid(logits).detach().cpu().numpy() > threshold).astype(np.uint8)\n",
        "        y_np = (y.detach().cpu().numpy() > 0.5).astype(np.uint8)\n",
        "\n",
        "        d, i, a = calc_metrics(y_np, preds_bin)\n",
        "        total_dice += d\n",
        "        total_iou += i\n",
        "        total_acc += a\n",
        "        n += 1\n",
        "\n",
        "        if keep_last_batch:\n",
        "            ch0 = x[:, 0:1].detach().cpu().numpy()\n",
        "            imgs = {\"mri\": ch0, \"y_true\": y.detach().cpu().numpy(), \"y_pred\": preds_bin}\n",
        "            if x.shape[1] == 2:\n",
        "                imgs[\"regions\"] = x[:, 1:2].detach().cpu().numpy()\n",
        "            last_batch_imgs = imgs\n",
        "\n",
        "    mean_loss = (total_loss / n) if (criterion is not None and n > 0) else None\n",
        "    mean_dice = total_dice / max(n, 1)\n",
        "    mean_iou = total_iou / max(n, 1)\n",
        "    mean_acc = total_acc / max(n, 1)\n",
        "\n",
        "    return mean_loss, mean_dice, mean_iou, mean_acc, last_batch_imgs\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def ensemble_infer_and_visualize(val_dataset, out_dir, models, weights, k_samples=3, threshold=0.5):\n",
        "    # Visualization only (random K patients)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    k = min(k_samples, len(val_dataset))\n",
        "    if k == 0:\n",
        "        print(\"Empty val_dataset.\")\n",
        "        return\n",
        "\n",
        "    set_seed(SEED)\n",
        "    idxs = random.sample(range(len(val_dataset)), k)\n",
        "    per_sample = []\n",
        "\n",
        "    for i, idx in enumerate(idxs, 1):\n",
        "        sample = val_dataset[idx]\n",
        "        pid = sample.get(\"patient_id\", f\"val_{idx}\")\n",
        "\n",
        "        mri = torch.tensor(sample[\"mri\"]).unsqueeze(0).unsqueeze(0).float()\n",
        "        y = torch.tensor(sample[\"tumor\"]).unsqueeze(0).unsqueeze(0).float()\n",
        "\n",
        "        if USE_ATLAS and (\"regions\" in sample):\n",
        "            reg = torch.tensor(sample[\"regions\"]).unsqueeze(0).unsqueeze(0).float()\n",
        "            x = torch.cat([mri, reg], dim=1)\n",
        "        else:\n",
        "            x = mri\n",
        "\n",
        "        x = x.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "        logits = ensemble_forward_logits_multi(x, models=models, weights=weights, target_hw=y.shape[-2:])\n",
        "        pred_bin = (torch.sigmoid(logits).cpu().numpy() > threshold).astype(np.uint8)\n",
        "        y_np = (y.cpu().numpy() > 0.5).astype(np.uint8)\n",
        "\n",
        "        d, iou, acc = calc_metrics(y_np, pred_bin)\n",
        "\n",
        "        x_np = x.detach().cpu().numpy()\n",
        "        imgs = {\"mri\": x_np[:, 0:1], \"y_true\": y.detach().cpu().numpy(), \"y_pred\": pred_bin.astype(np.float32)}\n",
        "        if USE_ATLAS and x_np.shape[1] == 2:\n",
        "            imgs[\"regions\"] = x_np[:, 1:2]\n",
        "\n",
        "        out_path = os.path.join(out_dir, f\"ensemble3_val_sample_{i}_{pid}.png\")\n",
        "        plot_prediction(imgs, out_path, title=f\"{pid} | Dice={d:.4f} IoU={iou:.4f} Acc={acc:.4f}\")\n",
        "        per_sample.append({\"pid\": pid, \"dice\": d, \"iou\": iou, \"acc\": acc, \"path\": out_path})\n",
        "\n",
        "    print(\"[Ensemble-3] Saved individual figures:\")\n",
        "    for r in per_sample:\n",
        "        print(f\" - {r['pid']}: Dice={r['dice']:.4f} IoU={r['iou']:.4f} Acc={r['acc']:.4f} -> {r['path']}\")\n",
        "\n",
        "    cols = 4 if USE_ATLAS else 3\n",
        "    fig, axs = plt.subplots(k, cols, figsize=(5 * cols, 4 * k))\n",
        "    if k == 1:\n",
        "        axs = np.expand_dims(axs, 0)\n",
        "\n",
        "    for row, idx in enumerate(idxs):\n",
        "        sample = val_dataset[idx]\n",
        "\n",
        "        mri = torch.tensor(sample[\"mri\"]).unsqueeze(0).unsqueeze(0).float()\n",
        "        y = torch.tensor(sample[\"tumor\"]).unsqueeze(0).unsqueeze(0).float()\n",
        "\n",
        "        if USE_ATLAS and (\"regions\" in sample):\n",
        "            reg = torch.tensor(sample[\"regions\"]).unsqueeze(0).unsqueeze(0).float()\n",
        "            x = torch.cat([mri, reg], dim=1)\n",
        "        else:\n",
        "            x = mri\n",
        "\n",
        "        x = x.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "        logits = ensemble_forward_logits_multi(x, models=models, weights=weights, target_hw=y.shape[-2:])\n",
        "        pred_bin = (torch.sigmoid(logits).cpu().numpy() > threshold).astype(np.uint8)\n",
        "        y_np = (y.cpu().numpy() > 0.5).astype(np.uint8)\n",
        "\n",
        "        d, _, _ = calc_metrics(y_np, pred_bin)\n",
        "        x_np = x.detach().cpu().numpy()\n",
        "\n",
        "        c = 0\n",
        "        axs[row, c].imshow(x_np[0, 0], cmap=\"gray\")\n",
        "        axs[row, c].set_title(\"MRI\")\n",
        "        axs[row, c].axis(\"off\")\n",
        "        c += 1\n",
        "\n",
        "        if USE_ATLAS and x_np.shape[1] == 2:\n",
        "            axs[row, c].imshow(x_np[0, 1], cmap=\"gray\")\n",
        "            axs[row, c].set_title(\"Regions\")\n",
        "            axs[row, c].axis(\"off\")\n",
        "            c += 1\n",
        "\n",
        "        axs[row, c].imshow(y_np[0, 0], cmap=\"gray\")\n",
        "        axs[row, c].set_title(\"GT\")\n",
        "        axs[row, c].axis(\"off\")\n",
        "        c += 1\n",
        "\n",
        "        axs[row, c].imshow(pred_bin[0, 0], cmap=\"gray\")\n",
        "        axs[row, c].set_title(f\"Ens τ={threshold}\\nDice={d:.3f}\")\n",
        "        axs[row, c].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    wtag = \"-\".join([f\"{w:.2f}\" for w in (np.array(weights) / (np.sum(weights) + 1e-8))])\n",
        "    grid_path = os.path.join(out_dir, f\"ensemble3_val_grid_tau{threshold}_w{wtag}.png\")\n",
        "    plt.savefig(grid_path, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    print(f\"[Ensemble-3] Saved grid -> {grid_path}\")\n",
        "\n",
        "\n",
        "# Assumes val_loader and val_dataset already exist in your notebook\n",
        "ens_loss, ens_d, ens_i, ens_a, last_imgs = eval_one_epoch_ensemble(\n",
        "    dataloader=val_loader,\n",
        "    models=MODELS,\n",
        "    weights=ENS_WEIGHTS,\n",
        "    threshold=THRESHOLD,\n",
        "    criterion=criterion,\n",
        "    keep_last_batch=True,\n",
        ")\n",
        "\n",
        "print(f\"[Ensemble-3 Val] Loss {ens_loss:.4f} | Dice {ens_d:.4f} | IoU {ens_i:.4f} | Acc {ens_a:.4f}\")\n",
        "\n",
        "if last_imgs is not None:\n",
        "    out_path = os.path.join(OUT_GRAPHS_DIR, f\"ensemble3_last_batch_tau{THRESHOLD}.png\")\n",
        "    plot_prediction(last_imgs, out_path, title=\"Ensemble-3 - last val batch\")\n",
        "\n",
        "ensemble_infer_and_visualize(\n",
        "    val_dataset=val_dataset,\n",
        "    out_dir=OUT_GRAPHS_DIR,\n",
        "    models=MODELS,\n",
        "    weights=ENS_WEIGHTS,\n",
        "    k_samples=K_SAMPLES,\n",
        "    threshold=THRESHOLD,\n",
        ")"
      ],
      "metadata": {
        "id": "GmoBrDCRS57T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **M2 - FL with ensemble**"
      ],
      "metadata": {
        "id": "3Pf52xcpQyUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile seg_data.py\n",
        "import os, pickle, numpy as np, torch\n",
        "from typing import List\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchio as tio\n",
        "\n",
        "# Global paths and configuration\n",
        "DATA_ROOT = \"/content/Preprocessed-Data\"\n",
        "METADATA_DF_PATH = \"cleaned_df.pkl\"\n",
        "USE_ATLAS = True\n",
        "EXCLUDE_IDS = [\"PatientID_0191\"]\n",
        "\n",
        "# Dataset that loads MRI, tumor mask and optional atlas for each patient\n",
        "class ImageOnlyGliomaDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        metadata_df_path,\n",
        "        data_root,\n",
        "        use_atlas=False,\n",
        "        exclude_ids=None,\n",
        "        transform=None,\n",
        "    ):\n",
        "        with open(metadata_df_path, \"rb\") as f:\n",
        "            df = pickle.load(f)\n",
        "\n",
        "        if exclude_ids is None:\n",
        "            exclude_ids = []\n",
        "\n",
        "        self.df = df[~df[\"Patient_ID\"].isin(exclude_ids)].reset_index(drop=True)\n",
        "        self.data_root = data_root\n",
        "        self.use_atlas = use_atlas\n",
        "        self.transform = transform\n",
        "\n",
        "        self.patient_ids = []\n",
        "        for pid in sorted(self.df[\"Patient_ID\"].tolist()):\n",
        "            base = os.path.join(self.data_root, pid)\n",
        "            mri_p = os.path.join(base, f\"{pid}_mri.npy\")\n",
        "            tumor_p = os.path.join(base, f\"{pid}_tumor.npy\")\n",
        "\n",
        "            is_valid = os.path.isfile(mri_p) and os.path.isfile(tumor_p)\n",
        "            if self.use_atlas:\n",
        "                reg_p = os.path.join(base, f\"{pid}_regions.npy\")\n",
        "                is_valid = is_valid and os.path.isfile(reg_p)\n",
        "\n",
        "            if is_valid:\n",
        "                self.patient_ids.append(pid)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patient_ids)\n",
        "\n",
        "    @staticmethod\n",
        "    def _minmax(x):\n",
        "        x = x.astype(np.float32)\n",
        "        mn, mx = np.min(x), np.max(x)\n",
        "        if mx > mn:\n",
        "            return (x - mn) / (mx - mn)\n",
        "        return np.zeros_like(x, dtype=np.float32)\n",
        "\n",
        "    def _to_torchio_format(self, arr):\n",
        "        \"\"\"\n",
        "        Convertește (H, W) -> (1, H, W, 1) pentru procesare internă TorchIO.\n",
        "        \"\"\"\n",
        "        if arr.ndim == 2:\n",
        "            return arr[np.newaxis, ..., np.newaxis]\n",
        "        return arr\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pid = self.patient_ids[idx]\n",
        "        base = os.path.join(self.data_root, pid)\n",
        "\n",
        "        mri = np.load(os.path.join(base, f\"{pid}_mri.npy\")).astype(np.float32)\n",
        "        tumor = np.load(os.path.join(base, f\"{pid}_tumor.npy\")).astype(np.float32)\n",
        "\n",
        "\n",
        "        mri = self._minmax(mri)\n",
        "        tumor = (tumor > 0.5).astype(np.float32)\n",
        "\n",
        "\n",
        "        regions = None\n",
        "        if self.use_atlas:\n",
        "            regions = np.load(os.path.join(base, f\"{pid}_regions.npy\")).astype(np.float32)\n",
        "\n",
        "        mri = self._minmax(mri)\n",
        "        tumor = (tumor > 0.5).astype(np.float32)\n",
        "\n",
        "        if regions is not None:\n",
        "            regions = self._minmax(regions)\n",
        "\n",
        "\n",
        "        if self.transform:\n",
        "            subject_dict = {\n",
        "                'mri': tio.ScalarImage(tensor=self._to_torchio_format(mri)),\n",
        "                'tumor': tio.LabelMap(tensor=self._to_torchio_format(tumor)),\n",
        "            }\n",
        "            if regions is not None:\n",
        "                subject_dict['regions'] = tio.ScalarImage(tensor=self._to_torchio_format(regions))\n",
        "\n",
        "            subject = tio.Subject(subject_dict)\n",
        "\n",
        "            subject = self.transform(subject)\n",
        "\n",
        "            out_mri = subject['mri'].data[0, ..., 0].numpy()\n",
        "            out_tumor = subject['tumor'].data[0, ..., 0].numpy()\n",
        "            sample = {\n",
        "                \"patient_id\": pid,\n",
        "                \"mri\": out_mri,       # Shape: (240, 240)\n",
        "                \"tumor\": out_tumor    # Shape: (240, 240)\n",
        "            }\n",
        "\n",
        "            if regions is not None:\n",
        "                out_regions = subject['regions'].data[0, ..., 0].numpy()\n",
        "                sample[\"regions\"] = out_regions # Shape: (240, 240)\n",
        "\n",
        "            return sample\n",
        "\n",
        "\n",
        "        sample = {\"patient_id\": pid, \"mri\": mri, \"tumor\": tumor}\n",
        "\n",
        "        if self.use_atlas:\n",
        "            sample[\"regions\"] = regions\n",
        "\n",
        "        return sample\n",
        "\n",
        "# Collate function to build batched tensors and patient ID list\n",
        "def image_only_collate_fn(batch, use_atlas=USE_ATLAS):\n",
        "    mri = torch.stack([torch.tensor(it[\"mri\"]) for it in batch]).unsqueeze(1)\n",
        "    y = torch.stack([torch.tensor(it[\"tumor\"]) for it in batch]).unsqueeze(1)\n",
        "\n",
        "    if use_atlas:\n",
        "        regs = torch.stack([torch.tensor(it[\"regions\"]) for it in batch]).unsqueeze(1)\n",
        "        x = torch.cat([mri.float(), regs.float()], dim=1)\n",
        "    else:\n",
        "        x = mri.float()\n",
        "\n",
        "    return {\"x\": x, \"y\": y.float(), \"pid\": [it[\"patient_id\"] for it in batch]}\n",
        "\n",
        "# Dataset wrapper that restricts to a subset of patient IDs\n",
        "class SubsetByPIDs(Dataset):\n",
        "    def __init__(self, full_dataset: ImageOnlyGliomaDataset, pid_list: List[str]):\n",
        "        self.ds = full_dataset\n",
        "        pid_to_idx = {pid: i for i, pid in enumerate(self.ds.patient_ids)}\n",
        "        self.indices = [pid_to_idx[p] for p in pid_list if p in pid_to_idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.ds[self.indices[i]]\n",
        "\n",
        "# Compute Dice, IoU and accuracy for binary masks\n",
        "def calc_metrics(y_true, y_pred):\n",
        "    y_true = y_true.astype(np.uint8).reshape(-1)\n",
        "    y_pred = y_pred.astype(np.uint8).reshape(-1)\n",
        "\n",
        "    inter = (y_true & y_pred).sum()\n",
        "    dice = (2.0 * inter) / (y_true.sum() + y_pred.sum() + 1e-8)\n",
        "    union = y_true.sum() + y_pred.sum() - inter + 1e-8\n",
        "    iou = inter / union\n",
        "    acc = (y_true == y_pred).mean()\n",
        "\n",
        "    return float(dice), float(iou), float(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPgflPCuQ5vh",
        "outputId": "1c8d3507-97f0-4058-a1fa-e40d4e61f38f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing seg_data.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile fl_client.py\n",
        "import os\n",
        "os.environ.setdefault(\"CUBLAS_WORKSPACE_CONFIG\", \":4096:8\")\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import flwr as fl\n",
        "import copy\n",
        "import segmentation_models_pytorch as smp\n",
        "import random\n",
        "import torchio as tio\n",
        "import sys\n",
        "import time\n",
        "\n",
        "from seg_data import (\n",
        "    ImageOnlyGliomaDataset,\n",
        "    SubsetByPIDs,\n",
        "    image_only_collate_fn,\n",
        "    calc_metrics,\n",
        "    DATA_ROOT,\n",
        "    METADATA_DF_PATH,\n",
        "    USE_ATLAS,\n",
        ")\n",
        "\n",
        "CLIENT_DIR = \"/content/client\"\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "NUM_WORKERS = 2\n",
        "SEED = 42\n",
        "\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "DEFAULT_MODEL_NAME = \"unet\"\n",
        "DEFAULT_ENCODER_NAME = \"timm-mobilenetv3_small_100\"\n",
        "DEFAULT_ENCODER_WEIGHTS = \"imagenet\"\n",
        "TRANSFORMS = None\n",
        "\n",
        "\n",
        "def _run_name(model_name: str, encoder_name: str) -> str:\n",
        "    return f\"{model_name}__{encoder_name}\".replace(\"/\", \"-\")\n",
        "\n",
        "\n",
        "def seed_everything(seed: int, deterministic: bool = True) -> None:\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    try:\n",
        "        torch.set_float32_matmul_precision(\"high\")\n",
        "    except Exception:\n",
        "        pass\n",
        "    if deterministic:\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.use_deterministic_algorithms(True)\n",
        "\n",
        "\n",
        "def seed_worker(worker_id: int) -> None:\n",
        "    worker_seed = (torch.initial_seed() + worker_id) % (2**32)\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "\n",
        "def log_transfer_metrics(file_path, cid, rnd, incoming, outgoing, overhead):\n",
        "    \"\"\"Functie helper pentru a salva metricile de transfer in fisier.\"\"\"\n",
        "    directory = os.path.dirname(file_path)\n",
        "    if directory and not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    msg = (f\"Client {cid} Round {rnd}: \"\n",
        "           f\"Incoming {incoming/1024:.2f} KB | \"\n",
        "           f\"Outgoing {outgoing/1024:.2f} KB | \"\n",
        "           f\"Overhead: {overhead:.6f}s\\n\")\n",
        "\n",
        "\n",
        "    with open(file_path, \"a\") as f:\n",
        "        f.write(msg)\n",
        "\n",
        "\n",
        "def get_model(\n",
        "    model_name=DEFAULT_MODEL_NAME,\n",
        "    encoder_name=DEFAULT_ENCODER_NAME,\n",
        "    encoder_weights=DEFAULT_ENCODER_WEIGHTS,\n",
        "):\n",
        "    in_ch = 2 if USE_ATLAS else 1\n",
        "    mn = model_name.lower()\n",
        "\n",
        "    if mn == \"unet\":\n",
        "        model = smp.Unet(\n",
        "            encoder_name=encoder_name,\n",
        "            encoder_weights=encoder_weights,\n",
        "            in_channels=in_ch,\n",
        "            classes=1,\n",
        "        )\n",
        "    elif mn in [\"deeplabv3plus\", \"deeplabv3+\", \"dlv3p\"]:\n",
        "        model = smp.DeepLabV3Plus(\n",
        "            encoder_name=encoder_name,\n",
        "            encoder_weights=encoder_weights,\n",
        "            in_channels=in_ch,\n",
        "            classes=1,\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model_name={model_name}. Use 'unet' or 'deeplabv3plus'.\")\n",
        "\n",
        "    return model.to(DEVICE)\n",
        "\n",
        "\n",
        "def get_loaders(cid: int, base_seed: int, transforms):\n",
        "    full = ImageOnlyGliomaDataset(\n",
        "        METADATA_DF_PATH,\n",
        "        DATA_ROOT,\n",
        "        use_atlas=USE_ATLAS,\n",
        "        exclude_ids=[\"PatientID_0191\"],\n",
        "        transform=transforms,\n",
        "    )\n",
        "\n",
        "    with open(os.path.join(CLIENT_DIR, f\"client_{cid}\", \"train_pids.json\")) as f:\n",
        "        tr_p = json.load(f)\n",
        "    with open(os.path.join(CLIENT_DIR, f\"client_{cid}\", \"val_pids.json\")) as f:\n",
        "        va_p = json.load(f)\n",
        "\n",
        "    ds_tr = SubsetByPIDs(full, tr_p)\n",
        "    ds_va = SubsetByPIDs(full, va_p)\n",
        "\n",
        "    g_tr = torch.Generator().manual_seed(base_seed + 12345)\n",
        "    g_va = torch.Generator().manual_seed(base_seed + 67890)\n",
        "\n",
        "    ld_tr = DataLoader(\n",
        "        ds_tr,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "        collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=g_tr,\n",
        "        persistent_workers=(NUM_WORKERS > 0),\n",
        "    )\n",
        "\n",
        "    ld_va = DataLoader(\n",
        "        ds_va,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "        collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=g_va,\n",
        "        persistent_workers=(NUM_WORKERS > 0),\n",
        "    )\n",
        "\n",
        "    return ld_tr, ld_va, len(ds_tr), len(ds_va)\n",
        "\n",
        "\n",
        "def get_parameters(model):\n",
        "    return [p.detach().cpu().numpy() for _, p in model.state_dict().items()]\n",
        "\n",
        "\n",
        "def set_parameters(model, params):\n",
        "    sd = model.state_dict()\n",
        "    for k, v in zip(sd.keys(), params):\n",
        "        sd[k] = torch.tensor(v)\n",
        "    model.load_state_dict(sd, strict=True)\n",
        "\n",
        "\n",
        "bce = nn.BCEWithLogitsLoss()\n",
        "dice_loss = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
        "\n",
        "\n",
        "def criterion(pred, y):\n",
        "    return 0.5 * bce(pred, y) + 0.5 * dice_loss(pred, y)\n",
        "\n",
        "\n",
        "def maybe_save_best(run_dir, cid, val_loss, val_dice, best_epoch, rnd, model):\n",
        "    ckpt_dir = os.path.join(run_dir, \"checkpoints\")\n",
        "    os.makedirs(ckpt_dir, exist_ok=True)\n",
        "\n",
        "    best_json = os.path.join(ckpt_dir, f\"client_{cid}_best.json\")\n",
        "    best_pt = os.path.join(ckpt_dir, f\"client_{cid}_best.pt\")\n",
        "\n",
        "    prev = {\"val_loss\": float(\"inf\"), \"val_dice\": -1.0}\n",
        "    if os.path.isfile(best_json):\n",
        "        try:\n",
        "            with open(best_json, \"r\") as f:\n",
        "                prev = json.load(f)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    improved = (val_loss < prev.get(\"val_loss\", float(\"inf\"))) and (val_dice > prev.get(\"val_dice\", -1.0))\n",
        "    if improved:\n",
        "        torch.save(model.state_dict(), best_pt)\n",
        "        with open(best_json, \"w\") as f:\n",
        "            json.dump(\n",
        "                {\n",
        "                    \"round\": int(rnd),\n",
        "                    \"epoch\": int(best_epoch),\n",
        "                    \"val_loss\": float(val_loss),\n",
        "                    \"val_dice\": float(val_dice),\n",
        "                },\n",
        "                f,\n",
        "            )\n",
        "\n",
        "\n",
        "class SegClient(fl.client.NumPyClient):\n",
        "    def __init__(\n",
        "        self,\n",
        "        cid: int,\n",
        "        model_name=DEFAULT_MODEL_NAME,\n",
        "        encoder_name=DEFAULT_ENCODER_NAME,\n",
        "        encoder_weights=DEFAULT_ENCODER_WEIGHTS,\n",
        "    ):\n",
        "        self.cid = int(cid)\n",
        "        self.model_name = model_name\n",
        "        self.encoder_name = encoder_name\n",
        "        self.encoder_weights = encoder_weights\n",
        "\n",
        "        self.base_seed = SEED + self.cid\n",
        "        seed_everything(self.base_seed, deterministic=True)\n",
        "\n",
        "        self.run_name = _run_name(model_name, encoder_name)\n",
        "        self.run_dir = os.path.join(\"AITDM\", self.run_name)\n",
        "\n",
        "        self.model = get_model(model_name, encoder_name, encoder_weights)\n",
        "        if self.cid != 2:\n",
        "            self.train_loader, self.val_loader, self.ntr, self.nva = get_loaders(self.cid, self.base_seed, transforms=TRANSFORMS)\n",
        "        else:\n",
        "            self.train_loader, self.val_loader, self.ntr, self.nva = get_loaders(self.cid, self.base_seed, transforms=TRANSFORMS)\n",
        "\n",
        "\n",
        "    def get_parameters(self, config):\n",
        "        return get_parameters(self.model)\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        # --- MEASURE INCOMING ---\n",
        "        incoming_size = sum([p.nbytes for p in parameters])\n",
        "\n",
        "        set_parameters(self.model, parameters)\n",
        "\n",
        "        epochs = int(config.get(\"local_epochs\", 1))\n",
        "        lr = float(config.get(\"lr\", 1e-3))\n",
        "        rnd = int(config.get(\"round\", 0))\n",
        "\n",
        "        opt = optim.AdamW(self.model.parameters(), lr=lr)\n",
        "        scaler = torch.amp.GradScaler(\"cuda\", enabled=(DEVICE.type == \"cuda\"))\n",
        "\n",
        "        best_state = None\n",
        "        best_val_loss = float(\"inf\")\n",
        "        best_val_dice = -1.0\n",
        "        best_epoch_idx = -1\n",
        "        epoch_logs = []\n",
        "\n",
        "        for epoch_idx in range(1, epochs + 1):\n",
        "            self.model.train()\n",
        "            tot_tr_loss = tot_tr_d = tot_tr_i = tot_tr_a = 0.0\n",
        "            nb_tr = 0\n",
        "\n",
        "            for batch in self.train_loader:\n",
        "                x = batch[\"x\"].to(DEVICE)\n",
        "                y = batch[\"y\"].to(DEVICE)\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "\n",
        "                with torch.amp.autocast(\"cuda\", enabled=(DEVICE.type == \"cuda\")):\n",
        "                    pred = self.model(x)\n",
        "                    loss = criterion(pred, y)\n",
        "\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(opt)\n",
        "                scaler.update()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    y_hat = (torch.sigmoid(pred).detach().cpu().numpy() > 0.5).astype(np.uint8)\n",
        "                    y_np = (y.detach().cpu().numpy() > 0.5).astype(np.uint8)\n",
        "                    d, i, a = calc_metrics(y_np, y_hat)\n",
        "\n",
        "                tot_tr_loss += float(loss.item())\n",
        "                tot_tr_d += d\n",
        "                tot_tr_i += i\n",
        "                tot_tr_a += a\n",
        "                nb_tr += 1\n",
        "\n",
        "            nb_tr = max(nb_tr, 1)\n",
        "            epoch_tr_loss = tot_tr_loss / nb_tr\n",
        "            epoch_tr_dice = tot_tr_d / nb_tr\n",
        "            epoch_tr_iou = tot_tr_i / nb_tr\n",
        "            epoch_tr_acc = tot_tr_a / nb_tr\n",
        "\n",
        "            self.model.eval()\n",
        "            tot_val_loss = tot_val_d = tot_val_i = tot_val_a = 0.0\n",
        "            nb_val = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in self.val_loader:\n",
        "                    x = batch[\"x\"].to(DEVICE)\n",
        "                    y = batch[\"y\"].to(DEVICE)\n",
        "                    pred = self.model(x)\n",
        "\n",
        "                    v_loss = float(criterion(pred, y).item())\n",
        "                    y_hat = (torch.sigmoid(pred).cpu().numpy() > 0.5).astype(np.uint8)\n",
        "                    y_np = (y.cpu().numpy() > 0.5).astype(np.uint8)\n",
        "                    d, i, a = calc_metrics(y_np, y_hat)\n",
        "\n",
        "                    tot_val_loss += v_loss\n",
        "                    tot_val_d += d\n",
        "                    tot_val_i += i\n",
        "                    tot_val_a += a\n",
        "                    nb_val += 1\n",
        "\n",
        "            nb_val = max(nb_val, 1)\n",
        "            epoch_val_loss = tot_val_loss / nb_val\n",
        "            epoch_val_dice = tot_val_d / nb_val\n",
        "            epoch_val_iou = tot_val_i / nb_val\n",
        "            epoch_val_acc = tot_val_a / nb_val\n",
        "\n",
        "            epoch_logs.append(\n",
        "                {\n",
        "                    \"epoch\": int(epoch_idx),\n",
        "                    \"train_loss\": float(epoch_tr_loss),\n",
        "                    \"train_dice\": float(epoch_tr_dice),\n",
        "                    \"train_iou\": float(epoch_tr_iou),\n",
        "                    \"train_acc\": float(epoch_tr_acc),\n",
        "                    \"val_loss\": float(epoch_val_loss),\n",
        "                    \"val_dice\": float(epoch_val_dice),\n",
        "                    \"val_iou\": float(epoch_val_iou),\n",
        "                    \"val_acc\": float(epoch_val_acc),\n",
        "                }\n",
        "            )\n",
        "\n",
        "            if (epoch_val_loss < best_val_loss) and (epoch_val_dice > best_val_dice):\n",
        "                best_val_loss = epoch_val_loss\n",
        "                best_val_dice = epoch_val_dice\n",
        "                best_state = copy.deepcopy(self.model.state_dict())\n",
        "                best_epoch_idx = epoch_idx\n",
        "\n",
        "        if best_state is not None:\n",
        "            self.model.load_state_dict(best_state)\n",
        "\n",
        "        for ep in epoch_logs:\n",
        "            ep[\"best_epoch\"] = (ep[\"epoch\"] == best_epoch_idx)\n",
        "\n",
        "        train_metrics = {\n",
        "            \"cid\": int(self.cid),\n",
        "            \"best_epoch\": int(best_epoch_idx),\n",
        "            \"best_val_loss\": float(best_val_loss),\n",
        "            \"best_val_dice\": float(best_val_dice),\n",
        "            \"per_epoch\": json.dumps(epoch_logs),\n",
        "            \"run_name\": self.run_name,\n",
        "            \"model_name\": self.model_name,\n",
        "            \"encoder_name\": self.encoder_name,\n",
        "        }\n",
        "\n",
        "        maybe_save_best(self.run_dir, self.cid, best_val_loss, best_val_dice, best_epoch_idx, rnd, self.model)\n",
        "\n",
        "        # --- PREPARE & MEASURE OUTGOING ---\n",
        "        out_params = get_parameters(self.model)\n",
        "\n",
        "\n",
        "        start_overhead = time.time()\n",
        "        final_params_to_send = out_params\n",
        "        end_overhead = time.time()\n",
        "\n",
        "        outgoing_size = sum([p.nbytes for p in final_params_to_send])\n",
        "\n",
        "        print(f\"Client {self.cid} Round {rnd}: Incoming {incoming_size/1024:.2f} KB | Outgoing {outgoing_size/1024:.2f} KB | Overhead: {end_overhead - start_overhead:.6f}s\")\n",
        "        log_file_path = os.path.join(self.run_dir, f\"client_{self.cid}_transfer_log.txt\")\n",
        "        log_transfer_metrics(log_file_path, self.cid, rnd, incoming_size, outgoing_size, end_overhead - start_overhead)\n",
        "\n",
        "        return final_params_to_send, self.ntr, train_metrics\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "        set_parameters(self.model, parameters)\n",
        "        self.model.eval()\n",
        "\n",
        "        tot_loss = tot_d = tot_i = tot_a = 0.0\n",
        "        nb = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in self.val_loader:\n",
        "                x = batch[\"x\"].to(DEVICE)\n",
        "                y = batch[\"y\"].to(DEVICE)\n",
        "                pred = self.model(x)\n",
        "\n",
        "                loss = float(criterion(pred, y).item())\n",
        "                y_hat = (torch.sigmoid(pred).cpu().numpy() > 0.5).astype(np.uint8)\n",
        "                y_np = (y.cpu().numpy() > 0.5).astype(np.uint8)\n",
        "                d, i, a = calc_metrics(y_np, y_hat)\n",
        "\n",
        "                tot_loss += loss\n",
        "                tot_d += d\n",
        "                tot_i += i\n",
        "                tot_a += a\n",
        "                nb += 1\n",
        "\n",
        "        nb = max(nb, 1)\n",
        "        metrics = {\n",
        "            \"loss\": tot_loss / nb,\n",
        "            \"dice\": tot_d / nb,\n",
        "            \"iou\": tot_i / nb,\n",
        "            \"acc\": tot_a / nb,\n",
        "            \"cid\": int(self.cid),\n",
        "            \"run_name\": self.run_name,\n",
        "        }\n",
        "\n",
        "        return metrics[\"loss\"], self.nva, metrics\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--cid\", type=int, required=True)\n",
        "    parser.add_argument(\"--server\", default=\"0.0.0.0:8080\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    fl.client.start_numpy_client(\n",
        "        server_address=args.server,\n",
        "        client=SegClient(args.cid),\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3xGGSblAMp7",
        "outputId": "ca183840-5ef9-444f-8241-ba2e97700068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing fl_client.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile fl_sim_colab.py\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "import torch\n",
        "import flwr as fl\n",
        "import time\n",
        "from flwr.common import FitIns\n",
        "import torchio as tio\n",
        "from fl_client import SegClient\n",
        "\n",
        "import logging\n",
        "import warnings\n",
        "logging.getLogger(\"flwr\").setLevel(logging.ERROR)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def ensure_csv(path: str, header: list[str]):\n",
        "    if not os.path.isfile(path):\n",
        "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "        with open(path, \"w\", newline=\"\") as f:\n",
        "            csv.writer(f).writerow(header)\n",
        "\n",
        "\n",
        "def append_row(path: str, row: list):\n",
        "    with open(path, \"a\", newline=\"\") as f:\n",
        "        csv.writer(f).writerow(row)\n",
        "\n",
        "\n",
        "def log_server_metrics(file_path, rnd, duration):\n",
        "    \"\"\"Functie helper pentru logare timp agregare server in fisier.\"\"\"\n",
        "    directory = os.path.dirname(file_path)\n",
        "    if directory and not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    msg = f\"Round {rnd} Aggregation Time: {duration:.4f} seconds\\n\"\n",
        "\n",
        "    with open(file_path, \"a\") as f:\n",
        "        f.write(msg)\n",
        "\n",
        "\n",
        "class PerClientLoggingFedAvg(fl.server.strategy.FedAvg):\n",
        "    def __init__(self, metrics_dir: str, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.metrics_dir = metrics_dir\n",
        "        self.header = [\n",
        "            \"round\",\"epoch\",\"train_loss\",\"train_dice\",\"train_iou\",\"train_acc\",\n",
        "            \"val_loss\",\"val_dice\",\"val_iou\",\"val_acc\",\"best_epoch\",\n",
        "        ]\n",
        "\n",
        "    def configure_fit(self, server_round, parameters, client_manager):\n",
        "        items = super().configure_fit(server_round, parameters, client_manager)\n",
        "        out = []\n",
        "        for it in items:\n",
        "            if isinstance(it, tuple):\n",
        "                client, fitins = it\n",
        "            else:\n",
        "                client, fitins = None, it\n",
        "\n",
        "            cfg = dict(fitins.config)\n",
        "            cfg[\"round\"] = server_round\n",
        "            new_fitins = FitIns(fitins.parameters, cfg)\n",
        "            out.append((client, new_fitins) if client is not None else new_fitins)\n",
        "        return out\n",
        "\n",
        "    def aggregate_fit(self, rnd, results, failures):\n",
        "        # --- START TIMER ---\n",
        "        start_time = time.time()\n",
        "\n",
        "        agg = super().aggregate_fit(rnd, results, failures)\n",
        "\n",
        "        # --- END TIMER ---\n",
        "        end_time = time.time()\n",
        "\n",
        "        print(f\"Round {rnd} Aggregation Time: {end_time - start_time:.4f} seconds\")\n",
        "        log_file_path = os.path.join(self.metrics_dir, \"server_aggregation_log.txt\")\n",
        "        log_server_metrics(log_file_path, rnd, end_time - start_time)\n",
        "\n",
        "        for client_proxy, fit_res in results:\n",
        "            m = fit_res.metrics or {}\n",
        "            cid = str(m.get(\"cid\", client_proxy.cid))\n",
        "\n",
        "            client_csv = os.path.join(self.metrics_dir, f\"metrics_client_{cid}.csv\")\n",
        "            ensure_csv(client_csv, self.header)\n",
        "\n",
        "            best_epoch = int(m.get(\"best_epoch\", -1))\n",
        "            per_epoch_raw = m.get(\"per_epoch\", \"[]\")\n",
        "\n",
        "            try:\n",
        "                per_epoch = json.loads(per_epoch_raw)\n",
        "            except Exception:\n",
        "                per_epoch = []\n",
        "\n",
        "            for ep in per_epoch:\n",
        "                epoch = ep.get(\"epoch\", \"\")\n",
        "                row = [\n",
        "                    rnd,\n",
        "                    epoch,\n",
        "                    ep.get(\"train_loss\", \"\"),\n",
        "                    ep.get(\"train_dice\", \"\"),\n",
        "                    ep.get(\"train_iou\", \"\"),\n",
        "                    ep.get(\"train_acc\", \"\"),\n",
        "                    ep.get(\"val_loss\", \"\"),\n",
        "                    ep.get(\"val_dice\", \"\"),\n",
        "                    ep.get(\"val_iou\", \"\"),\n",
        "                    ep.get(\"val_acc\", \"\"),\n",
        "                    \"x\" if int(epoch) == best_epoch else \"\",\n",
        "                ]\n",
        "                append_row(client_csv, row)\n",
        "\n",
        "        return agg\n",
        "\n",
        "\n",
        "def run_one_experiment(model_name: str, encoder_name: str, num_rounds=5, local_epochs=5, lr=1e-3):\n",
        "    run_name = f\"{model_name}__{encoder_name}\".replace(\"/\", \"-\")\n",
        "    base_dir = os.path.join(\"AITDM\", run_name)\n",
        "    metrics_dir = os.path.join(base_dir, \"metrics\")\n",
        "    os.makedirs(metrics_dir, exist_ok=True)\n",
        "\n",
        "    def client_fn(cid: str):\n",
        "        return SegClient(int(cid), model_name=model_name, encoder_name=encoder_name).to_client()\n",
        "\n",
        "    strategy = PerClientLoggingFedAvg(\n",
        "        metrics_dir=metrics_dir,\n",
        "        fraction_fit=1.0,\n",
        "        fraction_evaluate=1.0,\n",
        "        min_fit_clients=3,\n",
        "        min_evaluate_clients=3,\n",
        "        min_available_clients=3,\n",
        "        on_fit_config_fn=lambda rnd: {\"local_epochs\": local_epochs, \"lr\": lr},\n",
        "    )\n",
        "\n",
        "    use_gpu = torch.cuda.is_available()\n",
        "    client_resources = {\"num_cpus\": 1, \"num_gpus\": 1.0 if use_gpu else 0.0}\n",
        "\n",
        "    fl.simulation.start_simulation(\n",
        "        client_fn=client_fn,\n",
        "        num_clients=3,\n",
        "        config=fl.server.ServerConfig(num_rounds=num_rounds),\n",
        "        strategy=strategy,\n",
        "        client_resources=client_resources,\n",
        "        ray_init_args={\"include_dashboard\": False},\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    experiments = [\n",
        "        (\"unet\", \"resnet50\"),\n",
        "        (\"unet\", \"mit_b3\"),\n",
        "        (\"deeplabv3plus\", \"timm-mobilenetv3_small_100\"),\n",
        "    ]\n",
        "\n",
        "    for model_name, encoder_name in experiments:\n",
        "        print(f\"\\n=== Running: {model_name} + {encoder_name} ===\")\n",
        "        run_one_experiment(model_name, encoder_name, num_rounds=5, local_epochs=5, lr=1e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38AkBD0Kb16I",
        "outputId": "fa8d8c2f-53b2-48b2-9ed2-72380faa4afb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing fl_sim_colab.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/AITDM/client_0/*\n",
        "!rm -rf /content/AITDM/client_1/*\n",
        "!rm -rf /content/AITDM/client_2/*\n",
        "!rm -rf /content/AITDM/deeplabv3plus__timm-mobilenetv3_small_100/checkpoints/*\n",
        "!rm -rf /content/AITDM/unet__resnet50/checkpoints/*\n",
        "!rm -rf /content/AITDM/unet__mit_b3/checkpoints/*\n",
        "!rm -rf /content/AITDM/deeplabv3plus__timm-mobilenetv3_small_100/metrics/*\n",
        "!rm -rf /content/AITDM/unet__resnet50/metrics/*\n",
        "!rm -rf /content/AITDM/unet__mit_b3/metrics/*\n",
        "!python fl_sim_colab.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWdBLMGZQbPJ",
        "outputId": "fbba3dc7-c9e6-454b-c1e6-b7bd84dc0890",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Running: unet + resnet50 ===\n",
            "2026-01-05 19:22:56.211955: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2026-01-05 19:22:56.231084: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1767640976.252494    6040 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1767640976.259075    6040 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1767640976.275931    6040 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1767640976.275965    6040 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1767640976.275968    6040 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1767640976.275971    6040 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-05 19:22:56.280926: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "2026-01-05 19:23:04,840\tINFO worker.py:2012 -- Started a local Ray instance.\n",
            "\u001b[36m(pid=6393)\u001b[0m 2026-01-05 19:23:09.533388: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(pid=6393)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(pid=6393)\u001b[0m E0000 00:00:1767640989.555480    6393 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(pid=6393)\u001b[0m E0000 00:00:1767640989.561948    6393 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(pid=6393)\u001b[0m W0000 00:00:1767640989.578833    6393 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=6393)\u001b[0m W0000 00:00:1767640989.578862    6393 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=6393)\u001b[0m W0000 00:00:1767640989.578865    6393 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=6393)\u001b[0m W0000 00:00:1767640989.578867    6393 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=6393)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "\u001b[36m(pid=6393)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "\u001b[36m(pid=6393)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "\u001b[36m(pid=6393)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "\u001b[36m(pid=6393)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type swigvarlink has no __module__ attribute\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   _C._set_float32_matmul_precision(precision)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=6393) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m Client 2 Round 1: Incoming 127239.06 KB | Outgoing 127239.06 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=6393) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(pid=gcs_server)\u001b[0m [2026-01-05 19:23:32,004 E 6189 6189] (gcs_server) gcs_server.cc:302: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "\u001b[33m(raylet)\u001b[0m [2026-01-05 19:23:34,775 E 6326 6326] (raylet) main.cc:975: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m Client 1 Round 1: Incoming 127239.06 KB | Outgoing 127239.06 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=6393) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m [2026-01-05 19:23:38,354 E 6393 6474] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "[2026-01-05 19:23:38,876 E 6040 6387] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m Client 0 Round 1: Incoming 127239.06 KB | Outgoing 127239.06 KB | Overhead: 0.000001s\n",
            "Round 1 Aggregation Time: 0.4865 seconds\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=6393) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=6393) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(pid=6389)\u001b[0m [2026-01-05 19:23:38,863 E 6389 6948] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\u001b[32m [repeated 11x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=6393) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=6393) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m Client 0 Round 2: Incoming 127239.06 KB | Outgoing 127239.06 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=6393) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m Client 1 Round 2: Incoming 127239.06 KB | Outgoing 127239.06 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=6393) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m Client 2 Round 2: Incoming 127239.06 KB | Outgoing 127239.06 KB | Overhead: 0.000000s\n",
            "Round 2 Aggregation Time: 0.4427 seconds\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=6393) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=6393) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=6393) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=6393) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m Client 0 Round 3: Incoming 127239.06 KB | Outgoing 127239.06 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=6393) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m Client 2 Round 3: Incoming 127239.06 KB | Outgoing 127239.06 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=6393) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m Client 1 Round 3: Incoming 127239.06 KB | Outgoing 127239.06 KB | Overhead: 0.000001s\n",
            "Round 3 Aggregation Time: 0.3917 seconds\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=6393) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=6393) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=6393) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=6393) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m Client 1 Round 4: Incoming 127239.06 KB | Outgoing 127239.06 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=6393) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m Client 0 Round 4: Incoming 127239.06 KB | Outgoing 127239.06 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=6393) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m Client 2 Round 4: Incoming 127239.06 KB | Outgoing 127239.06 KB | Overhead: 0.000001s\n",
            "Round 4 Aggregation Time: 0.3994 seconds\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=6393) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=6393) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=6393) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=6393) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m Client 2 Round 5: Incoming 127239.06 KB | Outgoing 127239.06 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=6393) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m Client 1 Round 5: Incoming 127239.06 KB | Outgoing 127239.06 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=6393) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m Client 0 Round 5: Incoming 127239.06 KB | Outgoing 127239.06 KB | Overhead: 0.000001s\n",
            "Round 5 Aggregation Time: 0.3905 seconds\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=6393) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=6393) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=6393) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=6393)\u001b[0m   return data.pin_memory(device)\n",
            "\n",
            "=== Running: unet + mit_b3 ===\n",
            "2026-01-05 19:25:00,218\tINFO worker.py:2012 -- Started a local Ray instance.\n",
            "\u001b[36m(pid=8203)\u001b[0m 2026-01-05 19:25:04.970540: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(pid=8203)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(pid=8203)\u001b[0m E0000 00:00:1767641104.992125    8203 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(pid=8203)\u001b[0m E0000 00:00:1767641104.998964    8203 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(pid=8203)\u001b[0m W0000 00:00:1767641105.015392    8203 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=8203)\u001b[0m W0000 00:00:1767641105.015432    8203 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=8203)\u001b[0m W0000 00:00:1767641105.015436    8203 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=8203)\u001b[0m W0000 00:00:1767641105.015438    8203 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=8203)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "\u001b[36m(pid=8203)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "\u001b[36m(pid=8203)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "\u001b[36m(pid=8203)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "\u001b[36m(pid=8203)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type swigvarlink has no __module__ attribute\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   _C._set_float32_matmul_precision(precision)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=8203) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(pid=gcs_server)\u001b[0m [2026-01-05 19:25:27,800 E 8025 8025] (gcs_server) gcs_server.cc:302: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "\u001b[33m(raylet)\u001b[0m [2026-01-05 19:25:30,151 E 8141 8141] (raylet) main.cc:975: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m [2026-01-05 19:25:33,402 E 8203 8260] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "[2026-01-05 19:25:34,382 E 6040 8201] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m Client 0 Round 1: Incoming 184967.64 KB | Outgoing 184967.64 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m         \n",
            "\u001b[36m(pid=8213)\u001b[0m [2026-01-05 19:25:34,364 E 8213 8805] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=8203) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m Client 2 Round 1: Incoming 184967.64 KB | Outgoing 184967.64 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=8203) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m Client 1 Round 1: Incoming 184967.64 KB | Outgoing 184967.64 KB | Overhead: 0.000001s\n",
            "Round 1 Aggregation Time: 0.7091 seconds\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=8203) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=8203) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=8203) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=8203) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m Client 1 Round 2: Incoming 184967.64 KB | Outgoing 184967.64 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=8203) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m Client 0 Round 2: Incoming 184967.64 KB | Outgoing 184967.64 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=8203) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m Client 2 Round 2: Incoming 184967.64 KB | Outgoing 184967.64 KB | Overhead: 0.000002s\n",
            "Round 2 Aggregation Time: 0.5696 seconds\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=8203) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=8203) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=8203) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=8203) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m Client 0 Round 3: Incoming 184967.64 KB | Outgoing 184967.64 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=8203) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m Client 1 Round 3: Incoming 184967.64 KB | Outgoing 184967.64 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=8203) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m Client 2 Round 3: Incoming 184967.64 KB | Outgoing 184967.64 KB | Overhead: 0.000001s\n",
            "Round 3 Aggregation Time: 0.5711 seconds\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=8203) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=8203) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=8203) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=8203) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m Client 2 Round 4: Incoming 184967.64 KB | Outgoing 184967.64 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=8203) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m Client 1 Round 4: Incoming 184967.64 KB | Outgoing 184967.64 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=8203) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m Client 0 Round 4: Incoming 184967.64 KB | Outgoing 184967.64 KB | Overhead: 0.000001s\n",
            "Round 4 Aggregation Time: 0.5742 seconds\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=8203) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=8203) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=8203) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=8203) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m Client 1 Round 5: Incoming 184967.64 KB | Outgoing 184967.64 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=8203) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m Client 2 Round 5: Incoming 184967.64 KB | Outgoing 184967.64 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=8203) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m Client 0 Round 5: Incoming 184967.64 KB | Outgoing 184967.64 KB | Overhead: 0.000001s\n",
            "Round 5 Aggregation Time: 0.5752 seconds\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=8203) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=8203) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=8203) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=8203)\u001b[0m   return data.pin_memory(device)\n",
            "\n",
            "=== Running: deeplabv3plus + timm-mobilenetv3_small_100 ===\n",
            "2026-01-05 19:29:31,366\tINFO worker.py:2012 -- Started a local Ray instance.\n",
            "\u001b[36m(pid=10696)\u001b[0m 2026-01-05 19:29:36.068214: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(pid=10696)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(pid=10696)\u001b[0m E0000 00:00:1767641376.090064   10696 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(pid=10696)\u001b[0m E0000 00:00:1767641376.096728   10696 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(pid=10696)\u001b[0m W0000 00:00:1767641376.113853   10696 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=10696)\u001b[0m W0000 00:00:1767641376.113882   10696 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=10696)\u001b[0m W0000 00:00:1767641376.113885   10696 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=10696)\u001b[0m W0000 00:00:1767641376.113887   10696 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=10696)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "\u001b[36m(pid=10696)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "\u001b[36m(pid=10696)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "\u001b[36m(pid=10696)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "\u001b[36m(pid=10696)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type swigvarlink has no __module__ attribute\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   _C._set_float32_matmul_precision(precision)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=10696) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(pid=gcs_server)\u001b[0m [2026-01-05 19:29:58,869 E 10523 10523] (gcs_server) gcs_server.cc:302: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "\u001b[33m(raylet)\u001b[0m [2026-01-05 19:30:01,305 E 10635 10635] (raylet) main.cc:975: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m [2026-01-05 19:30:04,687 E 10696 10783] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "[2026-01-05 19:30:05,464 E 6040 10695] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m Client 0 Round 1: Incoming 8505.40 KB | Outgoing 8505.40 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(pid=10706)\u001b[0m [2026-01-05 19:30:05,390 E 10706 11266] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=10696) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m Client 1 Round 1: Incoming 8505.40 KB | Outgoing 8505.40 KB | Overhead: 0.000000s\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=10696) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m Client 2 Round 1: Incoming 8505.40 KB | Outgoing 8505.40 KB | Overhead: 0.000000s\n",
            "Round 1 Aggregation Time: 0.0937 seconds\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=10696) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=10696) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=10696) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=10696) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m Client 1 Round 2: Incoming 8505.40 KB | Outgoing 8505.40 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=10696) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m Client 2 Round 2: Incoming 8505.40 KB | Outgoing 8505.40 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=10696) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m Client 0 Round 2: Incoming 8505.40 KB | Outgoing 8505.40 KB | Overhead: 0.000001s\n",
            "Round 2 Aggregation Time: 0.0742 seconds\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=10696) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=10696) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=10696) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=10696) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m Client 0 Round 3: Incoming 8505.40 KB | Outgoing 8505.40 KB | Overhead: 0.000000s\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=10696) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   return data.pin_memory(device)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/server/server.py\", line 348, in fit_clients\n",
            "    finished_fs, _ = concurrent.futures.wait(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 305, in wait\n",
            "    waiter.event.wait(timeout)\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 655, in wait\n",
            "    signaled = self._cond.wait(timeout)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 355, in wait\n",
            "    waiter.acquire()\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/fl_sim_colab.py\", line 157, in <module>\n",
            "    run_one_experiment(model_name, encoder_name, num_rounds=5, local_epochs=5, lr=1e-3)\n",
            "  File \"/content/fl_sim_colab.py\", line 138, in run_one_experiment\n",
            "    fl.simulation.start_simulation(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/legacy_app.py\", line 361, in start_simulation\n",
            "    hist = run_fl(\n",
            "           ^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/server/server.py\", line 487, in run_fl\n",
            "    hist, elapsed_time = server.fit(\n",
            "                         ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/server/server.py\", line 114, in fit\n",
            "    res_fit = self.fit_round(\n",
            "              ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/server/server.py\", line 229, in fit_round\n",
            "    results, failures = fit_clients(\n",
            "                        ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/server/server.py\", line 343, in fit_clients\n",
            "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 647, in __exit__\n",
            "    self.shutdown(wait=True)\n",
            "  File \"/usr/lib/python3.12/concurrent/futures/thread.py\", line 239, in shutdown\n",
            "    t.join()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1149, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1169, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m Client 2 Round 3: Incoming 8505.40 KB | Outgoing 8505.40 KB | Overhead: 0.000000s\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=10696) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=10696)\u001b[0m Client 1 Round 3: Incoming 8505.40 KB | Outgoing 8505.40 KB | Overhead: 0.000000s\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import segmentation_models_pytorch as smp\n",
        "import logging\n",
        "from fl_client import TRANSFORMS\n",
        "\n",
        "from seg_data import (\n",
        "    ImageOnlyGliomaDataset,\n",
        "    SubsetByPIDs,\n",
        "    image_only_collate_fn,\n",
        "    DATA_ROOT,\n",
        "    METADATA_DF_PATH,\n",
        "    USE_ATLAS,\n",
        "    calc_metrics,\n",
        ")\n",
        "\n",
        "logging.getLogger(\"timm.models._builder\").setLevel(logging.ERROR)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SEED = 42\n",
        "THRESHOLD = 0.5\n",
        "\n",
        "CLIENT_DIR = \"/content/client\"\n",
        "BATCH_SIZE = 8\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "ENSEMBLE_CFGS = [\n",
        "    (\"unet\", \"resnet50\"),\n",
        "    (\"unet\", \"mit_b3\"),\n",
        "    (\"deeplabv3plus\", \"timm-mobilenetv3_small_100\"),\n",
        "]\n",
        "\n",
        "WEIGHT_MODE = \"power\"\n",
        "WEIGHT_POWER = 14.0\n",
        "WEIGHT_EPS = 1e-6\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "set_seed(SEED)\n",
        "\n",
        "bce = nn.BCEWithLogitsLoss()\n",
        "dice_loss = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
        "\n",
        "\n",
        "def criterion(logits, y):\n",
        "    return 0.5 * bce(logits, y) + 0.5 * dice_loss(logits, y)\n",
        "\n",
        "\n",
        "def get_val_loader(cid: int):\n",
        "    full = ImageOnlyGliomaDataset(\n",
        "        METADATA_DF_PATH, DATA_ROOT, use_atlas=USE_ATLAS, exclude_ids=[\"PatientID_0191\"]\n",
        "    )\n",
        "    with open(os.path.join(CLIENT_DIR, f\"client_{cid}\", \"val_pids.json\")) as f:\n",
        "        va_p = json.load(f)\n",
        "\n",
        "    ds_va = SubsetByPIDs(full, va_p)\n",
        "    g = torch.Generator().manual_seed(SEED)\n",
        "\n",
        "    ld_va = torch.utils.data.DataLoader(\n",
        "        ds_va,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "        collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "        generator=g,\n",
        "    )\n",
        "    return ld_va, len(ds_va)\n",
        "\n",
        "\n",
        "def run_name(model_name: str, encoder_name: str) -> str:\n",
        "    return f\"{model_name}__{encoder_name}\".replace(\"/\", \"-\")\n",
        "\n",
        "\n",
        "def build_model(model_name: str, encoder_name: str, encoder_weights=\"imagenet\"):\n",
        "    in_ch = 2 if USE_ATLAS else 1\n",
        "    mn = model_name.lower()\n",
        "\n",
        "    if mn == \"unet\":\n",
        "        m = smp.Unet(\n",
        "            encoder_name=encoder_name,\n",
        "            encoder_weights=encoder_weights,\n",
        "            in_channels=in_ch,\n",
        "            classes=1,\n",
        "        )\n",
        "    elif mn in [\"deeplabv3plus\", \"deeplabv3+\", \"dlv3p\"]:\n",
        "        m = smp.DeepLabV3Plus(\n",
        "            encoder_name=encoder_name,\n",
        "            encoder_weights=encoder_weights,\n",
        "            in_channels=in_ch,\n",
        "            classes=1,\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model_name={model_name}\")\n",
        "\n",
        "    return m.to(DEVICE)\n",
        "\n",
        "\n",
        "def ckpt_path(model_name: str, encoder_name: str, cid: int) -> str:\n",
        "    rn = run_name(model_name, encoder_name)\n",
        "    return os.path.join(\"AITDM\", rn, \"checkpoints\", f\"client_{cid}_best.pt\")\n",
        "\n",
        "\n",
        "def best_json_path(model_name: str, encoder_name: str, cid: int) -> str:\n",
        "    rn = run_name(model_name, encoder_name)\n",
        "    return os.path.join(\"AITDM\", rn, \"checkpoints\", f\"client_{cid}_best.json\")\n",
        "\n",
        "\n",
        "def load_model(model_name: str, encoder_name: str, cid: int):\n",
        "    path = ckpt_path(model_name, encoder_name, cid)\n",
        "    if not os.path.isfile(path):\n",
        "        raise FileNotFoundError(f\"Missing checkpoint: {path}\")\n",
        "    m = build_model(model_name, encoder_name).to(DEVICE)\n",
        "    sd = torch.load(path, map_location=\"cpu\")\n",
        "    m.load_state_dict(sd, strict=True)\n",
        "    m.eval()\n",
        "    return m\n",
        "\n",
        "\n",
        "def load_best_val_dice(model_name: str, encoder_name: str, cid: int) -> float:\n",
        "    p = best_json_path(model_name, encoder_name, cid)\n",
        "    if not os.path.isfile(p):\n",
        "        raise FileNotFoundError(f\"Missing best json: {p}\")\n",
        "    with open(p, \"r\") as f:\n",
        "        j = json.load(f)\n",
        "    return float(j.get(\"val_dice\", 0.0))\n",
        "\n",
        "\n",
        "def get_client_weights(cid: int, cfgs, mode=\"power\", power=2.0, eps=1e-6):\n",
        "    dices = [load_best_val_dice(mn, enc, cid) for (mn, enc) in cfgs]\n",
        "    d = np.array(dices, dtype=np.float32)\n",
        "\n",
        "    if mode == \"linear\":\n",
        "        raw = np.clip(d, 0.0, None)\n",
        "    elif mode == \"power\":\n",
        "        raw = np.power(np.clip(d, 0.0, None), power)\n",
        "    else:\n",
        "        raise ValueError(\"mode must be 'linear' or 'power'\")\n",
        "\n",
        "    raw = raw + eps\n",
        "    w = raw / raw.sum()\n",
        "    return w.tolist(), dices\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def ensemble_forward_logits(x, models, weights, target_hw=None):\n",
        "    w = np.array(weights, dtype=np.float32)\n",
        "    w = w / (w.sum() + 1e-8)\n",
        "\n",
        "    logits_sum = None\n",
        "    for mi, wi in zip(models, w):\n",
        "        li = mi(x)\n",
        "        if target_hw is not None and li.shape[-2:] != target_hw:\n",
        "            li = F.interpolate(li, size=target_hw, mode=\"bilinear\", align_corners=False)\n",
        "        logits_sum = li * float(wi) if logits_sum is None else logits_sum + li * float(wi)\n",
        "    return logits_sum\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_ensemble_on_client(cid: int, threshold=0.5):\n",
        "    val_loader, nva = get_val_loader(cid)\n",
        "    models = [load_model(mn, enc, cid) for (mn, enc) in ENSEMBLE_CFGS]\n",
        "\n",
        "    weights, best_dices = get_client_weights(\n",
        "        cid, ENSEMBLE_CFGS, mode=WEIGHT_MODE, power=WEIGHT_POWER, eps=WEIGHT_EPS\n",
        "    )\n",
        "\n",
        "    tot_loss = tot_d = tot_i = tot_a = 0.0\n",
        "    nb = 0\n",
        "\n",
        "    for batch in val_loader:\n",
        "        x = batch[\"x\"].to(DEVICE)\n",
        "        y = batch[\"y\"].to(DEVICE)\n",
        "\n",
        "        logits = ensemble_forward_logits(x, models=models, weights=weights, target_hw=y.shape[-2:])\n",
        "        loss = float(criterion(logits, y).item())\n",
        "\n",
        "        preds_bin = (torch.sigmoid(logits).cpu().numpy() > threshold).astype(np.uint8)\n",
        "        y_np = (y.cpu().numpy() > 0.5).astype(np.uint8)\n",
        "\n",
        "        d, i, a = calc_metrics(y_np, preds_bin)\n",
        "\n",
        "        tot_loss += loss\n",
        "        tot_d += d\n",
        "        tot_i += i\n",
        "        tot_a += a\n",
        "        nb += 1\n",
        "\n",
        "    nb = max(nb, 1)\n",
        "    return {\n",
        "        \"cid\": int(cid),\n",
        "        \"nva\": int(nva),\n",
        "        \"loss\": tot_loss / nb,\n",
        "        \"dice\": tot_d / nb,\n",
        "        \"iou\": tot_i / nb,\n",
        "        \"acc\": tot_a / nb,\n",
        "        \"weights\": weights,\n",
        "        \"best_dices\": best_dices,\n",
        "    }\n",
        "\n",
        "\n",
        "def log_client_metrics(file_path, r, bd, ww):\n",
        "\n",
        "    directory = os.path.dirname(file_path)\n",
        "    if directory and not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    log_message = (\n",
        "        f\"Client {r['cid']} (n={r['nva']}): \"\n",
        "        f\"loss={r['loss']:.4f} dice={r['dice']:.4f} iou={r['iou']:.4f} acc={r['acc']:.4f}\\n\"\n",
        "        f\"  best_dice={['%.4f' % x for x in bd]} -> weights={['%.3f' % x for x in ww]}\\n\"\n",
        "    )\n",
        "\n",
        "    print(log_message, end='')\n",
        "\n",
        "    with open(file_path, \"a\") as f:\n",
        "        f.write(log_message)\n",
        "\n",
        "def main():\n",
        "    print(f\"[Ensemble-3 | threshold={THRESHOLD} | weight_mode={WEIGHT_MODE} | power={WEIGHT_POWER}]\\n\")\n",
        "    for cid in [0, 1, 2]:\n",
        "        r = eval_ensemble_on_client(cid, threshold=THRESHOLD)\n",
        "        bd = r[\"best_dices\"]\n",
        "        ww = r[\"weights\"]\n",
        "        log_client_metrics(os.path.join('/content/AITDM', f\"client_{cid}\", \"val_metrics_ensemble.txt\"), r, bd, ww)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2mAaqi34g92",
        "outputId": "ba1c2d69-8503-44f8-e3d7-45c14d9fa092",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Ensemble-3 | threshold=0.5 | weight_mode=power | power=14.0]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Client 0 (n=9): loss=0.1908 dice=0.7110 iou=0.5785 acc=0.9813\n",
            "  best_dice=['0.6056', '0.6353', '0.6892'] -> weights=['0.110', '0.216', '0.674']\n",
            "Client 1 (n=25): loss=0.2104 dice=0.6987 iou=0.5431 acc=0.9746\n",
            "  best_dice=['0.6385', '0.6846', '0.6505'] -> weights=['0.202', '0.536', '0.262']\n",
            "Client 2 (n=8): loss=0.3864 dice=0.4307 iou=0.2744 acc=0.9857\n",
            "  best_dice=['0.3837', '0.4039', '0.4421'] -> weights=['0.135', '0.221', '0.644']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/drive/MyDrive/AITDM\n",
        "!cp -rf /content/AITDM /content/drive/MyDrive/AITDM/NORMAL_3"
      ],
      "metadata": {
        "id": "SxMOzehOSxst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "LPzGIFl30B5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **M2 - Loss-Based FL with ensemble**"
      ],
      "metadata": {
        "id": "It-TUVsjw8yZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile seg_data.py\n",
        "import os, pickle, numpy as np, torch\n",
        "from typing import List\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchio as tio\n",
        "\n",
        "# Global paths and configuration\n",
        "DATA_ROOT = \"/content/Preprocessed-Data\"\n",
        "METADATA_DF_PATH = \"cleaned_df.pkl\"\n",
        "USE_ATLAS = True\n",
        "EXCLUDE_IDS = [\"PatientID_0191\"]\n",
        "\n",
        "# Dataset that loads MRI, tumor mask and optional atlas for each patient\n",
        "class ImageOnlyGliomaDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        metadata_df_path,\n",
        "        data_root,\n",
        "        use_atlas=False,\n",
        "        exclude_ids=None,\n",
        "        transform=None,\n",
        "    ):\n",
        "        with open(metadata_df_path, \"rb\") as f:\n",
        "            df = pickle.load(f)\n",
        "\n",
        "        if exclude_ids is None:\n",
        "            exclude_ids = []\n",
        "\n",
        "        self.df = df[~df[\"Patient_ID\"].isin(exclude_ids)].reset_index(drop=True)\n",
        "        self.data_root = data_root\n",
        "        self.use_atlas = use_atlas\n",
        "        self.transform = transform\n",
        "\n",
        "        self.patient_ids = []\n",
        "        for pid in sorted(self.df[\"Patient_ID\"].tolist()):\n",
        "            base = os.path.join(self.data_root, pid)\n",
        "            mri_p = os.path.join(base, f\"{pid}_mri.npy\")\n",
        "            tumor_p = os.path.join(base, f\"{pid}_tumor.npy\")\n",
        "\n",
        "            is_valid = os.path.isfile(mri_p) and os.path.isfile(tumor_p)\n",
        "            if self.use_atlas:\n",
        "                reg_p = os.path.join(base, f\"{pid}_regions.npy\")\n",
        "                is_valid = is_valid and os.path.isfile(reg_p)\n",
        "\n",
        "            if is_valid:\n",
        "                self.patient_ids.append(pid)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patient_ids)\n",
        "\n",
        "    @staticmethod\n",
        "    def _minmax(x):\n",
        "        x = x.astype(np.float32)\n",
        "        mn, mx = np.min(x), np.max(x)\n",
        "        if mx > mn:\n",
        "            return (x - mn) / (mx - mn)\n",
        "        return np.zeros_like(x, dtype=np.float32)\n",
        "\n",
        "    def _to_torchio_format(self, arr):\n",
        "        \"\"\"\n",
        "        Convertește (H, W) -> (1, H, W, 1) pentru procesare internă TorchIO.\n",
        "        \"\"\"\n",
        "        if arr.ndim == 2:\n",
        "            return arr[np.newaxis, ..., np.newaxis]\n",
        "        return arr\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pid = self.patient_ids[idx]\n",
        "        base = os.path.join(self.data_root, pid)\n",
        "\n",
        "        mri = np.load(os.path.join(base, f\"{pid}_mri.npy\")).astype(np.float32)\n",
        "        tumor = np.load(os.path.join(base, f\"{pid}_tumor.npy\")).astype(np.float32)\n",
        "\n",
        "\n",
        "        mri = self._minmax(mri)\n",
        "        tumor = (tumor > 0.5).astype(np.float32)\n",
        "\n",
        "\n",
        "        regions = None\n",
        "        if self.use_atlas:\n",
        "            regions = np.load(os.path.join(base, f\"{pid}_regions.npy\")).astype(np.float32)\n",
        "\n",
        "        mri = self._minmax(mri)\n",
        "        tumor = (tumor > 0.5).astype(np.float32)\n",
        "\n",
        "        if regions is not None:\n",
        "            regions = self._minmax(regions)\n",
        "\n",
        "\n",
        "        if self.transform:\n",
        "            subject_dict = {\n",
        "                'mri': tio.ScalarImage(tensor=self._to_torchio_format(mri)),\n",
        "                'tumor': tio.LabelMap(tensor=self._to_torchio_format(tumor)),\n",
        "            }\n",
        "            if regions is not None:\n",
        "                subject_dict['regions'] = tio.ScalarImage(tensor=self._to_torchio_format(regions))\n",
        "\n",
        "            subject = tio.Subject(subject_dict)\n",
        "\n",
        "            subject = self.transform(subject)\n",
        "\n",
        "            out_mri = subject['mri'].data[0, ..., 0].numpy()\n",
        "            out_tumor = subject['tumor'].data[0, ..., 0].numpy()\n",
        "            sample = {\n",
        "                \"patient_id\": pid,\n",
        "                \"mri\": out_mri,       # Shape: (240, 240)\n",
        "                \"tumor\": out_tumor    # Shape: (240, 240)\n",
        "            }\n",
        "\n",
        "            if regions is not None:\n",
        "                out_regions = subject['regions'].data[0, ..., 0].numpy()\n",
        "                sample[\"regions\"] = out_regions # Shape: (240, 240)\n",
        "\n",
        "            return sample\n",
        "\n",
        "\n",
        "        sample = {\"patient_id\": pid, \"mri\": mri, \"tumor\": tumor}\n",
        "\n",
        "        if self.use_atlas:\n",
        "            sample[\"regions\"] = regions\n",
        "\n",
        "        return sample\n",
        "\n",
        "# Collate function to build batched tensors and patient ID list\n",
        "def image_only_collate_fn(batch, use_atlas=USE_ATLAS):\n",
        "    mri = torch.stack([torch.tensor(it[\"mri\"]) for it in batch]).unsqueeze(1)\n",
        "    y = torch.stack([torch.tensor(it[\"tumor\"]) for it in batch]).unsqueeze(1)\n",
        "\n",
        "    if use_atlas:\n",
        "        regs = torch.stack([torch.tensor(it[\"regions\"]) for it in batch]).unsqueeze(1)\n",
        "        x = torch.cat([mri.float(), regs.float()], dim=1)\n",
        "    else:\n",
        "        x = mri.float()\n",
        "\n",
        "    return {\"x\": x, \"y\": y.float(), \"pid\": [it[\"patient_id\"] for it in batch]}\n",
        "\n",
        "# Dataset wrapper that restricts to a subset of patient IDs\n",
        "class SubsetByPIDs(Dataset):\n",
        "    def __init__(self, full_dataset: ImageOnlyGliomaDataset, pid_list: List[str]):\n",
        "        self.ds = full_dataset\n",
        "        pid_to_idx = {pid: i for i, pid in enumerate(self.ds.patient_ids)}\n",
        "        self.indices = [pid_to_idx[p] for p in pid_list if p in pid_to_idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.ds[self.indices[i]]\n",
        "\n",
        "# Compute Dice, IoU and accuracy for binary masks\n",
        "def calc_metrics(y_true, y_pred):\n",
        "    y_true = y_true.astype(np.uint8).reshape(-1)\n",
        "    y_pred = y_pred.astype(np.uint8).reshape(-1)\n",
        "\n",
        "    inter = (y_true & y_pred).sum()\n",
        "    dice = (2.0 * inter) / (y_true.sum() + y_pred.sum() + 1e-8)\n",
        "    union = y_true.sum() + y_pred.sum() - inter + 1e-8\n",
        "    iou = inter / union\n",
        "    acc = (y_true == y_pred).mean()\n",
        "\n",
        "    return float(dice), float(iou), float(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_Go_Bd3xMds",
        "outputId": "43a09535-56de-4fb6-d843-91c9236db143"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing seg_data.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile fl_client.py\n",
        "import os\n",
        "os.environ.setdefault(\"CUBLAS_WORKSPACE_CONFIG\", \":4096:8\")\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import flwr as fl\n",
        "import copy\n",
        "import segmentation_models_pytorch as smp\n",
        "import random\n",
        "import torchio as tio\n",
        "import sys\n",
        "import time\n",
        "\n",
        "from seg_data import (\n",
        "    ImageOnlyGliomaDataset,\n",
        "    SubsetByPIDs,\n",
        "    image_only_collate_fn,\n",
        "    calc_metrics,\n",
        "    DATA_ROOT,\n",
        "    METADATA_DF_PATH,\n",
        "    USE_ATLAS,\n",
        ")\n",
        "\n",
        "CLIENT_DIR = \"/content/client\"\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "NUM_WORKERS = 2\n",
        "SEED = 42\n",
        "\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "DEFAULT_MODEL_NAME = \"unet\"\n",
        "DEFAULT_ENCODER_NAME = \"timm-mobilenetv3_small_100\"\n",
        "DEFAULT_ENCODER_WEIGHTS = \"imagenet\"\n",
        "TRANSFORMS = None\n",
        "\n",
        "\n",
        "def _run_name(model_name: str, encoder_name: str) -> str:\n",
        "    return f\"{model_name}__{encoder_name}\".replace(\"/\", \"-\")\n",
        "\n",
        "\n",
        "def seed_everything(seed: int, deterministic: bool = True) -> None:\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    try:\n",
        "        torch.set_float32_matmul_precision(\"high\")\n",
        "    except Exception:\n",
        "        pass\n",
        "    if deterministic:\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.use_deterministic_algorithms(True)\n",
        "\n",
        "\n",
        "def seed_worker(worker_id: int) -> None:\n",
        "    worker_seed = (torch.initial_seed() + worker_id) % (2**32)\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "\n",
        "def log_transfer_metrics(file_path, cid, rnd, incoming, outgoing, overhead):\n",
        "    \"\"\"Functie helper pentru a salva metricile de transfer in fisier.\"\"\"\n",
        "    directory = os.path.dirname(file_path)\n",
        "    if directory and not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    msg = (f\"Client {cid} Round {rnd}: \"\n",
        "           f\"Incoming {incoming/1024:.2f} KB | \"\n",
        "           f\"Outgoing {outgoing/1024:.2f} KB | \"\n",
        "           f\"Overhead: {overhead:.6f}s\\n\")\n",
        "\n",
        "\n",
        "    with open(file_path, \"a\") as f:\n",
        "        f.write(msg)\n",
        "\n",
        "\n",
        "def get_model(\n",
        "    model_name=DEFAULT_MODEL_NAME,\n",
        "    encoder_name=DEFAULT_ENCODER_NAME,\n",
        "    encoder_weights=DEFAULT_ENCODER_WEIGHTS,\n",
        "):\n",
        "    in_ch = 2 if USE_ATLAS else 1\n",
        "    mn = model_name.lower()\n",
        "\n",
        "    if mn == \"unet\":\n",
        "        model = smp.Unet(\n",
        "            encoder_name=encoder_name,\n",
        "            encoder_weights=encoder_weights,\n",
        "            in_channels=in_ch,\n",
        "            classes=1,\n",
        "        )\n",
        "    elif mn in [\"deeplabv3plus\", \"deeplabv3+\", \"dlv3p\"]:\n",
        "        model = smp.DeepLabV3Plus(\n",
        "            encoder_name=encoder_name,\n",
        "            encoder_weights=encoder_weights,\n",
        "            in_channels=in_ch,\n",
        "            classes=1,\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model_name={model_name}. Use 'unet' or 'deeplabv3plus'.\")\n",
        "\n",
        "    return model.to(DEVICE)\n",
        "\n",
        "\n",
        "def get_loaders(cid: int, base_seed: int, transforms):\n",
        "    full = ImageOnlyGliomaDataset(\n",
        "        METADATA_DF_PATH,\n",
        "        DATA_ROOT,\n",
        "        use_atlas=USE_ATLAS,\n",
        "        exclude_ids=[\"PatientID_0191\"],\n",
        "        transform=transforms,\n",
        "    )\n",
        "\n",
        "    with open(os.path.join(CLIENT_DIR, f\"client_{cid}\", \"train_pids.json\")) as f:\n",
        "        tr_p = json.load(f)\n",
        "    with open(os.path.join(CLIENT_DIR, f\"client_{cid}\", \"val_pids.json\")) as f:\n",
        "        va_p = json.load(f)\n",
        "\n",
        "    ds_tr = SubsetByPIDs(full, tr_p)\n",
        "    ds_va = SubsetByPIDs(full, va_p)\n",
        "\n",
        "    g_tr = torch.Generator().manual_seed(base_seed + 12345)\n",
        "    g_va = torch.Generator().manual_seed(base_seed + 67890)\n",
        "\n",
        "    ld_tr = DataLoader(\n",
        "        ds_tr,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "        collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=g_tr,\n",
        "        persistent_workers=(NUM_WORKERS > 0),\n",
        "    )\n",
        "\n",
        "    ld_va = DataLoader(\n",
        "        ds_va,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "        collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=g_va,\n",
        "        persistent_workers=(NUM_WORKERS > 0),\n",
        "    )\n",
        "\n",
        "    return ld_tr, ld_va, len(ds_tr), len(ds_va)\n",
        "\n",
        "\n",
        "def get_parameters(model):\n",
        "    return [p.detach().cpu().numpy() for _, p in model.state_dict().items()]\n",
        "\n",
        "\n",
        "def set_parameters(model, params):\n",
        "    sd = model.state_dict()\n",
        "    for k, v in zip(sd.keys(), params):\n",
        "        sd[k] = torch.tensor(v)\n",
        "    model.load_state_dict(sd, strict=True)\n",
        "\n",
        "\n",
        "bce = nn.BCEWithLogitsLoss()\n",
        "dice_loss = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
        "\n",
        "\n",
        "def criterion(pred, y):\n",
        "    return 0.5 * bce(pred, y) + 0.5 * dice_loss(pred, y)\n",
        "\n",
        "\n",
        "def maybe_save_best(run_dir, cid, val_loss, val_dice, best_epoch, rnd, model):\n",
        "    ckpt_dir = os.path.join(run_dir, \"checkpoints\")\n",
        "    os.makedirs(ckpt_dir, exist_ok=True)\n",
        "\n",
        "    best_json = os.path.join(ckpt_dir, f\"client_{cid}_best.json\")\n",
        "    best_pt = os.path.join(ckpt_dir, f\"client_{cid}_best.pt\")\n",
        "\n",
        "    prev = {\"val_loss\": float(\"inf\"), \"val_dice\": -1.0}\n",
        "    if os.path.isfile(best_json):\n",
        "        try:\n",
        "            with open(best_json, \"r\") as f:\n",
        "                prev = json.load(f)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    improved = (val_loss < prev.get(\"val_loss\", float(\"inf\"))) and (val_dice > prev.get(\"val_dice\", -1.0))\n",
        "    if improved:\n",
        "        torch.save(model.state_dict(), best_pt)\n",
        "        with open(best_json, \"w\") as f:\n",
        "            json.dump(\n",
        "                {\n",
        "                    \"round\": int(rnd),\n",
        "                    \"epoch\": int(best_epoch),\n",
        "                    \"val_loss\": float(val_loss),\n",
        "                    \"val_dice\": float(val_dice),\n",
        "                },\n",
        "                f,\n",
        "            )\n",
        "\n",
        "\n",
        "class SegClient(fl.client.NumPyClient):\n",
        "    def __init__(\n",
        "        self,\n",
        "        cid: int,\n",
        "        model_name=DEFAULT_MODEL_NAME,\n",
        "        encoder_name=DEFAULT_ENCODER_NAME,\n",
        "        encoder_weights=DEFAULT_ENCODER_WEIGHTS,\n",
        "    ):\n",
        "        self.cid = int(cid)\n",
        "        self.model_name = model_name\n",
        "        self.encoder_name = encoder_name\n",
        "        self.encoder_weights = encoder_weights\n",
        "\n",
        "        self.base_seed = SEED + self.cid\n",
        "        seed_everything(self.base_seed, deterministic=True)\n",
        "\n",
        "        self.run_name = _run_name(model_name, encoder_name)\n",
        "        self.run_dir = os.path.join(\"AITDM\", self.run_name)\n",
        "\n",
        "        self.model = get_model(model_name, encoder_name, encoder_weights)\n",
        "        if self.cid != 2:\n",
        "            self.train_loader, self.val_loader, self.ntr, self.nva = get_loaders(self.cid, self.base_seed, transforms=TRANSFORMS)\n",
        "        else:\n",
        "            self.train_loader, self.val_loader, self.ntr, self.nva = get_loaders(self.cid, self.base_seed, transforms=TRANSFORMS)\n",
        "\n",
        "\n",
        "    def get_parameters(self, config):\n",
        "        return get_parameters(self.model)\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        # --- MEASURE INCOMING ---\n",
        "        incoming_size = sum([p.nbytes for p in parameters])\n",
        "\n",
        "        set_parameters(self.model, parameters)\n",
        "\n",
        "        epochs = int(config.get(\"local_epochs\", 1))\n",
        "        lr = float(config.get(\"lr\", 1e-3))\n",
        "        rnd = int(config.get(\"round\", 0))\n",
        "\n",
        "        opt = optim.AdamW(self.model.parameters(), lr=lr)\n",
        "        scaler = torch.amp.GradScaler(\"cuda\", enabled=(DEVICE.type == \"cuda\"))\n",
        "\n",
        "        best_state = None\n",
        "        best_val_loss = float(\"inf\")\n",
        "        best_val_dice = -1.0\n",
        "        best_epoch_idx = -1\n",
        "        epoch_logs = []\n",
        "\n",
        "        for epoch_idx in range(1, epochs + 1):\n",
        "            self.model.train()\n",
        "            tot_tr_loss = tot_tr_d = tot_tr_i = tot_tr_a = 0.0\n",
        "            nb_tr = 0\n",
        "\n",
        "            for batch in self.train_loader:\n",
        "                x = batch[\"x\"].to(DEVICE)\n",
        "                y = batch[\"y\"].to(DEVICE)\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "\n",
        "                with torch.amp.autocast(\"cuda\", enabled=(DEVICE.type == \"cuda\")):\n",
        "                    pred = self.model(x)\n",
        "                    loss = criterion(pred, y)\n",
        "\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(opt)\n",
        "                scaler.update()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    y_hat = (torch.sigmoid(pred).detach().cpu().numpy() > 0.5).astype(np.uint8)\n",
        "                    y_np = (y.detach().cpu().numpy() > 0.5).astype(np.uint8)\n",
        "                    d, i, a = calc_metrics(y_np, y_hat)\n",
        "\n",
        "                tot_tr_loss += float(loss.item())\n",
        "                tot_tr_d += d\n",
        "                tot_tr_i += i\n",
        "                tot_tr_a += a\n",
        "                nb_tr += 1\n",
        "\n",
        "            nb_tr = max(nb_tr, 1)\n",
        "            epoch_tr_loss = tot_tr_loss / nb_tr\n",
        "            epoch_tr_dice = tot_tr_d / nb_tr\n",
        "            epoch_tr_iou = tot_tr_i / nb_tr\n",
        "            epoch_tr_acc = tot_tr_a / nb_tr\n",
        "\n",
        "            self.model.eval()\n",
        "            tot_val_loss = tot_val_d = tot_val_i = tot_val_a = 0.0\n",
        "            nb_val = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in self.val_loader:\n",
        "                    x = batch[\"x\"].to(DEVICE)\n",
        "                    y = batch[\"y\"].to(DEVICE)\n",
        "                    pred = self.model(x)\n",
        "\n",
        "                    v_loss = float(criterion(pred, y).item())\n",
        "                    y_hat = (torch.sigmoid(pred).cpu().numpy() > 0.5).astype(np.uint8)\n",
        "                    y_np = (y.cpu().numpy() > 0.5).astype(np.uint8)\n",
        "                    d, i, a = calc_metrics(y_np, y_hat)\n",
        "\n",
        "                    tot_val_loss += v_loss\n",
        "                    tot_val_d += d\n",
        "                    tot_val_i += i\n",
        "                    tot_val_a += a\n",
        "                    nb_val += 1\n",
        "\n",
        "            nb_val = max(nb_val, 1)\n",
        "            epoch_val_loss = tot_val_loss / nb_val\n",
        "            epoch_val_dice = tot_val_d / nb_val\n",
        "            epoch_val_iou = tot_val_i / nb_val\n",
        "            epoch_val_acc = tot_val_a / nb_val\n",
        "\n",
        "            epoch_logs.append(\n",
        "                {\n",
        "                    \"epoch\": int(epoch_idx),\n",
        "                    \"train_loss\": float(epoch_tr_loss),\n",
        "                    \"train_dice\": float(epoch_tr_dice),\n",
        "                    \"train_iou\": float(epoch_tr_iou),\n",
        "                    \"train_acc\": float(epoch_tr_acc),\n",
        "                    \"val_loss\": float(epoch_val_loss),\n",
        "                    \"val_dice\": float(epoch_val_dice),\n",
        "                    \"val_iou\": float(epoch_val_iou),\n",
        "                    \"val_acc\": float(epoch_val_acc),\n",
        "                }\n",
        "            )\n",
        "\n",
        "            if (epoch_val_loss < best_val_loss) and (epoch_val_dice > best_val_dice):\n",
        "                best_val_loss = epoch_val_loss\n",
        "                best_val_dice = epoch_val_dice\n",
        "                best_state = copy.deepcopy(self.model.state_dict())\n",
        "                best_epoch_idx = epoch_idx\n",
        "\n",
        "        if best_state is not None:\n",
        "            self.model.load_state_dict(best_state)\n",
        "\n",
        "        for ep in epoch_logs:\n",
        "            ep[\"best_epoch\"] = (ep[\"epoch\"] == best_epoch_idx)\n",
        "\n",
        "        train_metrics = {\n",
        "            \"cid\": int(self.cid),\n",
        "            \"best_epoch\": int(best_epoch_idx),\n",
        "            \"best_val_loss\": float(best_val_loss),\n",
        "            \"best_val_dice\": float(best_val_dice),\n",
        "            \"per_epoch\": json.dumps(epoch_logs),\n",
        "            \"run_name\": self.run_name,\n",
        "            \"model_name\": self.model_name,\n",
        "            \"encoder_name\": self.encoder_name,\n",
        "        }\n",
        "\n",
        "        maybe_save_best(self.run_dir, self.cid, best_val_loss, best_val_dice, best_epoch_idx, rnd, self.model)\n",
        "\n",
        "        # --- PREPARE & MEASURE OUTGOING ---\n",
        "        out_params = get_parameters(self.model)\n",
        "\n",
        "\n",
        "        start_overhead = time.time()\n",
        "        final_params_to_send = out_params\n",
        "        end_overhead = time.time()\n",
        "\n",
        "        outgoing_size = sum([p.nbytes for p in final_params_to_send])\n",
        "\n",
        "        print(f\"Client {self.cid} Round {rnd}: Incoming {incoming_size/1024:.2f} KB | Outgoing {outgoing_size/1024:.2f} KB | Overhead: {end_overhead - start_overhead:.6f}s\")\n",
        "        log_file_path = os.path.join(self.run_dir, f\"client_{self.cid}_transfer_log.txt\")\n",
        "        log_transfer_metrics(log_file_path, self.cid, rnd, incoming_size, outgoing_size, end_overhead - start_overhead)\n",
        "\n",
        "        return final_params_to_send, self.ntr, train_metrics\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "        set_parameters(self.model, parameters)\n",
        "        self.model.eval()\n",
        "\n",
        "        tot_loss = tot_d = tot_i = tot_a = 0.0\n",
        "        nb = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in self.val_loader:\n",
        "                x = batch[\"x\"].to(DEVICE)\n",
        "                y = batch[\"y\"].to(DEVICE)\n",
        "                pred = self.model(x)\n",
        "\n",
        "                loss = float(criterion(pred, y).item())\n",
        "                y_hat = (torch.sigmoid(pred).cpu().numpy() > 0.5).astype(np.uint8)\n",
        "                y_np = (y.cpu().numpy() > 0.5).astype(np.uint8)\n",
        "                d, i, a = calc_metrics(y_np, y_hat)\n",
        "\n",
        "                tot_loss += loss\n",
        "                tot_d += d\n",
        "                tot_i += i\n",
        "                tot_a += a\n",
        "                nb += 1\n",
        "\n",
        "        nb = max(nb, 1)\n",
        "        metrics = {\n",
        "            \"loss\": tot_loss / nb,\n",
        "            \"dice\": tot_d / nb,\n",
        "            \"iou\": tot_i / nb,\n",
        "            \"acc\": tot_a / nb,\n",
        "            \"cid\": int(self.cid),\n",
        "            \"run_name\": self.run_name,\n",
        "        }\n",
        "\n",
        "        return metrics[\"loss\"], self.nva, metrics\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--cid\", type=int, required=True)\n",
        "    parser.add_argument(\"--server\", default=\"0.0.0.0:8080\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    fl.client.start_numpy_client(\n",
        "        server_address=args.server,\n",
        "        client=SegClient(args.cid),\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_T8nH8zAxNTj",
        "outputId": "a3f9d91b-00ea-4354-bd7e-dba1ae3a0cf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing fl_client.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile fl_sim_colab.py\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "import torch\n",
        "import flwr as fl\n",
        "import time\n",
        "import numpy as np\n",
        "from flwr.common import FitIns\n",
        "import torchio as tio\n",
        "from fl_client import SegClient\n",
        "\n",
        "import logging\n",
        "import warnings\n",
        "logging.getLogger(\"flwr\").setLevel(logging.ERROR)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def ensure_csv(path: str, header: list[str]):\n",
        "    if not os.path.isfile(path):\n",
        "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "        with open(path, \"w\", newline=\"\") as f:\n",
        "            csv.writer(f).writerow(header)\n",
        "\n",
        "\n",
        "def append_row(path: str, row: list):\n",
        "    with open(path, \"a\", newline=\"\") as f:\n",
        "        csv.writer(f).writerow(row)\n",
        "\n",
        "\n",
        "def log_server_metrics(file_path, rnd, duration):\n",
        "    directory = os.path.dirname(file_path)\n",
        "    if directory and not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "    msg = f\"Round {rnd} Aggregation Time: {duration:.4f} seconds\\n\"\n",
        "    with open(file_path, \"a\") as f:\n",
        "        f.write(msg)\n",
        "\n",
        "\n",
        "class LossBasedStrategy(fl.server.strategy.FedAvg):\n",
        "    def __init__(self, metrics_dir: str, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.metrics_dir = metrics_dir\n",
        "        self.header = [\n",
        "            \"round\",\"epoch\",\"train_loss\",\"train_dice\",\"train_iou\",\"train_acc\",\n",
        "            \"val_loss\",\"val_dice\",\"val_iou\",\"val_acc\",\"best_epoch\",\n",
        "        ]\n",
        "\n",
        "        self.logical_client_losses = {}\n",
        "\n",
        "        self.proxy_to_logical_map = {}\n",
        "\n",
        "    def configure_fit(self, server_round, parameters, client_manager):\n",
        "        config = {}\n",
        "        if self.on_fit_config_fn is not None:\n",
        "            config = self.on_fit_config_fn(server_round)\n",
        "        fit_ins = FitIns(parameters, config)\n",
        "\n",
        "        sample_size = self.min_fit_clients\n",
        "        available_clients = client_manager.clients\n",
        "\n",
        "        proxy_cids = list(available_clients.keys())\n",
        "\n",
        "        if len(available_clients) < sample_size:\n",
        "            sample_size = len(available_clients)\n",
        "\n",
        "        if server_round == 1 or not self.logical_client_losses:\n",
        "            print(f\"\\n[Round {server_round}] Random Selection (No history/mapping yet)\")\n",
        "            selected_proxy_cids = np.random.choice(proxy_cids, size=3, replace=False)\n",
        "        else:\n",
        "            weights = []\n",
        "            max_loss = max(self.logical_client_losses.values()) if self.logical_client_losses else 1.0\n",
        "\n",
        "            for proxy_cid in proxy_cids:\n",
        "                logical_cid = self.proxy_to_logical_map.get(proxy_cid)\n",
        "\n",
        "                if logical_cid is not None:\n",
        "                    loss = self.logical_client_losses.get(str(logical_cid), max_loss)\n",
        "                else:\n",
        "                    loss = max_loss\n",
        "\n",
        "                weights.append(loss)\n",
        "\n",
        "            total_loss = sum(weights)\n",
        "            if total_loss == 0:\n",
        "                probs = [1.0 / len(proxy_cids)] * len(proxy_cids)\n",
        "            else:\n",
        "                probs = [w / total_loss for w in weights]\n",
        "\n",
        "            selected_proxy_cids = np.random.choice(proxy_cids, size=sample_size, replace=False, p=probs)\n",
        "\n",
        "            debug_info = {}\n",
        "            for pid, prob in zip(proxy_cids, probs):\n",
        "                lid = self.proxy_to_logical_map.get(pid, \"Unknown\")\n",
        "                loss_val = weights[proxy_cids.index(pid)]\n",
        "                debug_info[f\"LogID:{lid}\"] = f\"Loss:{loss_val:.4f}, Prob:{prob:.4f}\"\n",
        "\n",
        "            print(f\"\\n[Round {server_round}] Loss-Based Selection:\")\n",
        "            print(f\"   Candidates: {debug_info}\")\n",
        "            print(f\"   Selected Proxies (mapped): {[self.proxy_to_logical_map.get(p, 'Unknown') for p in selected_proxy_cids]}\")\n",
        "\n",
        "        selected_clients = []\n",
        "        for proxy_cid in selected_proxy_cids:\n",
        "            selected_clients.append(available_clients[proxy_cid])\n",
        "\n",
        "        return [(client, fit_ins) for client in selected_clients]\n",
        "\n",
        "    def aggregate_fit(self, rnd, results, failures):\n",
        "        start_time = time.time()\n",
        "        agg = super().aggregate_fit(rnd, results, failures)\n",
        "        end_time = time.time()\n",
        "\n",
        "        print(f\"Round {rnd} Aggregation Time: {end_time - start_time:.4f} seconds\")\n",
        "        log_file_path = os.path.join(self.metrics_dir, \"server_aggregation_log.txt\")\n",
        "        log_server_metrics(log_file_path, rnd, end_time - start_time)\n",
        "\n",
        "        for client_proxy, fit_res in results:\n",
        "            m = fit_res.metrics or {}\n",
        "\n",
        "            proxy_id = client_proxy.cid\n",
        "\n",
        "            if \"cid\" in m:\n",
        "                logical_id = str(m[\"cid\"])\n",
        "                self.proxy_to_logical_map[proxy_id] = logical_id\n",
        "\n",
        "                if \"best_val_loss\" in m:\n",
        "                    self.logical_client_losses[logical_id] = float(m[\"best_val_loss\"])\n",
        "\n",
        "            cid_log = str(m.get(\"cid\", \"unknown\"))\n",
        "            client_csv = os.path.join(self.metrics_dir, f\"metrics_client_{cid_log}.csv\")\n",
        "            ensure_csv(client_csv, self.header)\n",
        "\n",
        "            best_epoch = int(m.get(\"best_epoch\", -1))\n",
        "            per_epoch_raw = m.get(\"per_epoch\", \"[]\")\n",
        "            try:\n",
        "                per_epoch = json.loads(per_epoch_raw)\n",
        "            except:\n",
        "                per_epoch = []\n",
        "\n",
        "            for ep in per_epoch:\n",
        "                epoch = ep.get(\"epoch\", \"\")\n",
        "                row = [rnd, epoch, ep.get(\"train_loss\", \"\"), ep.get(\"train_dice\", \"\"),\n",
        "                       ep.get(\"train_iou\", \"\"), ep.get(\"train_acc\", \"\"), ep.get(\"val_loss\", \"\"),\n",
        "                       ep.get(\"val_dice\", \"\"), ep.get(\"val_iou\", \"\"), ep.get(\"val_acc\", \"\"),\n",
        "                       \"x\" if int(epoch) == best_epoch else \"\"]\n",
        "                append_row(client_csv, row)\n",
        "\n",
        "        return agg\n",
        "\n",
        "\n",
        "def run_one_experiment(model_name: str, encoder_name: str, num_rounds=5, local_epochs=5, lr=1e-3):\n",
        "    run_name = f\"{model_name}__{encoder_name}_loss_selection\".replace(\"/\", \"-\")\n",
        "    base_dir = os.path.join(\"AITDM\", run_name)\n",
        "    metrics_dir = os.path.join(base_dir, \"metrics\")\n",
        "    os.makedirs(metrics_dir, exist_ok=True)\n",
        "\n",
        "    def client_fn(cid: str):\n",
        "        return SegClient(int(cid), model_name=model_name, encoder_name=encoder_name).to_client()\n",
        "\n",
        "    strategy = LossBasedStrategy(\n",
        "        metrics_dir=metrics_dir,\n",
        "        fraction_fit=0.1,\n",
        "        fraction_evaluate=1.0,\n",
        "        min_fit_clients=2,\n",
        "        min_evaluate_clients=3,\n",
        "        min_available_clients=3,\n",
        "        on_fit_config_fn=lambda rnd: {\"local_epochs\": local_epochs, \"lr\": lr},\n",
        "    )\n",
        "\n",
        "    use_gpu = torch.cuda.is_available()\n",
        "    client_resources = {\"num_cpus\": 1, \"num_gpus\": 1.0 if use_gpu else 0.0}\n",
        "\n",
        "    fl.simulation.start_simulation(\n",
        "        client_fn=client_fn,\n",
        "        num_clients=3,\n",
        "        config=fl.server.ServerConfig(num_rounds=num_rounds),\n",
        "        strategy=strategy,\n",
        "        client_resources=client_resources,\n",
        "        ray_init_args={\"include_dashboard\": False},\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    experiments = [\n",
        "        (\"unet\", \"resnet50\"),\n",
        "        (\"unet\", \"mit_b3\"),\n",
        "        (\"deeplabv3plus\", \"timm-mobilenetv3_small_100\"),\n",
        "    ]\n",
        "\n",
        "    for model_name, encoder_name in experiments:\n",
        "        print(f\"\\n=== Running: {model_name} + {encoder_name} (Loss Based Selection) ===\")\n",
        "        run_one_experiment(model_name, encoder_name, num_rounds=7, local_epochs=7, lr=1e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcMEFKNUxRXI",
        "outputId": "8f28d47e-0a03-4e97-d216-a28e38b80989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing fl_sim_colab.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/AITDM/client_0/*\n",
        "!rm -rf /content/AITDM/client_1/*\n",
        "!rm -rf /content/AITDM/client_2/*\n",
        "!rm -rf /content/AITDM/deeplabv3plus__timm-mobilenetv3_small_100/checkpoints/*\n",
        "!rm -rf /content/AITDM/unet__resnet50/checkpoints/*\n",
        "!rm -rf /content/AITDM/unet__mit_b3/checkpoints/*\n",
        "!rm -rf /content/AITDM/deeplabv3plus__timm-mobilenetv3_small_100/metrics/*\n",
        "!rm -rf /content/AITDM/unet__resnet50/metrics/*\n",
        "!rm -rf /content/AITDM/unet__mit_b3/metrics/*\n",
        "!python fl_sim_colab.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nFUOsT7xX0o",
        "outputId": "549085a5-3f45-4f8d-bae4-bab5bccb2a04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Running: unet + resnet50 (Loss Based Selection) ===\n",
            "2026-01-08 13:45:42.349583: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2026-01-08 13:45:42.368751: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1767879942.391043    2272 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1767879942.397654    2272 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1767879942.414395    2272 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1767879942.414430    2272 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1767879942.414433    2272 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1767879942.414435    2272 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-08 13:45:42.419456: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "2026-01-08 13:45:51,336\tINFO worker.py:2012 -- Started a local Ray instance.\n",
            "\u001b[36m(pid=2612)\u001b[0m 2026-01-08 13:45:56.542645: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(pid=2612)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(pid=2612)\u001b[0m E0000 00:00:1767879956.565484    2612 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(pid=2612)\u001b[0m E0000 00:00:1767879956.573198    2612 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(pid=2612)\u001b[0m W0000 00:00:1767879956.590790    2612 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=2612)\u001b[0m W0000 00:00:1767879956.590846    2612 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=2612)\u001b[0m W0000 00:00:1767879956.590851    2612 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=2612)\u001b[0m W0000 00:00:1767879956.590855    2612 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=2612)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "\u001b[36m(pid=2612)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "\u001b[36m(pid=2612)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "\u001b[36m(pid=2612)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "\u001b[36m(pid=2612)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type swigvarlink has no __module__ attribute\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   _C._set_float32_matmul_precision(precision)\n",
            "\n",
            "[Round 1] Random Selection (No history/mapping yet)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2612) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(pid=gcs_server)\u001b[0m [2026-01-08 13:46:18,348 E 2412 2412] (gcs_server) gcs_server.cc:302: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m Client 2 Round 0: Incoming 127239.06 KB | Outgoing 127239.06 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2612) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[33m(raylet)\u001b[0m [2026-01-08 13:46:21,272 E 2549 2549] (raylet) main.cc:975: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m Client 0 Round 0: Incoming 127239.06 KB | Outgoing 127239.06 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m [2026-01-08 13:46:24,963 E 2612 2695] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "[2026-01-08 13:46:25,821 E 2272 2610] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2612) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m Client 1 Round 0: Incoming 127239.06 KB | Outgoing 127239.06 KB | Overhead: 0.000001s\n",
            "Round 1 Aggregation Time: 0.5012 seconds\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "\u001b[36m(pid=2616)\u001b[0m [2026-01-08 13:46:25,803 E 2616 3228] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\u001b[32m [repeated 11x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2612) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2612) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2612) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\n",
            "[Round 2] Loss-Based Selection:\n",
            "   Candidates: {'LogID:0': 'Loss:0.5671, Prob:0.2490', 'LogID:1': 'Loss:0.3642, Prob:0.1599', 'LogID:2': 'Loss:1.3459, Prob:0.5910'}\n",
            "   Selected Proxies (mapped): ['1', '2']\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2612) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m Client 1 Round 0: Incoming 127239.06 KB | Outgoing 127239.06 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2612) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m Client 2 Round 0: Incoming 127239.06 KB | Outgoing 127239.06 KB | Overhead: 0.000001s\n",
            "Round 2 Aggregation Time: 0.3252 seconds\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2612) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2612) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2612) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\n",
            "[Round 3] Loss-Based Selection:\n",
            "   Candidates: {'LogID:0': 'Loss:0.5671, Prob:0.4083', 'LogID:1': 'Loss:0.2757, Prob:0.1985', 'LogID:2': 'Loss:0.5462, Prob:0.3932'}\n",
            "   Selected Proxies (mapped): ['2', '0']\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2612) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m Client 2 Round 0: Incoming 127239.06 KB | Outgoing 127239.06 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2612) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m Client 0 Round 0: Incoming 127239.06 KB | Outgoing 127239.06 KB | Overhead: 0.000001s\n",
            "Round 3 Aggregation Time: 0.2795 seconds\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2612) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2612) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2612) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\n",
            "[Round 4] Loss-Based Selection:\n",
            "   Candidates: {'LogID:0': 'Loss:0.3267, Prob:0.3369', 'LogID:1': 'Loss:0.2757, Prob:0.2843', 'LogID:2': 'Loss:0.3675, Prob:0.3789'}\n",
            "   Selected Proxies (mapped): ['1', '2']\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2612) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m Client 1 Round 0: Incoming 127239.06 KB | Outgoing 127239.06 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2612) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m Client 2 Round 0: Incoming 127239.06 KB | Outgoing 127239.06 KB | Overhead: 0.000001s\n",
            "Round 4 Aggregation Time: 0.2873 seconds\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2612) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2612) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2612) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\n",
            "[Round 5] Loss-Based Selection:\n",
            "   Candidates: {'LogID:0': 'Loss:0.3267, Prob:0.3498', 'LogID:1': 'Loss:0.1977, Prob:0.2117', 'LogID:2': 'Loss:0.4094, Prob:0.4384'}\n",
            "   Selected Proxies (mapped): ['0', '2']\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2612) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m Client 0 Round 0: Incoming 127239.06 KB | Outgoing 127239.06 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2612) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m Client 2 Round 0: Incoming 127239.06 KB | Outgoing 127239.06 KB | Overhead: 0.000001s\n",
            "Round 5 Aggregation Time: 0.2888 seconds\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2612) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2612) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2612) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\n",
            "[Round 6] Loss-Based Selection:\n",
            "   Candidates: {'LogID:0': 'Loss:0.2684, Prob:0.3193', 'LogID:1': 'Loss:0.1977, Prob:0.2352', 'LogID:2': 'Loss:0.3747, Prob:0.4456'}\n",
            "   Selected Proxies (mapped): ['2', '1']\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2612) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m Client 2 Round 0: Incoming 127239.06 KB | Outgoing 127239.06 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2612) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m Client 1 Round 0: Incoming 127239.06 KB | Outgoing 127239.06 KB | Overhead: 0.000001s\n",
            "Round 6 Aggregation Time: 0.2726 seconds\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2612) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2612) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2612) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\n",
            "[Round 7] Loss-Based Selection:\n",
            "   Candidates: {'LogID:0': 'Loss:0.2684, Prob:0.3204', 'LogID:1': 'Loss:0.2217, Prob:0.2646', 'LogID:2': 'Loss:0.3476, Prob:0.4149'}\n",
            "   Selected Proxies (mapped): ['1', '2']\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2612) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m Client 1 Round 0: Incoming 127239.06 KB | Outgoing 127239.06 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2612) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m Client 2 Round 0: Incoming 127239.06 KB | Outgoing 127239.06 KB | Overhead: 0.000001s\n",
            "Round 7 Aggregation Time: 0.2726 seconds\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2612) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2612) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2612) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=2612)\u001b[0m   return data.pin_memory(device)\n",
            "\n",
            "=== Running: unet + mit_b3 (Loss Based Selection) ===\n",
            "2026-01-08 13:48:12,497\tINFO worker.py:2012 -- Started a local Ray instance.\n",
            "\u001b[36m(pid=4583)\u001b[0m 2026-01-08 13:48:17.321670: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(pid=4583)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(pid=4583)\u001b[0m E0000 00:00:1767880097.343479    4583 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(pid=4583)\u001b[0m E0000 00:00:1767880097.350205    4583 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(pid=4583)\u001b[0m W0000 00:00:1767880097.368272    4583 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=4583)\u001b[0m W0000 00:00:1767880097.368319    4583 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=4583)\u001b[0m W0000 00:00:1767880097.368322    4583 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=4583)\u001b[0m W0000 00:00:1767880097.368325    4583 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=4583)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "\u001b[36m(pid=4583)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "\u001b[36m(pid=4583)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "\u001b[36m(pid=4583)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "\u001b[36m(pid=4583)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type swigvarlink has no __module__ attribute\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   _C._set_float32_matmul_precision(precision)\n",
            "\n",
            "[Round 1] Random Selection (No history/mapping yet)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4583) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(pid=gcs_server)\u001b[0m [2026-01-08 13:48:39,792 E 4396 4396] (gcs_server) gcs_server.cc:302: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "\u001b[33m(raylet)\u001b[0m [2026-01-08 13:48:42,433 E 4512 4512] (raylet) main.cc:975: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m [2026-01-08 13:48:46,103 E 4583 4686] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "[2026-01-08 13:48:46,693 E 2272 4572] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m Client 0 Round 0: Incoming 184967.64 KB | Outgoing 184967.64 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(pid=4576)\u001b[0m [2026-01-08 13:48:46,681 E 4576 5185] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4583) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m Client 2 Round 0: Incoming 184967.64 KB | Outgoing 184967.64 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4583) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m Client 1 Round 0: Incoming 184967.64 KB | Outgoing 184967.64 KB | Overhead: 0.000001s\n",
            "Round 1 Aggregation Time: 0.6868 seconds\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4583) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4583) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4583) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\n",
            "[Round 2] Loss-Based Selection:\n",
            "   Candidates: {'LogID:0': 'Loss:0.4044, Prob:0.3307', 'LogID:1': 'Loss:0.2950, Prob:0.2413', 'LogID:2': 'Loss:0.5235, Prob:0.4281'}\n",
            "   Selected Proxies (mapped): ['2', '0']\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4583) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m Client 2 Round 0: Incoming 184967.64 KB | Outgoing 184967.64 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4583) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m Client 0 Round 0: Incoming 184967.64 KB | Outgoing 184967.64 KB | Overhead: 0.000001s\n",
            "Round 2 Aggregation Time: 0.4187 seconds\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4583) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4583) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4583) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\n",
            "[Round 3] Loss-Based Selection:\n",
            "   Candidates: {'LogID:0': 'Loss:0.3389, Prob:0.3456', 'LogID:1': 'Loss:0.2950, Prob:0.3008', 'LogID:2': 'Loss:0.3468, Prob:0.3536'}\n",
            "   Selected Proxies (mapped): ['0', '1']\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4583) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m Client 0 Round 0: Incoming 184967.64 KB | Outgoing 184967.64 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4583) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m Client 1 Round 0: Incoming 184967.64 KB | Outgoing 184967.64 KB | Overhead: 0.000001s\n",
            "Round 3 Aggregation Time: 0.3977 seconds\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4583) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4583) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4583) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\n",
            "[Round 4] Loss-Based Selection:\n",
            "   Candidates: {'LogID:0': 'Loss:0.2660, Prob:0.3330', 'LogID:1': 'Loss:0.1860, Prob:0.2329', 'LogID:2': 'Loss:0.3468, Prob:0.4341'}\n",
            "   Selected Proxies (mapped): ['1', '2']\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4583) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m Client 1 Round 0: Incoming 184967.64 KB | Outgoing 184967.64 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4583) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m Client 2 Round 0: Incoming 184967.64 KB | Outgoing 184967.64 KB | Overhead: 0.000001s\n",
            "Round 4 Aggregation Time: 0.4086 seconds\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4583) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4583) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4583) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\n",
            "[Round 5] Loss-Based Selection:\n",
            "   Candidates: {'LogID:0': 'Loss:0.2660, Prob:0.3789', 'LogID:1': 'Loss:0.1849, Prob:0.2635', 'LogID:2': 'Loss:0.2510, Prob:0.3576'}\n",
            "   Selected Proxies (mapped): ['1', '0']\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4583) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m Client 1 Round 0: Incoming 184967.64 KB | Outgoing 184967.64 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4583) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m Client 0 Round 0: Incoming 184967.64 KB | Outgoing 184967.64 KB | Overhead: 0.000001s\n",
            "Round 5 Aggregation Time: 0.3985 seconds\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4583) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4583) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4583) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\n",
            "[Round 6] Loss-Based Selection:\n",
            "   Candidates: {'LogID:0': 'Loss:0.2215, Prob:0.3326', 'LogID:1': 'Loss:0.1934, Prob:0.2904', 'LogID:2': 'Loss:0.2510, Prob:0.3770'}\n",
            "   Selected Proxies (mapped): ['0', '2']\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4583) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m Client 0 Round 0: Incoming 184967.64 KB | Outgoing 184967.64 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4583) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m Client 2 Round 0: Incoming 184967.64 KB | Outgoing 184967.64 KB | Overhead: 0.000001s\n",
            "Round 6 Aggregation Time: 0.3946 seconds\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4583) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4583) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4583) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\n",
            "[Round 7] Loss-Based Selection:\n",
            "   Candidates: {'LogID:0': 'Loss:0.1914, Prob:0.2961', 'LogID:1': 'Loss:0.1934, Prob:0.2992', 'LogID:2': 'Loss:0.2616, Prob:0.4047'}\n",
            "   Selected Proxies (mapped): ['2', '1']\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4583) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m Client 2 Round 0: Incoming 184967.64 KB | Outgoing 184967.64 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4583) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m Client 1 Round 0: Incoming 184967.64 KB | Outgoing 184967.64 KB | Overhead: 0.000001s\n",
            "Round 7 Aggregation Time: 0.4202 seconds\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4583) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4583) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=4583) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=4583)\u001b[0m   return data.pin_memory(device)\n",
            "\n",
            "=== Running: deeplabv3plus + timm-mobilenetv3_small_100 (Loss Based Selection) ===\n",
            "2026-01-08 13:54:04,359\tINFO worker.py:2012 -- Started a local Ray instance.\n",
            "\u001b[36m(pid=7421)\u001b[0m 2026-01-08 13:54:09.171623: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(pid=7421)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(pid=7421)\u001b[0m E0000 00:00:1767880449.193306    7421 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(pid=7421)\u001b[0m E0000 00:00:1767880449.199956    7421 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(pid=7421)\u001b[0m W0000 00:00:1767880449.218053    7421 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=7421)\u001b[0m W0000 00:00:1767880449.218090    7421 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=7421)\u001b[0m W0000 00:00:1767880449.218093    7421 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=7421)\u001b[0m W0000 00:00:1767880449.218095    7421 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=7421)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "\u001b[36m(pid=7421)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "\u001b[36m(pid=7421)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "\u001b[36m(pid=7421)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "\u001b[36m(pid=7421)\u001b[0m AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type swigvarlink has no __module__ attribute\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   _C._set_float32_matmul_precision(precision)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\n",
            "[Round 1] Random Selection (No history/mapping yet)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=7421) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(pid=gcs_server)\u001b[0m [2026-01-08 13:54:31,674 E 7237 7237] (gcs_server) gcs_server.cc:302: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "\u001b[33m(raylet)\u001b[0m [2026-01-08 13:54:34,295 E 7353 7353] (raylet) main.cc:975: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m [2026-01-08 13:54:37,570 E 7421 7472] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "[2026-01-08 13:54:38,564 E 2272 7413] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Client 2 Round 0: Incoming 8505.40 KB | Outgoing 8505.40 KB | Overhead: 0.000000s\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(pid=7415)\u001b[0m [2026-01-08 13:54:38,551 E 7415 8027] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=7421) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Client 1 Round 0: Incoming 8505.40 KB | Outgoing 8505.40 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=7421) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Client 0 Round 0: Incoming 8505.40 KB | Outgoing 8505.40 KB | Overhead: 0.000000s\n",
            "Round 1 Aggregation Time: 0.0798 seconds\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=7421) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=7421) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=7421) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\n",
            "[Round 2] Loss-Based Selection:\n",
            "   Candidates: {'LogID:0': 'Loss:0.6493, Prob:0.4294', 'LogID:1': 'Loss:0.2416, Prob:0.1598', 'LogID:2': 'Loss:0.6213, Prob:0.4109'}\n",
            "   Selected Proxies (mapped): ['1', '0']\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=7421) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Client 1 Round 0: Incoming 8505.40 KB | Outgoing 8505.40 KB | Overhead: 0.000000s\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=7421) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Client 0 Round 0: Incoming 8505.40 KB | Outgoing 8505.40 KB | Overhead: 0.000001s\n",
            "Round 2 Aggregation Time: 0.0529 seconds\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=7421) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=7421) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=7421) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\n",
            "[Round 3] Loss-Based Selection:\n",
            "   Candidates: {'LogID:0': 'Loss:0.2082, Prob:0.1940', 'LogID:1': 'Loss:0.2437, Prob:0.2271', 'LogID:2': 'Loss:0.6213, Prob:0.5790'}\n",
            "   Selected Proxies (mapped): ['1', '2']\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=7421) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Client 1 Round 0: Incoming 8505.40 KB | Outgoing 8505.40 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=7421) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Client 2 Round 0: Incoming 8505.40 KB | Outgoing 8505.40 KB | Overhead: 0.000001s\n",
            "Round 3 Aggregation Time: 0.0510 seconds\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=7421) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=7421) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=7421) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   self.pid = os.fork()\n",
            "\n",
            "[Round 4] Loss-Based Selection:\n",
            "   Candidates: {'LogID:0': 'Loss:0.2082, Prob:0.2488', 'LogID:1': 'Loss:0.2752, Prob:0.3289', 'LogID:2': 'Loss:0.3533, Prob:0.4223'}\n",
            "   Selected Proxies (mapped): ['0', '2']\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=7421) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Client 0 Round 0: Incoming 8505.40 KB | Outgoing 8505.40 KB | Overhead: 0.000000s\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=7421) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Client 2 Round 0: Incoming 8505.40 KB | Outgoing 8505.40 KB | Overhead: 0.000001s\n",
            "Round 4 Aggregation Time: 0.0524 seconds\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=7421) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=7421) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=7421) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\n",
            "[Round 5] Loss-Based Selection:\n",
            "   Candidates: {'LogID:0': 'Loss:0.2104, Prob:0.2519', 'LogID:1': 'Loss:0.2752, Prob:0.3295', 'LogID:2': 'Loss:0.3496, Prob:0.4186'}\n",
            "   Selected Proxies (mapped): ['2', '0']\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=7421) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Client 2 Round 0: Incoming 8505.40 KB | Outgoing 8505.40 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=7421) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Client 0 Round 0: Incoming 8505.40 KB | Outgoing 8505.40 KB | Overhead: 0.000001s\n",
            "Round 5 Aggregation Time: 0.0535 seconds\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=7421) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=7421) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=7421) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\n",
            "[Round 6] Loss-Based Selection:\n",
            "   Candidates: {'LogID:0': 'Loss:0.2077, Prob:0.2480', 'LogID:1': 'Loss:0.2752, Prob:0.3286', 'LogID:2': 'Loss:0.3546, Prob:0.4234'}\n",
            "   Selected Proxies (mapped): ['0', '2']\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=7421) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Client 0 Round 0: Incoming 8505.40 KB | Outgoing 8505.40 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=7421) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Client 2 Round 0: Incoming 8505.40 KB | Outgoing 8505.40 KB | Overhead: 0.000001s\n",
            "Round 6 Aggregation Time: 0.0523 seconds\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=7421) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=7421) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=7421) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\n",
            "[Round 7] Loss-Based Selection:\n",
            "   Candidates: {'LogID:0': 'Loss:0.2279, Prob:0.2707', 'LogID:1': 'Loss:0.2752, Prob:0.3270', 'LogID:2': 'Loss:0.3387, Prob:0.4023'}\n",
            "   Selected Proxies (mapped): ['1', '2']\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=7421) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Client 1 Round 0: Incoming 8505.40 KB | Outgoing 8505.40 KB | Overhead: 0.000001s\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=7421) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Client 2 Round 0: Incoming 8505.40 KB | Outgoing 8505.40 KB | Overhead: 0.000001s\n",
            "Round 7 Aggregation Time: 0.0514 seconds\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=7421) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=7421) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/__init__.py:73: DeprecationWarning: `timm-` encoders are deprecated and will be removed in the future. Please use `tu-` equivalent encoders instead (see 'Timm encoders' section in the documentation).\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=7421) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   self.pid = os.fork()\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "\u001b[36m(ClientAppActor pid=7421)\u001b[0m   return data.pin_memory(device)\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import segmentation_models_pytorch as smp\n",
        "import logging\n",
        "from fl_client import TRANSFORMS\n",
        "\n",
        "from seg_data import (\n",
        "    ImageOnlyGliomaDataset,\n",
        "    SubsetByPIDs,\n",
        "    image_only_collate_fn,\n",
        "    DATA_ROOT,\n",
        "    METADATA_DF_PATH,\n",
        "    USE_ATLAS,\n",
        "    calc_metrics,\n",
        ")\n",
        "\n",
        "logging.getLogger(\"timm.models._builder\").setLevel(logging.ERROR)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SEED = 42\n",
        "THRESHOLD = 0.5\n",
        "\n",
        "CLIENT_DIR = \"/content/client\"\n",
        "BATCH_SIZE = 8\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "ENSEMBLE_CFGS = [\n",
        "    (\"unet\", \"resnet50\"),\n",
        "    (\"unet\", \"mit_b3\"),\n",
        "    (\"deeplabv3plus\", \"timm-mobilenetv3_small_100\"),\n",
        "]\n",
        "\n",
        "WEIGHT_MODE = \"power\"\n",
        "WEIGHT_POWER = 14.0\n",
        "WEIGHT_EPS = 1e-6\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "set_seed(SEED)\n",
        "\n",
        "bce = nn.BCEWithLogitsLoss()\n",
        "dice_loss = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
        "\n",
        "\n",
        "def criterion(logits, y):\n",
        "    return 0.5 * bce(logits, y) + 0.5 * dice_loss(logits, y)\n",
        "\n",
        "\n",
        "def get_val_loader(cid: int):\n",
        "    full = ImageOnlyGliomaDataset(\n",
        "        METADATA_DF_PATH, DATA_ROOT, use_atlas=USE_ATLAS, exclude_ids=[\"PatientID_0191\"]\n",
        "    )\n",
        "    with open(os.path.join(CLIENT_DIR, f\"client_{cid}\", \"val_pids.json\")) as f:\n",
        "        va_p = json.load(f)\n",
        "\n",
        "    ds_va = SubsetByPIDs(full, va_p)\n",
        "    g = torch.Generator().manual_seed(SEED)\n",
        "\n",
        "    ld_va = torch.utils.data.DataLoader(\n",
        "        ds_va,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "        collate_fn=lambda b: image_only_collate_fn(b, use_atlas=USE_ATLAS),\n",
        "        generator=g,\n",
        "    )\n",
        "    return ld_va, len(ds_va)\n",
        "\n",
        "\n",
        "def run_name(model_name: str, encoder_name: str) -> str:\n",
        "    return f\"{model_name}__{encoder_name}\".replace(\"/\", \"-\")\n",
        "\n",
        "\n",
        "def build_model(model_name: str, encoder_name: str, encoder_weights=\"imagenet\"):\n",
        "    in_ch = 2 if USE_ATLAS else 1\n",
        "    mn = model_name.lower()\n",
        "\n",
        "    if mn == \"unet\":\n",
        "        m = smp.Unet(\n",
        "            encoder_name=encoder_name,\n",
        "            encoder_weights=encoder_weights,\n",
        "            in_channels=in_ch,\n",
        "            classes=1,\n",
        "        )\n",
        "    elif mn in [\"deeplabv3plus\", \"deeplabv3+\", \"dlv3p\"]:\n",
        "        m = smp.DeepLabV3Plus(\n",
        "            encoder_name=encoder_name,\n",
        "            encoder_weights=encoder_weights,\n",
        "            in_channels=in_ch,\n",
        "            classes=1,\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model_name={model_name}\")\n",
        "\n",
        "    return m.to(DEVICE)\n",
        "\n",
        "\n",
        "def ckpt_path(model_name: str, encoder_name: str, cid: int) -> str:\n",
        "    rn = run_name(model_name, encoder_name)\n",
        "    return os.path.join(\"AITDM\", rn, \"checkpoints\", f\"client_{cid}_best.pt\")\n",
        "\n",
        "\n",
        "def best_json_path(model_name: str, encoder_name: str, cid: int) -> str:\n",
        "    rn = run_name(model_name, encoder_name)\n",
        "    return os.path.join(\"AITDM\", rn, \"checkpoints\", f\"client_{cid}_best.json\")\n",
        "\n",
        "\n",
        "def load_model(model_name: str, encoder_name: str, cid: int):\n",
        "    path = ckpt_path(model_name, encoder_name, cid)\n",
        "    if not os.path.isfile(path):\n",
        "        raise FileNotFoundError(f\"Missing checkpoint: {path}\")\n",
        "    m = build_model(model_name, encoder_name).to(DEVICE)\n",
        "    sd = torch.load(path, map_location=\"cpu\")\n",
        "    m.load_state_dict(sd, strict=True)\n",
        "    m.eval()\n",
        "    return m\n",
        "\n",
        "\n",
        "def load_best_val_dice(model_name: str, encoder_name: str, cid: int) -> float:\n",
        "    p = best_json_path(model_name, encoder_name, cid)\n",
        "    if not os.path.isfile(p):\n",
        "        raise FileNotFoundError(f\"Missing best json: {p}\")\n",
        "    with open(p, \"r\") as f:\n",
        "        j = json.load(f)\n",
        "    return float(j.get(\"val_dice\", 0.0))\n",
        "\n",
        "\n",
        "def get_client_weights(cid: int, cfgs, mode=\"power\", power=2.0, eps=1e-6):\n",
        "    dices = [load_best_val_dice(mn, enc, cid) for (mn, enc) in cfgs]\n",
        "    d = np.array(dices, dtype=np.float32)\n",
        "\n",
        "    if mode == \"linear\":\n",
        "        raw = np.clip(d, 0.0, None)\n",
        "    elif mode == \"power\":\n",
        "        raw = np.power(np.clip(d, 0.0, None), power)\n",
        "    else:\n",
        "        raise ValueError(\"mode must be 'linear' or 'power'\")\n",
        "\n",
        "    raw = raw + eps\n",
        "    w = raw / raw.sum()\n",
        "    return w.tolist(), dices\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def ensemble_forward_logits(x, models, weights, target_hw=None):\n",
        "    w = np.array(weights, dtype=np.float32)\n",
        "    w = w / (w.sum() + 1e-8)\n",
        "\n",
        "    logits_sum = None\n",
        "    for mi, wi in zip(models, w):\n",
        "        li = mi(x)\n",
        "        if target_hw is not None and li.shape[-2:] != target_hw:\n",
        "            li = F.interpolate(li, size=target_hw, mode=\"bilinear\", align_corners=False)\n",
        "        logits_sum = li * float(wi) if logits_sum is None else logits_sum + li * float(wi)\n",
        "    return logits_sum\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_ensemble_on_client(cid: int, threshold=0.5):\n",
        "    val_loader, nva = get_val_loader(cid)\n",
        "    models = [load_model(mn, enc, cid) for (mn, enc) in ENSEMBLE_CFGS]\n",
        "\n",
        "    weights, best_dices = get_client_weights(\n",
        "        cid, ENSEMBLE_CFGS, mode=WEIGHT_MODE, power=WEIGHT_POWER, eps=WEIGHT_EPS\n",
        "    )\n",
        "\n",
        "    tot_loss = tot_d = tot_i = tot_a = 0.0\n",
        "    nb = 0\n",
        "\n",
        "    for batch in val_loader:\n",
        "        x = batch[\"x\"].to(DEVICE)\n",
        "        y = batch[\"y\"].to(DEVICE)\n",
        "\n",
        "        logits = ensemble_forward_logits(x, models=models, weights=weights, target_hw=y.shape[-2:])\n",
        "        loss = float(criterion(logits, y).item())\n",
        "\n",
        "        preds_bin = (torch.sigmoid(logits).cpu().numpy() > threshold).astype(np.uint8)\n",
        "        y_np = (y.cpu().numpy() > 0.5).astype(np.uint8)\n",
        "\n",
        "        d, i, a = calc_metrics(y_np, preds_bin)\n",
        "\n",
        "        tot_loss += loss\n",
        "        tot_d += d\n",
        "        tot_i += i\n",
        "        tot_a += a\n",
        "        nb += 1\n",
        "\n",
        "    nb = max(nb, 1)\n",
        "    return {\n",
        "        \"cid\": int(cid),\n",
        "        \"nva\": int(nva),\n",
        "        \"loss\": tot_loss / nb,\n",
        "        \"dice\": tot_d / nb,\n",
        "        \"iou\": tot_i / nb,\n",
        "        \"acc\": tot_a / nb,\n",
        "        \"weights\": weights,\n",
        "        \"best_dices\": best_dices,\n",
        "    }\n",
        "\n",
        "\n",
        "def log_client_metrics(file_path, r, bd, ww):\n",
        "\n",
        "    directory = os.path.dirname(file_path)\n",
        "    if directory and not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    log_message = (\n",
        "        f\"Client {r['cid']} (n={r['nva']}): \"\n",
        "        f\"loss={r['loss']:.4f} dice={r['dice']:.4f} iou={r['iou']:.4f} acc={r['acc']:.4f}\\n\"\n",
        "        f\"  best_dice={['%.4f' % x for x in bd]} -> weights={['%.3f' % x for x in ww]}\\n\"\n",
        "    )\n",
        "\n",
        "    print(log_message, end='')\n",
        "\n",
        "    with open(file_path, \"a\") as f:\n",
        "        f.write(log_message)\n",
        "\n",
        "def main():\n",
        "    print(f\"[Ensemble-3 | threshold={THRESHOLD} | weight_mode={WEIGHT_MODE} | power={WEIGHT_POWER}]\\n\")\n",
        "    for cid in [0, 1, 2]:\n",
        "        r = eval_ensemble_on_client(cid, threshold=THRESHOLD)\n",
        "        bd = r[\"best_dices\"]\n",
        "        ww = r[\"weights\"]\n",
        "        log_client_metrics(os.path.join('/content/AITDM', f\"client_{cid}\", \"val_metrics_ensemble.txt\"), r, bd, ww)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEXaVqP9xboG",
        "outputId": "5ec5a0bb-ba8c-4b1b-fd87-67c614fab426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Ensemble-3 | threshold=0.5 | weight_mode=power | power=14.0]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Client 0 (n=9): loss=0.1749 dice=0.7317 iou=0.6009 acc=0.9836\n",
            "  best_dice=['0.5891', '0.7087', '0.6915'] -> weights=['0.042', '0.561', '0.397']\n",
            "Client 1 (n=25): loss=0.1643 dice=0.7678 iou=0.6263 acc=0.9783\n",
            "  best_dice=['0.7250', '0.7549', '0.6837'] -> weights=['0.312', '0.550', '0.137']\n",
            "Client 2 (n=8): loss=0.2507 dice=0.5643 iou=0.3931 acc=0.9885\n",
            "  best_dice=['0.4149', '0.5648', '0.4454'] -> weights=['0.015', '0.948', '0.037']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/drive/MyDrive/AITDM\n",
        "!cp -rf /content/AITDM /content/drive/MyDrive/AITDM/SELECTION_LOSS"
      ],
      "metadata": {
        "id": "aeDG5ABrxfsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "gsFUGgV-xjVc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}